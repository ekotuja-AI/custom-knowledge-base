# ğŸ“– Sistema RAG Offline - Wikipedia + LangChain + Ollama

**Sistema completo de Retrieval-Augmented Generation (RAG) offline utilizando Wikipedia, LangChain e LLM local.**

[![FastAPI](https://img.shields.io/badge/FastAPI-005571?style=for-the-badge&logo=fastapi)](https://fastapi.tiangolo.com/)
[![Python](https://img.shields.io/badge/python-3.11+-blue.svg?style=for-the-badge&logo=python)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)](https://www.docker.com/)
[![Qdrant](https://img.shields.io/badge/Qdrant-DC244C?style=for-the-badge&logo=qdrant&logoColor=white)](https://qdrant.tech/)
[![LangChain](https://img.shields.io/badge/LangChain-121212?style=for-the-badge)](https://langchain.com/)

## ğŸ¯ VisÃ£o Geral

Um sistema RAG (Retrieval-Augmented Generation) completo e **100% offline** que combina:
- ğŸŒ **Wikipedia** como base de conhecimento
- ğŸ” **Busca vetorial semÃ¢ntica** com Qdrant
- ğŸ¤– **LLM local** (Qwen 2.5 7B) via Ollama
- âš¡ **LangChain** para processamento de documentos
- ğŸ¨ **Interface web moderna** para consultas

### âœ¨ Funcionalidades Principais

- âœ… **Busca SemÃ¢ntica**: Encontra informaÃ§Ãµes por similaridade, nÃ£o apenas palavras-chave
- âœ… **Perguntas Inteligentes**: Respostas contextualizadas usando LLM + conhecimento da Wikipedia
- âœ… **100% Offline**: Funciona completamente sem internet apÃ³s setup inicial
- âœ… **Tool Calling**: Suporte nativo a function calling com Qwen 2.5
- âœ… **Processamento em Lote**: IngestÃ£o eficiente de dumps da Wikipedia
- âœ… **Interface Web**: UI moderna com gradiente purple

## ğŸ—ï¸ Arquitetura

### Diagrama de Componentes

<div align="center">
  <img src="docs/architecture-diagram.svg" alt="Arquitetura do Sistema RAG" width="100%"/>
</div>

<details>
<summary>ğŸ“Š Ver diagrama Mermaid interativo (clique para expandir)</summary>

```mermaid
graph TB
    User[ğŸ‘¤ USUÃRIO<br/>Interface Web<br/>localhost:9000]
    
    subgraph API[FastAPI REST API - Port 9000]
        Routes[Endpoints:<br/>GET / - Interface<br/>POST /buscar - Busca<br/>POST /perguntar - RAG<br/>POST /adicionar - Novos artigos<br/>GET /estatisticas - MÃ©tricas]
    end
    
    subgraph Processing[Camada de Processamento]
        LangChain[ğŸ”— LangChain<br/>â€¢ TextSplitter<br/>â€¢ Document Loader<br/>â€¢ Retriever<br/>â€¢ Embeddings]
        WikiService[ğŸ“š Wikipedia Service<br/>â€¢ Busca hÃ­brida<br/>â€¢ RAG Pipeline<br/>â€¢ Cache management<br/>â€¢ Fallback strategies]
    end
    
    subgraph Storage[Camada de Armazenamento]
        ST[ğŸ§  Sentence Transformers<br/>paraphrase-multilingual-<br/>MiniLM-L12-v2<br/>384 dimensions]
        Qdrant[(ğŸ—„ï¸ Qdrant Vector DB<br/>Port 6333<br/><br/>Collection: wikipedia_langchain<br/>â€¢ 1566 chunks<br/>â€¢ 384 dimensions<br/>â€¢ Cosine distance)]
    end
    
    subgraph LLM[Camada de GeraÃ§Ã£o]
        Ollama[ğŸ¤– Ollama LLM Server<br/>Port 11434<br/><br/>Model: qwen2.5:7b 4.7GB<br/>â€¢ 7B parÃ¢metros<br/>â€¢ Tool calling<br/>â€¢ 128K context<br/>â€¢ Q4_0 quantization<br/><br/>Config:<br/>temp=0.7, top_p=0.9<br/>max_tokens=512, ctx=8192]
    end
    
    subgraph Data[Fontes de Dados]
        Wiki[ğŸ“– Wikipedia<br/>â€¢ Simple English Dump 320MB<br/>â€¢ 100 artigos<br/>â€¢ 1566 chunks<br/>â€¢ Wikipedia API]
    end
    
    User -->|HTTP Requests| API
    API --> LangChain
    API --> WikiService
    LangChain <-->|Documents| WikiService
    WikiService --> ST
    ST -->|Vectors 384d| Qdrant
    WikiService -->|Context| Ollama
    Qdrant -->|Retrieval| WikiService
    Data -->|Ingest| LangChain
    
    classDef userClass fill:#9b59b6,stroke:#8e44ad,color:#fff
    classDef apiClass fill:#3498db,stroke:#2980b9,color:#fff
    classDef processClass fill:#1abc9c,stroke:#16a085,color:#fff
    classDef storageClass fill:#e74c3c,stroke:#c0392b,color:#fff
    classDef llmClass fill:#f39c12,stroke:#d68910,color:#fff
    classDef dataClass fill:#95a5a6,stroke:#7f8c8d,color:#fff
    
    class User userClass
    class API,Routes apiClass
    class LangChain,WikiService processClass
    class ST,Qdrant storageClass
    class Ollama llmClass
    class Wiki dataClass
```

</details>


### Fluxo de Dados

#### 1ï¸âƒ£ **IngestÃ£o de Documentos**

```mermaid
graph LR
    A[ğŸ“¥ Wikipedia Dump<br/>320MB] --> B[ğŸ” Parser]
    B --> C[âœ‚ï¸ LangChain<br/>TextSplitter]
    C --> D[ğŸ§  Sentence<br/>Transformers]
    D --> E[ğŸ“Š Embeddings<br/>384 dimensions]
    E --> F[(ğŸ—„ï¸ Qdrant<br/>Vector DB)]
    
    classDef inputClass fill:#3498db,stroke:#2980b9,color:#fff
    classDef processClass fill:#1abc9c,stroke:#16a085,color:#fff
    classDef outputClass fill:#e74c3c,stroke:#c0392b,color:#fff
    
    class A inputClass
    class B,C,D,E processClass
    class F outputClass
```

#### 2ï¸âƒ£ **Busca SemÃ¢ntica**

```mermaid
graph LR
    A[â“ Query do<br/>UsuÃ¡rio] --> B[ğŸ§  Embedding<br/>Generator]
    B --> C[(ğŸ” Qdrant<br/>Search)]
    C --> D[ğŸ“Š Top K<br/>Documentos]
    D --> E[âš–ï¸ Ranking +<br/>Filtros]
    E --> F[âœ… Resultados<br/>Relevantes]
    
    classDef inputClass fill:#9b59b6,stroke:#8e44ad,color:#fff
    classDef processClass fill:#1abc9c,stroke:#16a085,color:#fff
    classDef outputClass fill:#f39c12,stroke:#d68910,color:#fff
    
    class A inputClass
    class B,C,D,E processClass
    class F outputClass
```

#### 3ï¸âƒ£ **RAG (Retrieval-Augmented Generation)**

```mermaid
graph TB
    A[â“ Pergunta do<br/>UsuÃ¡rio] --> B[ğŸ” Busca<br/>SemÃ¢ntica]
    B --> C[ğŸ“š Top N<br/>Chunks]
    C --> D[ğŸ“ ConstruÃ§Ã£o<br/>de Contexto]
    D --> E[âœï¸ Prompt<br/>Engineering]
    E --> F[ğŸ¤– Ollama<br/>Qwen 2.5 7B]
    F --> G[ğŸ’¬ Resposta +<br/>CitaÃ§Ãµes]
    
    classDef inputClass fill:#9b59b6,stroke:#8e44ad,color:#fff
    classDef processClass fill:#1abc9c,stroke:#16a085,color:#fff
    classDef llmClass fill:#f39c12,stroke:#d68910,color:#fff
    classDef outputClass fill:#27ae60,stroke:#229954,color:#fff
    
    class A inputClass
    class B,C,D,E processClass
    class F llmClass
    class G outputClass
```

### Stack TecnolÃ³gico

| Componente | Tecnologia | VersÃ£o | FunÃ§Ã£o |
|------------|------------|--------|--------|
| **API Framework** | FastAPI | 0.104+ | REST API assÃ­ncrona |
| **Vector Database** | Qdrant | 1.11.3 | Armazenamento vetorial |
| **LLM Server** | Ollama | 0.12.9 | Servidor de modelos |
| **LLM Model** | Qwen 2.5 | 7B | GeraÃ§Ã£o de respostas |
| **Embeddings** | SentenceTransformers | 2.3.0 | VetorizaÃ§Ã£o multilÃ­ngue |
| **Document Processing** | LangChain | 0.1.0+ | Pipeline de documentos |
| **Containerization** | Docker Compose | - | OrquestraÃ§Ã£o |
| **Language** | Python | 3.11+ | Runtime |

## ğŸš€ Quick Start

### PrÃ©-requisitos

- **Docker & Docker Compose** instalados
- **8GB+ RAM** disponÃ­vel
- **15GB+ espaÃ§o em disco** livre
- **Windows**, **Linux** ou **macOS**

### 1. Clone o RepositÃ³rio

```bash
git clone https://github.com/ekotuja-AI/dicionario_vetorial.git
cd dicionario_vetorial
```

### 2. Inicie os Containers

```bash
# Inicia todos os serviÃ§os
docker-compose up -d

# Monitore os logs
docker-compose logs -f
```

### 3. Aguarde InicializaÃ§Ã£o

O sistema irÃ¡:
- âœ… Inicializar Qdrant (banco vetorial)
- âœ… Baixar Qwen 2.5 7B (~4.7GB) - **primeira vez apenas**
- âœ… Carregar modelo de embeddings
- âœ… Configurar FastAPI

**Tempo estimado**: 5-10 minutos na primeira execuÃ§Ã£o

### 4. Acesse o Sistema

- ğŸŒ **Interface Web**: http://localhost:9000
- ğŸ“– **DocumentaÃ§Ã£o API**: http://localhost:9000/docs
- ğŸ“Š **Status**: http://localhost:9000/status
- ğŸ“ˆ **EstatÃ­sticas**: http://localhost:9000/estatisticas

## ğŸ“Š Dados Atuais

### Base de Conhecimento

- **Fonte**: Simple Wikipedia (inglÃªs simplificado)
- **Artigos**: 100 processados
- **Chunks**: 1566 vetorizados
- **DimensÃµes**: 384 (multilÃ­ngue)
- **Modelo Embedding**: paraphrase-multilingual-MiniLM-L12-v2

### Modelo LLM

- **Nome**: Qwen 2.5 (7B parÃ¢metros)
- **Tamanho**: 4.7 GB
- **QuantizaÃ§Ã£o**: Q4_0
- **Contexto**: 128K tokens
- **Capabilities**: Tool calling, RAG, MultilÃ­ngue
- **Idiomas**: PortuguÃªs, InglÃªs, Espanhol, ChinÃªs, +25 outros

## ğŸ”§ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente (.env)

```env
# Qdrant Vector Database
QDRANT_HOST=localhost
QDRANT_PORT=6333

# Ollama LLM Server
OLLAMA_HOST=localhost
OLLAMA_PORT=11434
LLM_MODEL=qwen2.5:7b
LLM_MAX_TOKENS=512
LLM_TEMPERATURE=0.7

# Embeddings Model
EMBEDDING_MODEL=paraphrase-multilingual-MiniLM-L12-v2

# Data Directories
DATA_DIR=./data
MODELS_DIR=./models
```

### Volumes Docker

| Volume | DescriÃ§Ã£o | Tamanho Aproximado |
|--------|-----------|-------------------|
| `qdrant_storage` | Dados do banco vetorial | ~500 MB |
| `ollama_models` | Modelos LLM | ~5 GB |
| `./data` | Dumps e cache | ~1 GB |

## ğŸ“š Uso da API

### Busca SemÃ¢ntica

```bash
curl -X POST "http://localhost:9000/buscar" \
  -H "Content-Type: application/json" \
  -d '{"query": "inteligÃªncia artificial", "limite": 5}'
```

**PowerShell:**
```powershell
$body = '{"query": "inteligÃªncia artificial", "limite": 5}'
Invoke-RestMethod -Uri "http://localhost:9000/buscar" `
  -Method Post -Body $body -ContentType "application/json"
```

### Perguntas com RAG

```bash
curl -X POST "http://localhost:9000/perguntar" \
  -H "Content-Type: application/json" \
  -d '{"pergunta": "O que Ã© Python?"}'
```

**PowerShell:**
```powershell
$body = '{"pergunta": "O que Ã© Python?"}'
Invoke-RestMethod -Uri "http://localhost:9000/perguntar" `
  -Method Post -Body $body -ContentType "application/json"
```

### Adicionar Artigos

```bash
curl -X POST "http://localhost:9000/adicionar" \
  -H "Content-Type: application/json" \
  -d '{"titulo": "Machine Learning"}'
```

## ğŸ¨ Interface Web

A interface web oferece:
- ğŸ” **Busca interativa** com resultados em tempo real
- ğŸ’¬ **Chat com IA** para perguntas complexas
- â• **Adicionar artigos** dinamicamente
- ğŸ“Š **EstatÃ­sticas** da base de conhecimento
- ğŸ¨ **Design moderno** com gradiente purple

## ğŸ“– Expandindo a Base

### Processar Dumps da Wikipedia

```bash
# 1. Baixar dump (executar dentro do container)
docker exec offline_wikipedia_app python scripts/download_wikipedia.py \
  --language simple \
  --max-articles 1000

# 2. O processamento Ã© automÃ¡tico com LangChain
```

### Adicionar Artigos EspecÃ­ficos via API

```python
import requests

# Lista de artigos para adicionar
artigos = [
    "Artificial Intelligence",
    "Machine Learning",
    "Deep Learning",
    "Natural Language Processing"
]

for titulo in artigos:
    response = requests.post(
        "http://localhost:9000/adicionar",
        json={"titulo": titulo}
    )
    print(f"{titulo}: {response.json()}")
```

## ğŸ³ Gerenciamento Docker

### Comandos Ãšteis

```bash
# Iniciar serviÃ§os
docker-compose up -d

# Parar serviÃ§os
docker-compose stop

# Reiniciar serviÃ§os
docker-compose restart

# Ver logs
docker-compose logs -f

# Ver logs de um serviÃ§o especÃ­fico
docker-compose logs -f app

# Remover tudo (incluindo volumes)
docker-compose down -v

# Rebuild apÃ³s mudanÃ§as
docker-compose up -d --build
```

### PowerShell Script (Windows)

O projeto inclui `docker-restart.ps1`:

```powershell
.\docker-restart.ps1
```

## ğŸ” Troubleshooting

### Problema: Container nÃ£o inicia

```bash
# Verificar logs
docker-compose logs app

# Reiniciar serviÃ§os
docker-compose restart
```

### Problema: Modelo Ollama nÃ£o encontrado

```bash
# Entrar no container Ollama
docker exec -it ollama_server bash

# Listar modelos
ollama list

# Baixar modelo manualmente
ollama pull qwen2.5:7b
```

### Problema: Embeddings nÃ£o funcionam

```bash
# Reinstalar dependÃªncias no container
docker exec offline_wikipedia_app pip install sentence-transformers==2.3.0 transformers==4.36.0

# Reiniciar container
docker-compose restart app
```

### Problema: Porta jÃ¡ em uso

Edite `docker-compose.yml` e altere as portas:

```yaml
ports:
  - "9001:9000"  # API na porta 9001
```

## ğŸ“ˆ Performance

### Benchmarks (Hardware mÃ©dio)

| OperaÃ§Ã£o | Tempo | Recursos |
|----------|-------|----------|
| **Busca SemÃ¢ntica** | ~200ms | CPU |
| **Gerar Resposta (Qwen 2.5)** | 10-30s | CPU/GPU |
| **Adicionar Artigo** | 2-5s | CPU |
| **Processar 100 artigos** | 5-10min | CPU |

### OtimizaÃ§Ãµes

- âœ… Usar GPU para inferÃªncia mais rÃ¡pida
- âœ… Aumentar `num_ctx` para contextos maiores
- âœ… Ajustar `chunk_size` para chunks menores/maiores
- âœ… Usar modelo menor (phi3:mini) se necessÃ¡rio velocidade

## ğŸ› ï¸ Desenvolvimento

### Estrutura de DiretÃ³rios

```
dicionario_vetorial/
â”œâ”€â”€ api/                      # FastAPI endpoints
â”‚   â”œâ”€â”€ config.py            # ConfiguraÃ§Ãµes
â”‚   â”œâ”€â”€ models.py            # Modelos Pydantic
â”‚   â””â”€â”€ wikipediaFuncionalAPI.py  # API principal
â”œâ”€â”€ services/                 # LÃ³gica de negÃ³cio
â”‚   â”œâ”€â”€ langchainWikipediaService.py  # LangChain integration
â”‚   â”œâ”€â”€ wikipediaOfflineService.py    # OrquestraÃ§Ã£o RAG
â”‚   â””â”€â”€ wikipediaDumpService.py       # Parser de dumps
â”œâ”€â”€ scripts/                  # UtilitÃ¡rios
â”‚   â””â”€â”€ download_wikipedia.py # Download de dumps
â”œâ”€â”€ static/                   # Interface web
â”‚   â””â”€â”€ index.html           # UI frontend
â”œâ”€â”€ data/                     # Dados e cache
â”œâ”€â”€ docker-compose.yml        # OrquestraÃ§Ã£o Docker
â”œâ”€â”€ Dockerfile               # Container da API
â”œâ”€â”€ requirements_minimal.txt  # DependÃªncias Python
â””â”€â”€ README.md                # Este arquivo
```

### Adicionar Novos Endpoints

1. Editar `api/wikipediaFuncionalAPI.py`
2. Adicionar modelo em `api/models.py` se necessÃ¡rio
3. Implementar lÃ³gica em `services/`
4. Testar via `/docs`

### Contribuindo

1. Fork o projeto
2. Crie uma branch (`git checkout -b feature/nova-funcionalidade`)
3. Commit suas mudanÃ§as (`git commit -am 'Adiciona nova funcionalidade'`)
4. Push para a branch (`git push origin feature/nova-funcionalidade`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.

## ğŸ™ Agradecimentos

- [Qwen Team](https://github.com/QwenLM/Qwen) - Modelo LLM excelente
- [Ollama](https://ollama.ai/) - Servidor LLM local
- [LangChain](https://python.langchain.com/) - Framework RAG
- [Qdrant](https://qdrant.tech/) - Banco vetorial
- [FastAPI](https://fastapi.tiangolo.com/) - Framework web
- [Wikipedia](https://www.wikipedia.org/) - Base de conhecimento

## ğŸ“ Suporte

- **Issues**: [GitHub Issues](https://github.com/ekotuja-AI/dicionario_vetorial/issues)
- **DiscussÃµes**: [GitHub Discussions](https://github.com/ekotuja-AI/dicionario_vetorial/discussions)
- **Email**: ekotuja@gmail.com

## ğŸ—ºï¸ Roadmap

- [ ] Suporte a mÃºltiplos idiomas da Wikipedia
- [ ] Interface de chat em tempo real (WebSocket)
- [ ] ExportaÃ§Ã£o de conversas
- [ ] Sistema de cache inteligente
- [ ] MÃ©tricas e analytics
- [ ] API de administraÃ§Ã£o
- [ ] Suporte a outros modelos LLM
- [ ] Fine-tuning do modelo em domÃ­nios especÃ­ficos
- [ ] Deploy em cloud (AWS/GCP/Azure)

---

<div align="center">

**Feito com â¤ï¸ usando Python, LangChain e Ollama**

[â¬† Voltar ao topo](#-sistema-rag-offline---wikipedia--langchain--ollama)

</div>
