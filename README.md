# ğŸ“– Sistema Wikipedia Offline# Wikipedia Semantic Search API ğŸ”



Sistema completo de consulta offline da Wikipedia em portuguÃªs utilizando tecnologias modernas de IA e processamento vetorial.**API REST para busca semÃ¢ntica em artigos da Wikipedia com sistema RAG integrado usando LangChain e LLM.**



## âš¡ CaracterÃ­sticas Principais[![FastAPI](https://img.shields.io/badge/FastAPI-005571?style=for-the-badge&logo=fastapi)](https://fastapi.tiangolo.com/)

[![Python](https://img.shields.io/badge/python-3.11+-blue.svg?style=for-the-badge&logo=python)](https://www.python.org/downloads/)

- ğŸŒ **100% Offline**: Funciona sem conexÃ£o com a internet[![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)](https://www.docker.com/)

- ğŸ¤– **IA Integrada**: Respostas inteligentes usando Ollama (phi3:mini)[![OpenAI](https://img.shields.io/badge/OpenAI-412991?style=for-the-badge&logo=openai&logoColor=white)](https://openai.com/)

- ğŸ” **Busca Vetorial**: Pesquisa semÃ¢ntica usando Qdrant

- ğŸ“š **Wikipedia Real**: Processa dumps oficiais da Wikipedia em portuguÃªs## ğŸ¯ VisÃ£o Geral

- ğŸ³ **Docker**: Deploy simplificado com containers

- âš¡ **Performance**: Processamento otimizado de grandes volumes de dadosUma API moderna que combina **busca semÃ¢ntica** com **inteligÃªncia artificial** para explorar o conhecimento da Wikipedia de forma inteligente. Utilizando **embeddings**, **LangChain** e **LLM**, oferece tanto busca tradicional quanto respostas contextualizadas para perguntas complexas.



## ğŸ—ï¸ Arquitetura### âœ¨ Funcionalidades Principais



```ğŸ” **Busca SemÃ¢ntica AvanÃ§ada**

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”- Busca por conceitos, nÃ£o apenas palavras-chave

â”‚   FastAPI       â”‚    â”‚    Qdrant       â”‚    â”‚    Ollama       â”‚- Encontra artigos relacionados semanticamente

â”‚   (Port 9000)   â”‚â—„â”€â”€â–ºâ”‚   (Port 6333)   â”‚    â”‚  (Port 11434)   â”‚- Resultados ordenados por relevÃ¢ncia

â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚

â”‚ â€¢ Wikipedia API â”‚    â”‚ â€¢ Vector DB     â”‚    â”‚ â€¢ LLM Local     â”‚ğŸ¤– **Sistema RAG (Retrieval-Augmented Generation)**

â”‚ â€¢ Dump Parser   â”‚    â”‚ â€¢ 384 dims      â”‚    â”‚ â€¢ phi3:mini     â”‚- Respostas inteligentes para perguntas complexas

â”‚ â€¢ Status/Stats  â”‚    â”‚ â€¢ Cosine dist   â”‚    â”‚ â€¢ 2.2GB model   â”‚- LLM integrado com conhecimento da Wikipedia

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜- CitaÃ§Ã£o automÃ¡tica das fontes utilizadas

```

ğŸ“š **Base de Conhecimento DinÃ¢mica**

## ğŸš€ InÃ­cio RÃ¡pido- Artigos da Wikipedia em portuguÃªs

- AdiÃ§Ã£o dinÃ¢mica de novos artigos

### PrÃ©-requisitos- Processamento automÃ¡tico com LangChain

- Docker e Docker Compose

- 8GB+ RAM disponÃ­velğŸš€ **API REST Moderna**

- 15GB+ espaÃ§o em disco- DocumentaÃ§Ã£o interativa automÃ¡tica

- ValidaÃ§Ã£o automÃ¡tica de dados

### 1. Clone e Execute- Endpoints intuitivos e bem documentados

```bash

git clone <repo-url>## ğŸ—ï¸ Arquitetura

cd dicionario_vetorial

docker-compose up -d```

```â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚   FastAPI       â”‚    â”‚  LangChain   â”‚    â”‚    Qdrant      â”‚

### 2. Aguarde a InicializaÃ§Ã£oâ”‚   (REST API)    â”‚â—„â”€â”€â–ºâ”‚ (Processing) â”‚â—„â”€â”€â–ºâ”‚ (Vector Store)  â”‚

O sistema irÃ¡:â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

- âœ… Inicializar Qdrant (banco vetorial)         â”‚                       â”‚                    â”‚

- âœ… Baixar Ollama phi3:mini (2.2GB)         â–¼                       â–¼                    â–¼

- âœ… Configurar FastAPIâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚   OpenAI GPT    â”‚    â”‚  Wikipedia   â”‚    â”‚ SentenceTransf. â”‚

### 3. Acesse a APIâ”‚     (LLM)       â”‚    â”‚    (API)     â”‚    â”‚  (Embeddings)   â”‚

- **Interface Web**: http://localhost:9000/docsâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

- **Status do Sistema**: http://localhost:9000/status```

- **EstatÃ­sticas**: http://localhost:9000/estatisticas

### ğŸ› ï¸ Stack TecnolÃ³gico

## ğŸ“Š Endpoints Principais

- **[FastAPI](https://fastapi.tiangolo.com/)**: Framework web assÃ­ncrono de alta performance

### Sistema- **[Qdrant](https://qdrant.tech/)**: Banco de dados vetorial especializado

- `GET /status` - Status geral do sistema- **[LangChain](https://python.langchain.com/)**: Framework para aplicaÃ§Ãµes com LLM

- `GET /estatisticas` - MÃ©tricas da base de dados- **[SentenceTransformers](https://www.sbert.net/)**: Modelos de embedding multilÃ­ngues

- `GET /health` - Health check- **[OpenAI GPT](https://openai.com/)**: Large Language Model para respostas inteligentes

- **[Wikipedia API](https://pypi.org/project/wikipedia/)**: Acesso aos artigos da Wikipedia

### Consultas- **[Docker](https://www.docker.com/)**: ContainerizaÃ§Ã£o para deployment fÃ¡cil

- `POST /pesquisar` - Busca semÃ¢ntica na Wikipedia

- `POST /responder` - Pergunta com IA (contexto + LLM)## ğŸš€ Quick Start



### Dados Wikipedia### PrÃ©-requisitos

- `POST /dumps/download` - Download de dumps oficiais

- `POST /dumps/processar-real` - Processa dumps baixados- **Docker & Docker Compose** (recomendado)

- `POST /dumps/descomprimir-e-processar` - MÃ©todo otimizado para BZ2- Ou **Python 3.11+** para execuÃ§Ã£o local

- `GET /dumps/status-download` - Progresso dos downloads- **Chave da API OpenAI** (opcional, para funcionalidade RAG)



### ExpansÃ£o de Dados### 1. Clone o RepositÃ³rio

- `POST /wikipedia-api/expandir-base` - Adiciona artigos via API

```bash

## ğŸ”§ ConfiguraÃ§Ã£ogit clone https://github.com/ekotuja-AI/dicionario_vetorial.git

cd dicionario_vetorial

### VariÃ¡veis de Ambiente (.env)```

```env

# Qdrant### 2. ConfiguraÃ§Ã£o (Opcional)

QDRANT_HOST=qdrant

QDRANT_PORT=6333Para usar o sistema RAG com LLM, configure sua chave da OpenAI:



# Ollama```bash

OLLAMA_HOST=ollama# Copie o arquivo de exemplo

OLLAMA_PORT=11434cp .env.example .env

LLM_MODEL=phi3:mini

# Edite .env e adicione sua chave

# EmbeddingsOPENAI_API_KEY=your_openai_api_key_here

EMBEDDING_MODEL=paraphrase-multilingual-MiniLM-L12-v2```



# Dados### 3. Inicie com Docker (Recomendado)

DATA_DIR=/app/data

``````bash

# Inicia todos os serviÃ§os automaticamente

### Volumes Persistentesdocker-compose up --build

- `qdrant_storage`: Dados do banco vetorial

- `ollama_models`: Modelos de IA baixados# Para executar em background

- `./data`: Dumps XML e cachedocker-compose up --build -d

```

## ğŸ“ˆ Dados Atuais

### 4. Acesse a API

O sistema jÃ¡ possui:

- **93 chunks** de artigos da WikipediaA API estarÃ¡ disponÃ­vel em:

- **384 dimensÃµes** por vetor- **API**: http://localhost:9000

- **DistÃ¢ncia Cosine** para similaridade- **DocumentaÃ§Ã£o Swagger**: http://localhost:9000/docs

- **Processamento BZ2** para dumps oficiais- **DocumentaÃ§Ã£o ReDoc**: http://localhost:9000/redoc



## ğŸ› ï¸ Comandos Ãšteis## ğŸ“– Uso da API



### Download e Processamento### ğŸ” Busca SemÃ¢ntica

```bash

# Baixar dumps da Wikipedia PTEncontre artigos relacionados a um conceito:

curl -X POST "http://localhost:9000/dumps/download?dump_type=pages-articles"

```bash

# Processar dump baixado (mÃ©todo otimizado)curl -X POST "http://localhost:9000/buscar" \

curl -X POST "http://localhost:9000/dumps/descomprimir-e-processar?filename=ptwiki-20251020-pages-articles.xml.bz2&max_artigos=1000"  -H "Content-Type: application/json" \

  -d '{

# Expandir base via API Wikipedia    "query": "inteligÃªncia artificial e machine learning",

curl -X POST "http://localhost:9000/wikipedia-api/expandir-base?num_artigos=100"    "limit": 5

```  }'

```

### Consultas

```bash**Resposta:**

# Busca semÃ¢ntica```json

curl -X POST "http://localhost:9000/pesquisar" \{

  -H "Content-Type: application/json" \  "query": "inteligÃªncia artificial e machine learning",

  -d '{"query": "inteligÃªncia artificial", "limit": 5}'  "total_resultados": 5,

  "resultados": [

# Pergunta com IA    {

curl -X POST "http://localhost:9000/responder" \      "title": "InteligÃªncia artificial",

  -H "Content-Type: application/json" \      "content": "InteligÃªncia artificial Ã© a simulaÃ§Ã£o de processos...",

  -d '{"pergunta": "O que Ã© machine learning?"}'      "url": "https://pt.wikipedia.org/wiki/Intelig%C3%AAncia_artificial",

```      "score": 0.9234

    }

### Monitoramento  ],

```bash  "tempo_busca_ms": 45.2

# Status dos containers}

docker-compose ps```



# Logs da aplicaÃ§Ã£o### ğŸ¤– Perguntas com RAG

docker-compose logs -f app

FaÃ§a perguntas complexas e obtenha respostas contextualizadas:

# EstatÃ­sticas do sistema

curl http://localhost:9000/estatisticas```bash

```curl -X POST "http://localhost:9000/perguntar" \

  -H "Content-Type: application/json" \

## ğŸ“ Estrutura do Projeto  -d '{

    "pergunta": "O que Ã© inteligÃªncia artificial e como ela funciona?",

```    "max_chunks": 5

dicionario_vetorial/  }'

â”œâ”€â”€ docker-compose.yml           # OrquestraÃ§Ã£o```

â”œâ”€â”€ Dockerfile                   # Imagem da app

â”œâ”€â”€ requirements_minimal.txt     # Dependencies**Resposta:**

â”œâ”€â”€ .env / .env.example         # ConfiguraÃ§Ãµes```json

â”œâ”€â”€ data/                       # Dumps e cache{

â”œâ”€â”€ api/  "pergunta": "O que Ã© inteligÃªncia artificial e como ela funciona?",

â”‚   â”œâ”€â”€ wikipediaFuncionalAPI.py # API principal  "resposta": "InteligÃªncia artificial (IA) Ã© um campo da ciÃªncia da computaÃ§Ã£o que visa criar sistemas capazes de realizar tarefas que normalmente requerem inteligÃªncia humana...",

â”‚   â”œâ”€â”€ config.py               # ConfiguraÃ§Ãµes  "fontes": [

â”‚   â””â”€â”€ models.py               # Modelos Pydantic    {

â””â”€â”€ services/      "title": "InteligÃªncia artificial",

    â”œâ”€â”€ wikipediaOfflineService.py  # IntegraÃ§Ã£o Qdrant      "content": "ConteÃºdo relevante...",

    â”œâ”€â”€ wikipediaDumpService.py     # Parser XML      "url": "https://pt.wikipedia.org/wiki/Intelig%C3%AAncia_artificial",

    â””â”€â”€ offlineWikipediaService.py  # ServiÃ§os base      "score": 0.9234

```    }

  ],

## ğŸ” Funcionalidades AvanÃ§adas  "raciocinio": "Resposta gerada baseada em 3 fontes relevantes da Wikipedia.",

  "tempo_processamento_ms": 1250.5

### Processamento de Dumps}

- âœ… **BZ2/GZ Support**: DescompressÃ£o automÃ¡tica```

- âœ… **Chunking Inteligente**: DivisÃ£o otimizada de artigos

- âœ… **Namespace Filtering**: Apenas artigos principais### ğŸ“š Adicionar ConteÃºdo

- âœ… **Progress Monitoring**: Acompanhamento em tempo real

Adicione novos artigos da Wikipedia Ã  base:

### Busca Vetorial

- âœ… **Embeddings MultilÃ­ngues**: Modelo otimizado para portuguÃªs```bash

- âœ… **Similaridade SemÃ¢ntica**: Busca por contexto, nÃ£o apenas palavrascurl -X POST "http://localhost:9000/adicionar" \

- âœ… **Ranking por RelevÃ¢ncia**: Resultados ordenados por similaridade  -H "Content-Type: application/json" \

  -d '{

### IA Conversacional    "titulo": "CiÃªncia de dados"

- âœ… **LLM Local**: Processamento sem envio de dados externos  }'

- âœ… **Contexto Relevante**: Respostas baseadas em artigos similares```

- âœ… **Respostas Estruturadas**: Output formatado e organizado

## ğŸ“Š Endpoints DisponÃ­veis

## ğŸš¨ SoluÃ§Ã£o de Problemas

| Endpoint | MÃ©todo | DescriÃ§Ã£o |

### Container nÃ£o inicia|----------|--------|-----------|

```bash| `/` | GET | InformaÃ§Ãµes da API |

docker-compose down| `/status` | GET | Status dos componentes |

docker-compose build --no-cache| `/estatisticas` | GET | EstatÃ­sticas da base |

docker-compose up -d| `/buscar` | POST | Busca semÃ¢ntica |

```| `/perguntar` | POST | Perguntas com RAG |

| `/adicionar` | POST | Adicionar artigo |

### Ollama nÃ£o baixa modelo| `/docs` | GET | DocumentaÃ§Ã£o Swagger |

```bash| `/redoc` | GET | DocumentaÃ§Ã£o ReDoc |

docker-compose exec ollama ollama pull phi3:mini

```## ğŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada



### Qdrant sem conexÃ£o### VariÃ¡veis de Ambiente

```bash

# Verificar se porta 6333 estÃ¡ livre```bash

netstat -an | findstr 6333# Qdrant

```QDRANT_HOST=localhost

QDRANT_PORT=6333

### Performance lenta

- Aumente memÃ³ria disponÃ­vel para Docker# OpenAI (para RAG)

- Verifique espaÃ§o em disco disponÃ­velOPENAI_API_KEY=your_api_key_here

- Use SSD para melhor I/O

# ConfiguraÃ§Ãµes do LLM

## ğŸ“Š Monitoramento de PerformanceLLM_MODEL=gpt-3.5-turbo

LLM_TEMPERATURE=0.3

### MÃ©tricas ImportantesLLM_MAX_TOKENS=1000

- **Chunks/segundo**: Velocidade de processamento

- **Tempo de resposta**: LatÃªncia das consultas# Processamento de texto

- **Uso de memÃ³ria**: Ollama + Qdrant + AppCHUNK_SIZE=1000

- **EspaÃ§o em disco**: Dumps + modelos + Ã­ndicesCHUNK_OVERLAP=200



### Logs Ãšteis# Busca

```bashDEFAULT_SEARCH_LIMIT=10

# Performance de processamentoMAX_SEARCH_LIMIT=50

docker-compose logs app | grep "chunks/segundo"```



# Erros de conexÃ£o### ExecuÃ§Ã£o Local (Desenvolvimento)

docker-compose logs app | grep "ERROR"

Se preferir executar sem Docker:

# Status de downloads

curl http://localhost:9000/dumps/status-download```bash

```# 1. Inicie apenas o Qdrant

docker run -p 6333:6333 qdrant/qdrant:v1.11.3

## ğŸ¯ PrÃ³ximos Passos

# 2. Instale dependÃªncias

1. **Expandir Base**: Processar dumps completos (10GB+)pip install -r requirements.txt

2. **Otimizar Queries**: Cache de resultados frequentes

3. **Interface Web**: Frontend para uso mais amigÃ¡vel# 3. Execute a API

4. **APIs Externas**: IntegraÃ§Ã£o com outras fontespython -m api.wikipediaAPI

5. **Backup/Restore**: Sistema de backup dos dados```



---## ğŸ“ Estrutura do Projeto



**Desenvolvido com â¤ï¸ usando FastAPI, Qdrant, Ollama e Docker**```
dicionario_vetorial/
â”œâ”€â”€ ğŸ“ api/                     # Camada da API REST
â”‚   â”œâ”€â”€ wikipediaAPI.py         # Endpoints FastAPI
â”‚   â”œâ”€â”€ models.py               # Modelos Pydantic
â”‚   â””â”€â”€ config.py               # ConfiguraÃ§Ãµes
â”œâ”€â”€ ğŸ“ services/                # LÃ³gica de negÃ³cio
â”‚   â””â”€â”€ wikipediaService.py     # ServiÃ§o principal
â”œâ”€â”€ ğŸ³ docker-compose.yml       # OrquestraÃ§Ã£o Docker
â”œâ”€â”€ ğŸ³ Dockerfile              # Imagem da aplicaÃ§Ã£o
â”œâ”€â”€ ğŸ“‹ requirements.txt         # DependÃªncias Python
â”œâ”€â”€ ğŸ”§ .env.example            # Exemplo de configuraÃ§Ã£o
â””â”€â”€ ğŸ“– README.md               # Esta documentaÃ§Ã£o
```

## ğŸ§ª Exemplos de Uso

### Casos de Uso Comuns

1. **Pesquisa AcadÃªmica**
   ```bash
   # Buscar artigos sobre um tÃ³pico
   "energia renovÃ¡vel e sustentabilidade"
   
   # Fazer pergunta especÃ­fica
   "Quais sÃ£o os principais tipos de energia renovÃ¡vel?"
   ```

2. **ExploraÃ§Ã£o de Conceitos**
   ```bash
   # Buscar conceitos relacionados
   "blockchain e criptomoedas"
   
   # Entender relaÃ§Ãµes
   "Como blockchain se relaciona com seguranÃ§a digital?"
   ```

3. **EducaÃ§Ã£o e Aprendizado**
   ```bash
   # Buscar material educativo
   "histÃ³ria da computaÃ§Ã£o"
   
   # Obter explicaÃ§Ãµes didÃ¡ticas
   "Explique como funciona a internet de forma simples"
   ```

### Scripts de Teste

```python
import requests

# Teste de busca semÃ¢ntica
response = requests.post('http://localhost:9000/buscar', json={
    'query': 'inteligÃªncia artificial',
    'limit': 3
})
print(response.json())

# Teste de pergunta RAG
response = requests.post('http://localhost:9000/perguntar', json={
    'pergunta': 'O que Ã© machine learning?',
    'max_chunks': 3
})
print(response.json())

# Adicionar novo artigo
response = requests.post('http://localhost:9000/adicionar', json={
    'titulo': 'Deep learning'
})
print(response.json())
```

## ğŸ”§ Troubleshooting

### Problemas Comuns

**1. Qdrant nÃ£o conecta**
```bash
# Verifique se o container estÃ¡ rodando
docker ps | grep qdrant

# Verifique os logs
docker logs qdrant
```

**2. LLM nÃ£o funciona**
```bash
# Verifique se a chave da OpenAI estÃ¡ configurada
echo $OPENAI_API_KEY

# Teste a conectividade
curl -H "Authorization: Bearer $OPENAI_API_KEY" \
     https://api.openai.com/v1/models
```

**3. Modelo demora para carregar**
```bash
# O modelo SentenceTransformers Ã© baixado na primeira execuÃ§Ã£o
# Downloads subsequentes usam cache local em ~/.cache/
```

**4. Erro de memÃ³ria**
```bash
# Se executando no Docker, aumente a memÃ³ria disponÃ­vel
# Ou use uma mÃ¡quina com pelo menos 4GB RAM
```

### Logs e Monitoramento

```bash
# Logs da aplicaÃ§Ã£o
docker logs wikipedia_search_app

# Logs do Qdrant
docker logs qdrant

# Logs em tempo real
docker-compose logs -f
```

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Para contribuir:

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

### Desenvolvendo

```bash
# Clone e configure ambiente de desenvolvimento
git clone https://github.com/ekotuja-AI/dicionario_vetorial.git
cd dicionario_vetorial

# Instale dependÃªncias de desenvolvimento
pip install -r requirements.txt
pip install black isort pytest

# Execute testes
pytest

# Formate cÃ³digo
black .
isort .
```

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a LicenÃ§a MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

## ğŸ™ Agradecimentos

- [Wikipedia](https://www.wikipedia.org/) pela base de conhecimento
- [LangChain](https://python.langchain.com/) pelo framework de processamento
- [Qdrant](https://qdrant.tech/) pelo banco de dados vetorial
- [FastAPI](https://fastapi.tiangolo.com/) pelo framework web
- [OpenAI](https://openai.com/) pelos modelos de linguagem

---

**Feito com â¤ï¸ usando Python e tecnologias modernas de IA**