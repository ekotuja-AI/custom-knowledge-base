# ğŸ“– Sistema RAG Offline - Wikipedia + LangChain + Ollama

**Sistema completo de Retrieval-Augmented Generation (RAG) offline utilizando Wikipedia, LangChain e LLM local.**

[![FastAPI](https://img.shields.io/badge/FastAPI-005571?style=for-the-badge&logo=fastapi)](https://fastapi.tiangolo.com/)
[![Python](https://img.shields.io/badge/python-3.11+-blue.svg?style=for-the-badge&logo=python)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)](https://www.docker.com/)
[![Qdrant](https://img.shields.io/badge/Qdrant-DC244C?style=for-the-badge&logo=qdrant&logoColor=white)](https://qdrant.tech/)
[![LangChain](https://img.shields.io/badge/LangChain-121212?style=for-the-badge)](https://langchain.com/)

## ğŸ¯ VisÃ£o Geral

Um sistema RAG (Retrieval-Augmented Generation) completo e **100% offline** que combina:
- ğŸŒ **Wikipedia** como base de conhecimento
- ğŸ” **Busca vetorial semÃ¢ntica** com Qdrant
- ğŸ¤– **LLM local** (Qwen 2.5 7B) via Ollama
- âš¡ **LangChain** para processamento de documentos
- ğŸ¨ **Interface web moderna** para consultas

### âœ¨ Funcionalidades Principais

- âœ… **Busca SemÃ¢ntica**: Encontra informaÃ§Ãµes por similaridade, nÃ£o apenas palavras-chave
- âœ… **Perguntas Inteligentes**: Respostas contextualizadas usando LLM + conhecimento da Wikipedia
- âœ… **100% Offline**: Funciona completamente sem internet apÃ³s setup inicial
- âœ… **Tool Calling**: Suporte nativo a function calling com Qwen 2.5
- âœ… **Processamento em Lote**: IngestÃ£o eficiente de dumps da Wikipedia
- âœ… **Interface Web**: UI moderna com gradiente purple

## ğŸ—ï¸ Arquitetura

### Diagrama de Componentes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USUÃRIO / INTERFACE WEB                      â”‚
â”‚                         http://localhost:9000                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          FASTAPI REST API                            â”‚
â”‚                         (Port 9000)                                  â”‚
â”‚                                                                       â”‚
â”‚  Endpoints:                                                          â”‚
â”‚  â€¢ GET  /              â†’ Interface Web                               â”‚
â”‚  â€¢ POST /buscar        â†’ Busca semÃ¢ntica                            â”‚
â”‚  â€¢ POST /perguntar     â†’ RAG com LLM                                â”‚
â”‚  â€¢ POST /adicionar     â†’ Novos artigos                              â”‚
â”‚  â€¢ GET  /estatisticas  â†’ MÃ©tricas do sistema                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                       â”‚
         â”‚                                       â”‚
         â–¼                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LANGCHAIN         â”‚            â”‚   WIKIPEDIA OFFLINE SERVICE      â”‚
â”‚   (Processing)      â”‚            â”‚   (OrquestraÃ§Ã£o)                 â”‚
â”‚                     â”‚            â”‚                                  â”‚
â”‚ â€¢ TextSplitter      â”‚            â”‚ â€¢ Busca hÃ­brida                  â”‚
â”‚ â€¢ Document Loader   â”‚            â”‚ â€¢ RAG Pipeline                   â”‚
â”‚ â€¢ Retriever         â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ â€¢ Cache management               â”‚
â”‚ â€¢ Embeddings        â”‚            â”‚ â€¢ Fallback strategies            â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                     â”‚
       â”‚                                     â”‚
       â–¼                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SENTENCE           â”‚            â”‚   QDRANT VECTOR DATABASE         â”‚
â”‚  TRANSFORMERS       â”‚            â”‚   (Port 6333)                    â”‚
â”‚                     â”‚            â”‚                                  â”‚
â”‚ Model:              â”‚            â”‚ Collection: wikipedia_langchain  â”‚
â”‚ paraphrase-         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ â€¢ 1566 chunks                    â”‚
â”‚ multilingual-       â”‚  Vectors   â”‚ â€¢ 384 dimensions                 â”‚
â”‚ MiniLM-L12-v2       â”‚  (384d)    â”‚ â€¢ Cosine distance                â”‚
â”‚                     â”‚            â”‚ â€¢ Persistent storage             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                             â–¼                       â”‚
â”‚                        OLLAMA LLM SERVER                            â”‚
â”‚                        (Port 11434)                                 â”‚
â”‚                                                                     â”‚
â”‚  Modelo Ativo: qwen2.5:7b (4.7 GB)                                â”‚
â”‚  â€¢ 7 bilhÃµes de parÃ¢metros                                         â”‚
â”‚  â€¢ Tool calling nativo                                             â”‚
â”‚  â€¢ Contexto: 128K tokens                                           â”‚
â”‚  â€¢ QuantizaÃ§Ã£o: Q4_0                                               â”‚
â”‚  â€¢ MultilÃ­ngue (PT, EN, ES, ZH)                                    â”‚
â”‚                                                                     â”‚
â”‚  ConfiguraÃ§Ãµes:                                                     â”‚
â”‚  â€¢ temperature: 0.7                                                 â”‚
â”‚  â€¢ top_p: 0.9                                                       â”‚
â”‚  â€¢ max_tokens: 512                                                  â”‚
â”‚  â€¢ num_ctx: 8192                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FONTES DE DADOS                                  â”‚
â”‚                                                                       â”‚
â”‚  â€¢ Wikipedia Simple English Dump (320 MB)                           â”‚
â”‚  â€¢ 100 artigos processados                                          â”‚
â”‚  â€¢ 1566 chunks vetorizados                                          â”‚
â”‚  â€¢ Wikipedia API (para expansÃ£o)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fluxo de Dados

#### 1ï¸âƒ£ **IngestÃ£o de Documentos**
```
Wikipedia Dump â†’ Parser â†’ LangChain TextSplitter â†’ SentenceTransformers
                                                            â†“
                                                      Embeddings (384d)
                                                            â†“
                                                    Qdrant Vector DB
```

#### 2ï¸âƒ£ **Busca SemÃ¢ntica**
```
Query do UsuÃ¡rio â†’ Embedding â†’ Qdrant Search â†’ Top K Documentos
                                                      â†“
                                              Ranking + Filtros
                                                      â†“
                                                 Resultados
```

#### 3ï¸âƒ£ **RAG (Retrieval-Augmented Generation)**
```
Pergunta â†’ Busca SemÃ¢ntica â†’ Top N Chunks â†’ Contexto
                                                â†“
                                        Prompt Engineering
                                                â†“
                                    Ollama (Qwen 2.5 7B)
                                                â†“
                                      Resposta + CitaÃ§Ãµes
```

### Stack TecnolÃ³gico

| Componente | Tecnologia | VersÃ£o | FunÃ§Ã£o |
|------------|------------|--------|--------|
| **API Framework** | FastAPI | 0.104+ | REST API assÃ­ncrona |
| **Vector Database** | Qdrant | 1.11.3 | Armazenamento vetorial |
| **LLM Server** | Ollama | 0.12.9 | Servidor de modelos |
| **LLM Model** | Qwen 2.5 | 7B | GeraÃ§Ã£o de respostas |
| **Embeddings** | SentenceTransformers | 2.3.0 | VetorizaÃ§Ã£o multilÃ­ngue |
| **Document Processing** | LangChain | 0.1.0+ | Pipeline de documentos |
| **Containerization** | Docker Compose | - | OrquestraÃ§Ã£o |
| **Language** | Python | 3.11+ | Runtime |

## ğŸš€ Quick Start

### PrÃ©-requisitos

- **Docker & Docker Compose** instalados
- **8GB+ RAM** disponÃ­vel
- **15GB+ espaÃ§o em disco** livre
- **Windows**, **Linux** ou **macOS**

### 1. Clone o RepositÃ³rio

```bash
git clone https://github.com/ekotuja-AI/dicionario_vetorial.git
cd dicionario_vetorial
```

### 2. Inicie os Containers

```bash
# Inicia todos os serviÃ§os
docker-compose up -d

# Monitore os logs
docker-compose logs -f
```

### 3. Aguarde InicializaÃ§Ã£o

O sistema irÃ¡:
- âœ… Inicializar Qdrant (banco vetorial)
- âœ… Baixar Qwen 2.5 7B (~4.7GB) - **primeira vez apenas**
- âœ… Carregar modelo de embeddings
- âœ… Configurar FastAPI

**Tempo estimado**: 5-10 minutos na primeira execuÃ§Ã£o

### 4. Acesse o Sistema

- ğŸŒ **Interface Web**: http://localhost:9000
- ğŸ“– **DocumentaÃ§Ã£o API**: http://localhost:9000/docs
- ğŸ“Š **Status**: http://localhost:9000/status
- ğŸ“ˆ **EstatÃ­sticas**: http://localhost:9000/estatisticas

## ğŸ“Š Dados Atuais

### Base de Conhecimento

- **Fonte**: Simple Wikipedia (inglÃªs simplificado)
- **Artigos**: 100 processados
- **Chunks**: 1566 vetorizados
- **DimensÃµes**: 384 (multilÃ­ngue)
- **Modelo Embedding**: paraphrase-multilingual-MiniLM-L12-v2

### Modelo LLM

- **Nome**: Qwen 2.5 (7B parÃ¢metros)
- **Tamanho**: 4.7 GB
- **QuantizaÃ§Ã£o**: Q4_0
- **Contexto**: 128K tokens
- **Capabilities**: Tool calling, RAG, MultilÃ­ngue
- **Idiomas**: PortuguÃªs, InglÃªs, Espanhol, ChinÃªs, +25 outros

## ğŸ”§ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente (.env)

```env
# Qdrant Vector Database
QDRANT_HOST=localhost
QDRANT_PORT=6333

# Ollama LLM Server
OLLAMA_HOST=localhost
OLLAMA_PORT=11434
LLM_MODEL=qwen2.5:7b
LLM_MAX_TOKENS=512
LLM_TEMPERATURE=0.7

# Embeddings Model
EMBEDDING_MODEL=paraphrase-multilingual-MiniLM-L12-v2

# Data Directories
DATA_DIR=./data
MODELS_DIR=./models
```

### Volumes Docker

| Volume | DescriÃ§Ã£o | Tamanho Aproximado |
|--------|-----------|-------------------|
| `qdrant_storage` | Dados do banco vetorial | ~500 MB |
| `ollama_models` | Modelos LLM | ~5 GB |
| `./data` | Dumps e cache | ~1 GB |

## ğŸ“š Uso da API

### Busca SemÃ¢ntica

```bash
curl -X POST "http://localhost:9000/buscar" \
  -H "Content-Type: application/json" \
  -d '{"query": "inteligÃªncia artificial", "limite": 5}'
```

**PowerShell:**
```powershell
$body = '{"query": "inteligÃªncia artificial", "limite": 5}'
Invoke-RestMethod -Uri "http://localhost:9000/buscar" `
  -Method Post -Body $body -ContentType "application/json"
```

### Perguntas com RAG

```bash
curl -X POST "http://localhost:9000/perguntar" \
  -H "Content-Type: application/json" \
  -d '{"pergunta": "O que Ã© Python?"}'
```

**PowerShell:**
```powershell
$body = '{"pergunta": "O que Ã© Python?"}'
Invoke-RestMethod -Uri "http://localhost:9000/perguntar" `
  -Method Post -Body $body -ContentType "application/json"
```

### Adicionar Artigos

```bash
curl -X POST "http://localhost:9000/adicionar" \
  -H "Content-Type: application/json" \
  -d '{"titulo": "Machine Learning"}'
```

## ğŸ¨ Interface Web

A interface web oferece:
- ğŸ” **Busca interativa** com resultados em tempo real
- ğŸ’¬ **Chat com IA** para perguntas complexas
- â• **Adicionar artigos** dinamicamente
- ğŸ“Š **EstatÃ­sticas** da base de conhecimento
- ğŸ¨ **Design moderno** com gradiente purple

## ğŸ“– Expandindo a Base

### Processar Dumps da Wikipedia

```bash
# 1. Baixar dump (executar dentro do container)
docker exec offline_wikipedia_app python scripts/download_wikipedia.py \
  --language simple \
  --max-articles 1000

# 2. O processamento Ã© automÃ¡tico com LangChain
```

### Adicionar Artigos EspecÃ­ficos via API

```python
import requests

# Lista de artigos para adicionar
artigos = [
    "Artificial Intelligence",
    "Machine Learning",
    "Deep Learning",
    "Natural Language Processing"
]

for titulo in artigos:
    response = requests.post(
        "http://localhost:9000/adicionar",
        json={"titulo": titulo}
    )
    print(f"{titulo}: {response.json()}")
```

## ğŸ³ Gerenciamento Docker

### Comandos Ãšteis

```bash
# Iniciar serviÃ§os
docker-compose up -d

# Parar serviÃ§os
docker-compose stop

# Reiniciar serviÃ§os
docker-compose restart

# Ver logs
docker-compose logs -f

# Ver logs de um serviÃ§o especÃ­fico
docker-compose logs -f app

# Remover tudo (incluindo volumes)
docker-compose down -v

# Rebuild apÃ³s mudanÃ§as
docker-compose up -d --build
```

### PowerShell Script (Windows)

O projeto inclui `docker-restart.ps1`:

```powershell
.\docker-restart.ps1
```

## ğŸ” Troubleshooting

### Problema: Container nÃ£o inicia

```bash
# Verificar logs
docker-compose logs app

# Reiniciar serviÃ§os
docker-compose restart
```

### Problema: Modelo Ollama nÃ£o encontrado

```bash
# Entrar no container Ollama
docker exec -it ollama_server bash

# Listar modelos
ollama list

# Baixar modelo manualmente
ollama pull qwen2.5:7b
```

### Problema: Embeddings nÃ£o funcionam

```bash
# Reinstalar dependÃªncias no container
docker exec offline_wikipedia_app pip install sentence-transformers==2.3.0 transformers==4.36.0

# Reiniciar container
docker-compose restart app
```

### Problema: Porta jÃ¡ em uso

Edite `docker-compose.yml` e altere as portas:

```yaml
ports:
  - "9001:9000"  # API na porta 9001
```

## ğŸ“ˆ Performance

### Benchmarks (Hardware mÃ©dio)

| OperaÃ§Ã£o | Tempo | Recursos |
|----------|-------|----------|
| **Busca SemÃ¢ntica** | ~200ms | CPU |
| **Gerar Resposta (Qwen 2.5)** | 10-30s | CPU/GPU |
| **Adicionar Artigo** | 2-5s | CPU |
| **Processar 100 artigos** | 5-10min | CPU |

### OtimizaÃ§Ãµes

- âœ… Usar GPU para inferÃªncia mais rÃ¡pida
- âœ… Aumentar `num_ctx` para contextos maiores
- âœ… Ajustar `chunk_size` para chunks menores/maiores
- âœ… Usar modelo menor (phi3:mini) se necessÃ¡rio velocidade

## ğŸ› ï¸ Desenvolvimento

### Estrutura de DiretÃ³rios

```
dicionario_vetorial/
â”œâ”€â”€ api/                      # FastAPI endpoints
â”‚   â”œâ”€â”€ config.py            # ConfiguraÃ§Ãµes
â”‚   â”œâ”€â”€ models.py            # Modelos Pydantic
â”‚   â””â”€â”€ wikipediaFuncionalAPI.py  # API principal
â”œâ”€â”€ services/                 # LÃ³gica de negÃ³cio
â”‚   â”œâ”€â”€ langchainWikipediaService.py  # LangChain integration
â”‚   â”œâ”€â”€ wikipediaOfflineService.py    # OrquestraÃ§Ã£o RAG
â”‚   â””â”€â”€ wikipediaDumpService.py       # Parser de dumps
â”œâ”€â”€ scripts/                  # UtilitÃ¡rios
â”‚   â””â”€â”€ download_wikipedia.py # Download de dumps
â”œâ”€â”€ static/                   # Interface web
â”‚   â””â”€â”€ index.html           # UI frontend
â”œâ”€â”€ data/                     # Dados e cache
â”œâ”€â”€ docker-compose.yml        # OrquestraÃ§Ã£o Docker
â”œâ”€â”€ Dockerfile               # Container da API
â”œâ”€â”€ requirements_minimal.txt  # DependÃªncias Python
â””â”€â”€ README.md                # Este arquivo
```

### Adicionar Novos Endpoints

1. Editar `api/wikipediaFuncionalAPI.py`
2. Adicionar modelo em `api/models.py` se necessÃ¡rio
3. Implementar lÃ³gica em `services/`
4. Testar via `/docs`

### Contribuindo

1. Fork o projeto
2. Crie uma branch (`git checkout -b feature/nova-funcionalidade`)
3. Commit suas mudanÃ§as (`git commit -am 'Adiciona nova funcionalidade'`)
4. Push para a branch (`git push origin feature/nova-funcionalidade`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.

## ğŸ™ Agradecimentos

- [Qwen Team](https://github.com/QwenLM/Qwen) - Modelo LLM excelente
- [Ollama](https://ollama.ai/) - Servidor LLM local
- [LangChain](https://python.langchain.com/) - Framework RAG
- [Qdrant](https://qdrant.tech/) - Banco vetorial
- [FastAPI](https://fastapi.tiangolo.com/) - Framework web
- [Wikipedia](https://www.wikipedia.org/) - Base de conhecimento

## ğŸ“ Suporte

- **Issues**: [GitHub Issues](https://github.com/ekotuja-AI/dicionario_vetorial/issues)
- **DiscussÃµes**: [GitHub Discussions](https://github.com/ekotuja-AI/dicionario_vetorial/discussions)
- **Email**: ekotuja@gmail.com

## ğŸ—ºï¸ Roadmap

- [ ] Suporte a mÃºltiplos idiomas da Wikipedia
- [ ] Interface de chat em tempo real (WebSocket)
- [ ] ExportaÃ§Ã£o de conversas
- [ ] Sistema de cache inteligente
- [ ] MÃ©tricas e analytics
- [ ] API de administraÃ§Ã£o
- [ ] Suporte a outros modelos LLM
- [ ] Fine-tuning do modelo em domÃ­nios especÃ­ficos
- [ ] Deploy em cloud (AWS/GCP/Azure)

---

<div align="center">

**Feito com â¤ï¸ usando Python, LangChain e Ollama**

[â¬† Voltar ao topo](#-sistema-rag-offline---wikipedia--langchain--ollama)

</div>
