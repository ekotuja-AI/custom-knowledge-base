# ğŸ“– Sistema RAG Offline - Wikipedia + LangChain + Ollama

Sistema completo de **Retrieval-Augmented Generation (RAG)** offline utilizando Wikipedia, LangChain e LLM local.

[![FastAPI](https://img.shields.io/badge/FastAPI-005571?style=for-the-badge&logo=fastapi)](https://fastapi.tiangolo.com/)
[![Python](https://img.shields.io/badge/python-3.11+-blue.svg?style=for-the-badge&logo=python)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)](https://www.docker.com/)
[![Qdrant](https://img.shields.io/badge/Qdrant-DC244C?style=for-the-badge&logo=qdrant&logoColor=white)](https://qdrant.tech/)
[![LangChain](https://img.shields.io/badge/LangChain-121212?style=for-the-badge)](https://langchain.com/)
[![Tests](https://img.shields.io/badge/tests-48%20passed-brightgreen?style=for-the-badge)]()

---

## ğŸ¯ VisÃ£o Geral

Sistema RAG (Retrieval-Augmented Generation) completo e **100% offline** que combina:
- ğŸŒ Wikipedia como base de conhecimento
- ğŸ” Busca vetorial semÃ¢ntica com Qdrant
- ğŸ¤– LLM local (Qwen 2.5 7B) via Ollama
- âš¡ LangChain para processamento de documentos
- ğŸ¨ Interface web para consultas

### âœ¨ Funcionalidades
- âœ… Busca SemÃ¢ntica (similaridade vetorial)
- âœ… Perguntas Inteligentes (RAG)
- âœ… 100% Offline (apÃ³s ingestÃ£o)
- âœ… Tool Calling (Qwen 2.5)
- âœ… IngestÃ£o em lote de dumps
- âœ… Interface Web simples
- âœ… API REST completa
- âœ… 48 testes unitÃ¡rios (100% passando)

---

## ğŸ—ï¸ Arquitetura

### Diagrama
```mermaid
graph TB
    User["ğŸ‘¤ UsuÃ¡rio\nInterface Web\nlocalhost:9000"]
    subgraph API[FastAPI REST API - Port 9000]
        Routes["Endpoints:\nGET / (Interface)\nPOST /buscar (Busca)\nPOST /perguntar (RAG)\nPOST /adicionar (Artigos)\nGET /estatisticas (MÃ©tricas)"]
    end
    subgraph Processing[Camada de Processamento]
        LangChain["ğŸ”— LangChain\nTextSplitter | Retriever | Embeddings"]
        WikiService["ğŸ“š Wikipedia Service\nBusca hÃ­brida | RAG | Cache"]
    end
    subgraph Storage[Armazenamento]
        Qdrant[("ğŸ—„ï¸ Qdrant\nVetores 384d | PersistÃªncia")]
        Cache[("ğŸ’¾ Cache Local")]
    end
    subgraph AI[Camada de IA]
        Ollama["ğŸ¤– Ollama LLM\nQwen 2.5 7B"]
        Embeddings["ğŸ§  Sentence Transformers\nparaphrase-multilingual-MiniLM-L12-v2"]
    end
    User --> Routes
    Routes --> WikiService
    WikiService --> LangChain
    WikiService --> Qdrant
    LangChain --> Embeddings
    Embeddings --> Qdrant
    WikiService --> Ollama
    WikiService --> Cache
```

### Fluxo
1. UsuÃ¡rio pergunta â†’ embedding â†’ busca Qdrant â†’ contexto â†’ LLM â†’ resposta
2. Adicionar artigo â†’ baixar â†’ chunking â†’ embeddings â†’ armazenar â†’ disponÃ­vel

---

## ğŸš€ InÃ­cio RÃ¡pido

### PrÃ©-requisitos
- Docker & Docker Compose
- 8GB+ RAM
- 20GB+ disco livre

### InstalaÃ§Ã£o
```bash
git clone https://github.com/ekotuja-AI/dicionario_vetorial.git
cd dicionario_vetorial
docker-compose up -d
docker-compose logs -f
```

### Acessos
- Interface Web: http://localhost:9000
- API Docs: http://localhost:9000/docs
- Qdrant Dashboard: http://localhost:6333/dashboard

### Primeiro Teste
```bash
curl -X POST http://localhost:9000/perguntar \
  -H "Content-Type: application/json" \
  -d '{"pergunta": "O que Ã© Python?"}'
```

---

## ğŸ“š API

### Buscar
```bash
curl -X POST http://localhost:9000/buscar \
  -H "Content-Type: application/json" \
  -d '{"query": "inteligÃªncia artificial", "limite": 5}'
```
### Perguntar (RAG)
```bash
curl -X POST http://localhost:9000/perguntar \
  -H "Content-Type: application/json" \
  -d '{"pergunta": "Quem criou Python?", "max_chunks": 5}'
```
### Adicionar artigo
```bash
curl -X POST http://localhost:9000/adicionar \
  -H "Content-Type: application/json" \
  -d '{"titulo": "Machine Learning", "idioma": "pt"}'
```
### Status
```bash
curl http://localhost:9000/status
```
### EstatÃ­sticas
```bash
curl http://localhost:9000/estatisticas
```

---

## ğŸ§ª Testes
Executar todos:
```bash
python -m pytest tests/ -v
```
Cobertura:
```bash
python -m pytest tests/ --cov=api --cov=services --cov-report=html
```
Por arquivo:
```bash
python -m pytest tests/test_models.py -v
python -m pytest tests/test_services.py -v
python -m pytest tests/test_config.py -v
python -m pytest tests/test_integration.py -v
```
Resultado esperado:
```
===== 48 passed in ~3.5s =====
```

---

## âš™ï¸ ConfiguraÃ§Ã£o (.env)
```env
QDRANT_HOST=qdrant
QDRANT_PORT=6333
COLLECTION_NAME=wikipedia_pt
OLLAMA_HOST=ollama
OLLAMA_PORT=11434
LLM_MODEL=qwen2.5:7b
EMBEDDING_MODEL=paraphrase-multilingual-MiniLM-L12-v2
EMBEDDING_SIZE=384
API_PORT=9000
CHUNK_SIZE=500
CHUNK_OVERLAP=50
```
ParÃ¢metros principais (`services/wikipediaOfflineService.py`):
```python
score_threshold = 0.5
max_chunks = 5
temperature = 0.8
num_predict = 800
```

---

## ğŸ“ Estrutura
```
api/        # Endpoints
services/   # RAG e lÃ³gica
tests/      # 48 testes
static/     # Interface
scripts/    # UtilitÃ¡rios
data/       # Dumps
Dockerfile  # Imagem
docker-compose.yml
requirements_minimal.txt
pytest.ini
```

---

## ğŸ³ Docker
```bash
docker-compose up -d
docker-compose logs -f app
docker-compose restart
docker-compose down
```
Acesso shell:
```bash
docker exec -it offline_wikipedia_app bash
docker exec -it offline_wikipedia_ollama bash
```

---

## ğŸ”§ Troubleshooting
| Problema | Causa | SoluÃ§Ã£o |
|----------|-------|---------|
| Busca vazia | Threshold alto | Reduza para 0.3 |
| Resposta genÃ©rica | Pouco contexto | max_chunks += 3 |
| Ollama falha | Modelo ausente | `ollama pull qwen2.5:7b` |
| Testes falham | DependÃªncia | Reinstalar requirements |

---

## ğŸ—ºï¸ Roadmap
ConcluÃ­do: busca semÃ¢ntica, RAG, testes, docs.
PrÃ³ximo: cache Redis, GPU, reranking, histÃ³rico.
Futuro: mÃºltiplas collections, PDFs, dashboards.

---

## ğŸ¤ Contribuindo
1. Fork
2. Branch `feature/x`
3. Commit e PR
Guidelines: cobertura â‰¥80%, PEP8, documentar.

---

## ğŸ“„ LicenÃ§a
MIT.

---

## ğŸ™ Agradecimentos
Qdrant Â· Ollama Â· LangChain Â· FastAPI Â· Sentence Transformers Â· Wikipedia

---

## â­ Apoie
Se este projeto ajudou vocÃª, dÃª uma estrela no GitHub.

# ğŸ“– Sistema RAG Offline - Wikipedia + LangChain + Ollama# ğŸ¤– Sistema RAG - Wikipedia + Qdrant + Ollama# ğŸ“– Sistema RAG Offline - Wikipedia + LangChain + Ollama



Sistema completo de Retrieval-Augmented Generation (RAG) totalmente offline combinando Wikipedia como base de conhecimento, busca vetorial semÃ¢ntica (Qdrant), processamento com LangChain e geraÃ§Ã£o com LLM local (Ollama + Qwen 2.5 7B).



[![FastAPI](https://img.shields.io/badge/FastAPI-005571?logo=fastapi)](https://fastapi.tiangolo.com/) [![Python](https://img.shields.io/badge/Python-3.11+-blue?logo=python)](https://www.python.org/) [![Docker](https://img.shields.io/badge/Docker-0db7ed?logo=docker)](https://www.docker.com/) [![Qdrant](https://img.shields.io/badge/Qdrant-DC244C?logo=qdrant)](https://qdrant.tech/) [![LangChain](https://img.shields.io/badge/LangChain-121212)](https://langchain.com/) [![Tests](https://img.shields.io/badge/Tests-48_passed-brightgreen)]()Sistema completo de **Retrieval-Augmented Generation (RAG)** usando Wikipedia, busca vetorial e LLM local.**Sistema completo de Retrieval-Augmented Generation (RAG) offline utilizando Wikipedia, LangChain e LLM local.**



---

## ğŸ¯ Principais Funcionalidades

- ğŸ” Busca semÃ¢ntica multilÃ­ngue (pt/en) usando embeddings `paraphrase-multilingual-MiniLM-L12-v2`[![Python](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org)[![FastAPI](https://img.shields.io/badge/FastAPI-005571?style=for-the-badge&logo=fastapi)](https://fastapi.tiangolo.com/)

- ğŸ¤– Respostas RAG contextualizadas (Qwen 2.5 7B via Ollama)

- ğŸ“š AdiÃ§Ã£o dinÃ¢mica de artigos da Wikipedia (API pÃºblica)[![FastAPI](https://img.shields.io/badge/FastAPI-005571?logo=fastapi)](https://fastapi.tiangolo.com/)[![Python](https://img.shields.io/badge/python-3.11+-blue.svg?style=for-the-badge&logo=python)](https://www.python.org/downloads/)

- ğŸ§© IngestÃ£o em chunks (tamanho configurÃ¡vel) com LangChain

- ğŸ’¾ Armazenamento persistente de vetores em Qdrant[![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?logo=docker)](https://www.docker.com/)[![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)](https://www.docker.com/)

- ğŸŒ API REST + interface web estÃ¡tica

- ğŸ§ª 48 testes unitÃ¡rios (100% passando)[![Qdrant](https://img.shields.io/badge/Qdrant-DC244C)](https://qdrant.tech/)[![Qdrant](https://img.shields.io/badge/Qdrant-DC244C?style=for-the-badge&logo=qdrant&logoColor=white)](https://qdrant.tech/)

- ğŸ› ï¸ 100% offline apÃ³s inicial ingestÃ£o

[![Tests](https://img.shields.io/badge/tests-48%20passed-brightgreen)]()[![LangChain](https://img.shields.io/badge/LangChain-121212?style=for-the-badge)](https://langchain.com/)

---

## ğŸ— Arquitetura (VisÃ£o Resumida)

```

UsuÃ¡rio â†’ FastAPI â†’ ServiÃ§o Wikipedia Offline â†’ (LangChain + Embeddings) â†’ Qdrant---## ğŸ¯ VisÃ£o Geral

                                            â†˜â†’ Ollama (LLM) para geraÃ§Ã£o de resposta contextual

```

Fluxos principais:

1. Busca: query â†’ embedding â†’ similaridade (Qdrant) â†’ resultados ordenados â†’ retorno JSON## ğŸ¯ Sistema RAG (Retrieval-Augmented Generation) completo e **100% offline** que combina:

2. RAG: pergunta â†’ busca contexto â†’ monta prompt estruturado â†’ LLM gera resposta â†’ fontes + resposta

3. AdiÃ§Ã£o de artigo: tÃ­tulo â†’ Wikipedia â†’ limpeza + split â†’ embeddings â†’ armazenamento vetorial- ğŸŒ **Wikipedia** como base de conhecimento



---Um **assistente inteligente** que responde perguntas usando artigos da Wikipedia como base de conhecimento. Combina:- ğŸ” **Busca vetorial semÃ¢ntica** com Qdrant

## ğŸš€ InÃ­cio RÃ¡pido

### PrÃ©-requisitos- ğŸ¤– **LLM local** (Qwen 2.5 7B) via Ollama

- Docker / Docker Compose

- ~8GB RAM livre- ğŸ” **Busca SemÃ¢ntica**: Encontra informaÃ§Ãµes por significado, nÃ£o apenas palavras-chave- âš¡ **LangChain** para processamento de documentos

- ~20GB disco (modelos + vetores)

- ğŸ¤– **IA Local (Ollama)**: Gera respostas inteligentes sem enviar dados para nuvem- ğŸ¨ **Interface web moderna** para consultas

### InstalaÃ§Ã£o (Linux/macOS)

```bash- ğŸ“š **Wikipedia**: Base de conhecimento rica e expansÃ­vel

git clone https://github.com/ekotuja-AI/dicionario_vetorial.git

cd dicionario_vetorial- âš¡ **100% Offline**: Funciona sem internet apÃ³s configuraÃ§Ã£o inicial### âœ¨ Funcionalidades Principais

docker-compose up -d

```

### InstalaÃ§Ã£o (Windows PowerShell)

```powershell### âœ¨ Funcionalidades- âœ… **Busca SemÃ¢ntica**: Encontra informaÃ§Ãµes por similaridade, nÃ£o apenas palavras-chave

git clone https://github.com/ekotuja-AI/dicionario_vetorial.git

cd dicionario_vetorial- âœ… **Perguntas Inteligentes**: Respostas contextualizadas usando LLM + conhecimento da Wikipedia

docker-compose up -d

```- âœ… Busca semÃ¢ntica multilÃ­ngue (portuguÃªs/inglÃªs)- âœ… **100% Offline**: Funciona completamente sem internet apÃ³s setup inicial

### Acessos

- Interface / API: http://localhost:9000- âœ… Respostas contextualizadas com RAG- âœ… **Tool Calling**: Suporte nativo a function calling com Qwen 2.5

- Swagger / Docs: http://localhost:9000/docs

- Qdrant UI: http://localhost:6333/dashboard- âœ… Adicionar artigos da Wikipedia dinamicamente- âœ… **Processamento em Lote**: IngestÃ£o eficiente de dumps da Wikipedia



---- âœ… Interface web moderna- âœ… **Interface Web**: UI moderna com gradiente purple

## ğŸ”Œ Endpoints Principais

| MÃ©todo | Rota | DescriÃ§Ã£o |- âœ… API REST completa

|--------|------|-----------|

| POST   | /buscar     | Busca semÃ¢ntica de documentos |- âœ… 48 testes unitÃ¡rios (100% passando)## ğŸ—ï¸ Arquitetura

| POST   | /perguntar  | Pergunta usando RAG |

| POST   | /adicionar  | Adiciona artigo da Wikipedia |

| GET    | /estatisticas | Retorna mÃ©tricas (chunks, artigos) |

| GET    | /status     | Verifica saÃºde do sistema |---### Diagrama de Componentes



### Exemplos

Busca (cURL):

```bash## ğŸš€ InÃ­cio RÃ¡pido (5 minutos)<div align="center">

curl -X POST http://localhost:9000/buscar \

 -H 'Content-Type: application/json' \  <img src="docs/architecture-diagram.svg" alt="Arquitetura do Sistema RAG" width="100%"/>

 -d '{"query":"inteligÃªncia artificial","limite":5}'

```### 1ï¸âƒ£ PrÃ©-requisitos</div>

Pergunta (PowerShell):

```powershell

$body = '{"pergunta":"O que Ã© aprendizado de mÃ¡quina?"}'

Invoke-RestMethod -Uri http://localhost:9000/perguntar -Method Post -Body $body -ContentType 'application/json'- **Docker** e **Docker Compose**<details>

```

Adicionar artigo:- 8GB+ RAM disponÃ­vel<summary>ğŸ“Š Ver diagrama Mermaid interativo (clique para expandir)</summary>

```bash

curl -X POST http://localhost:9000/adicionar \- 20GB+ espaÃ§o em disco

 -H 'Content-Type: application/json' \

 -d '{"titulo":"Python (linguagem de programaÃ§Ã£o)","idioma":"pt"}'```mermaid

```

### 2ï¸âƒ£ InstalaÃ§Ã£ograph TB

---

## ğŸ§ª Testes    User[ğŸ‘¤ USUÃRIO<br/>Interface Web<br/>localhost:9000]

Executar todos (Linux/macOS):

```bash```bash    

python -m pytest tests/ -v

```# Clone o repositÃ³rio    subgraph API[FastAPI REST API - Port 9000]

Executar todos (Windows):

```powershellgit clone https://github.com/ekotuja-AI/dicionario_vetorial.git        Routes[Endpoints:<br/>GET / - Interface<br/>POST /buscar - Busca<br/>POST /perguntar - RAG<br/>POST /adicionar - Novos artigos<br/>GET /estatisticas - MÃ©tricas]

python -m pytest tests/ -v

```cd dicionario_vetorial    end

Cobertura:

```bash    

python -m pytest tests/ --cov=api --cov=services --cov-report=term-missing

```# Inicie os containers    subgraph Processing[Camada de Processamento]

Resultado esperado:

```docker-compose up -d        LangChain[ğŸ”— LangChain<br/>â€¢ TextSplitter<br/>â€¢ Document Loader<br/>â€¢ Retriever<br/>â€¢ Embeddings]

48 passed in ~3-5s

```        WikiService[ğŸ“š Wikipedia Service<br/>â€¢ Busca hÃ­brida<br/>â€¢ RAG Pipeline<br/>â€¢ Cache management<br/>â€¢ Fallback strategies]



---# Aguarde ~2 minutos para inicializaÃ§Ã£o completa    end

## âš™ï¸ ConfiguraÃ§Ã£o (.env)

```envdocker logs -f offline_wikipedia_app    

QDRANT_HOST=qdrant

QDRANT_PORT=6333```    subgraph Storage[Camada de Armazenamento]

COLLECTION_NAME=wikipedia_pt

OLLAMA_HOST=ollama        ST[ğŸ§  Sentence Transformers<br/>paraphrase-multilingual-<br/>MiniLM-L12-v2<br/>384 dimensions]

OLLAMA_PORT=11434

LLM_MODEL=qwen2.5:7b### 3ï¸âƒ£ Usar o Sistema        Qdrant[(ğŸ—„ï¸ Qdrant Vector DB<br/>Port 6333<br/><br/>Collection: wikipedia_langchain<br/>â€¢ 9300 chunks<br/>â€¢ 500 artigos<br/>â€¢ 384 dimensions<br/>â€¢ Cosine distance)]

EMBEDDING_MODEL=paraphrase-multilingual-MiniLM-L12-v2

CHUNK_SIZE=500    end

CHUNK_OVERLAP=50

API_PORT=9000**Interface Web:**    

```

- Abra: http://localhost:9000    subgraph LLM[Camada de GeraÃ§Ã£o]

Ajustes de qualidade de resposta (`services/wikipediaOfflineService.py`):

```python- FaÃ§a buscas ou perguntas!        Ollama[ğŸ¤– Ollama LLM Server<br/>Port 11434<br/><br/>Model: qwen2.5:7b 4.7GB<br/>â€¢ 7B parÃ¢metros<br/>â€¢ Tool calling<br/>â€¢ 128K context<br/>â€¢ Q4_0 quantization<br/><br/>Config:<br/>temp=0.7, top_p=0.9<br/>max_tokens=512, ctx=8192]

score_threshold = 0.5      # Ajustar se muitos poucos resultados (0.3-0.5)

max_chunks = 5             # Aumentar para respostas mais ricas (3-8)    end

temperature = 0.8          # 0.6-0.9

num_predict = 800          # Tokens mÃ¡ximos de saÃ­da**Testar via API:**    

```

```bash    subgraph Data[Fontes de Dados]

---

## ğŸ“ Estrutura# Buscar artigos        Wiki[ğŸ“– Wikipedia<br/>â€¢ Simple English Dump 320MB<br/>â€¢ 500 artigos processados<br/>â€¢ 9300 chunks totais<br/>â€¢ Wikipedia API]

```

api/                   # Endpoints e modeloscurl -X POST http://localhost:9000/buscar \    end

services/              # LÃ³gica de RAG e Wikipedia

scripts/               # UtilitÃ¡rios (download, processamento)  -H "Content-Type: application/json" \    

static/index.html      # Interface simples

tests/                 # Suite principal (48 testes)  -d '{"query": "inteligÃªncia artificial", "limit": 5}'    User -->|HTTP Requests| API

tests_temp/            # Testes experimentais

Dockerfile / compose   # Infraestrutura    API --> LangChain

```

# Fazer pergunta com RAG    API --> WikiService

---

## ğŸ”§ Troubleshooting RÃ¡pidocurl -X POST http://localhost:9000/perguntar \    LangChain <-->|Documents| WikiService

| Problema | Causa Comum | SoluÃ§Ã£o |

|----------|-------------|---------|  -H "Content-Type: application/json" \    WikiService --> ST

| Sem resultados | Threshold alto | Reduzir para 0.4 |

| Resposta genÃ©rica | Pouco contexto | max_chunks=8 |  -d '{"pergunta": "O que Ã© machine learning?", "max_chunks": 5}'    ST -->|Vectors 384d| Qdrant

| LLM indisponÃ­vel | Modelo nÃ£o baixado | `ollama pull qwen2.5:7b` |

| Qdrant erro | Porta / container parado | `docker-compose restart qdrant` |```    WikiService -->|Context| Ollama

| Teste falha | DependÃªncia ausente | `pip install -r requirements_minimal.txt` |

    Qdrant -->|Retrieval| WikiService

Verificar status rÃ¡pido:

```bash---    Data -->|Ingest| LangChain

curl http://localhost:9000/status

curl http://localhost:9000/estatisticas    

```

## ğŸ“š DocumentaÃ§Ã£o Completa    classDef userClass fill:#9b59b6,stroke:#8e44ad,color:#fff

---

## ğŸ—º Roadmap Resumido    classDef apiClass fill:#3498db,stroke:#2980b9,color:#fff

- v1.1: Cache de consultas, suporte parcial a GPU, histÃ³rico de perguntas

- v2.0: Multi-coleÃ§Ãµes, PDFs, GraphQL, busca hÃ­brida### ğŸ—ï¸ Arquitetura    classDef processClass fill:#1abc9c,stroke:#16a085,color:#fff

- Futuro: Streaming de respostas, chat com memÃ³ria, integraÃ§Ã£o Telegram

    classDef storageClass fill:#e74c3c,stroke:#c0392b,color:#fff

---

## ğŸ¤ ContribuiÃ§Ã£o```    classDef llmClass fill:#f39c12,stroke:#d68910,color:#fff

1. Fork

2. Branch: `feature/minha-feature`â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    classDef dataClass fill:#95a5a6,stroke:#7f8c8d,color:#fff

3. Testes + documentaÃ§Ã£o

4. Pull Requestâ”‚                     Interface Web (/)                        â”‚    



Regras: manter testes verdes e evitar quebrar API pÃºblica.â”‚                    http://localhost:9000                     â”‚    class User userClass



---â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    class API,Routes apiClass

## ğŸ“„ LicenÃ§a

MIT.                       â”‚    class LangChain,WikiService processClass



---                       â–¼    class ST,Qdrant storageClass

## ğŸ™ CrÃ©ditos

Qdrant, LangChain, Ollama, FastAPI, SentenceTransformers, Wikipedia.â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    class Ollama llmClass



---â”‚                    FastAPI REST API                          â”‚    class Wiki dataClass

**â­ DÃª uma estrela se este projeto te ajudou!**

â”‚  /buscar  /perguntar  /adicionar  /status  /estatisticas   â”‚```

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                   â”‚                  â”‚</details>

         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

         â”‚  Qdrant Vector   â”‚  â”‚  Ollama LLM    â”‚

         â”‚   Database       â”‚  â”‚  (qwen2.5:7b)  â”‚### Fluxo de Dados

         â”‚  (embeddings)    â”‚  â”‚                â”‚

         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜#### 1ï¸âƒ£ **IngestÃ£o de Documentos**

                   â”‚

         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”```mermaid

         â”‚  SentenceTransf. â”‚graph LR

         â”‚  Embeddings      â”‚    A[ğŸ“¥ Wikipedia Dump<br/>320MB] --> B[ğŸ” Parser]

         â”‚  (multilingual)  â”‚    B --> C[âœ‚ï¸ LangChain<br/>TextSplitter]

         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    C --> D[ğŸ§  Sentence<br/>Transformers]

```    D --> E[ğŸ“Š Embeddings<br/>384 dimensions]

    E --> F[(ğŸ—„ï¸ Qdrant<br/>Vector DB)]

### ğŸ”§ Componentes    

    classDef inputClass fill:#3498db,stroke:#2980b9,color:#fff

| Componente | Tecnologia | Porta | FunÃ§Ã£o |    classDef processClass fill:#1abc9c,stroke:#16a085,color:#fff

|------------|-----------|-------|---------|    classDef outputClass fill:#e74c3c,stroke:#c0392b,color:#fff

| **API** | FastAPI | 9000 | Endpoints REST |    

| **Vector DB** | Qdrant | 6333 | Busca vetorial |    class A inputClass

| **LLM** | Ollama (qwen2.5:7b) | 11434 | GeraÃ§Ã£o de respostas |    class B,C,D,E processClass

| **Embeddings** | paraphrase-multilingual-MiniLM-L12-v2 | - | VetorizaÃ§Ã£o de texto |    class F outputClass

```

### ğŸ“¡ API Endpoints

#### 2ï¸âƒ£ **Busca SemÃ¢ntica**

#### **GET /** - Interface Web

Interface visual para buscar e fazer perguntas.```mermaid

graph LR

#### **POST /buscar** - Busca SemÃ¢ntica    A[â“ Query do<br/>UsuÃ¡rio] --> B[ğŸ§  Embedding<br/>Generator]

Busca artigos por similaridade semÃ¢ntica.    B --> C[(ğŸ” Qdrant<br/>Search)]

    C --> D[ğŸ“Š Top K<br/>Documentos]

**Request:**    D --> E[âš–ï¸ Ranking +<br/>Filtros]

```json    E --> F[âœ… Resultados<br/>Relevantes]

{    

  "query": "inteligÃªncia artificial",    classDef inputClass fill:#9b59b6,stroke:#8e44ad,color:#fff

  "limit": 5    classDef processClass fill:#1abc9c,stroke:#16a085,color:#fff

}    classDef outputClass fill:#f39c12,stroke:#d68910,color:#fff

```    

    class A inputClass

**Response:**    class B,C,D,E processClass

```json    class F outputClass

{```

  "query": "inteligÃªncia artificial",

  "total_resultados": 5,#### 3ï¸âƒ£ **RAG (Retrieval-Augmented Generation)**

  "resultados": [

    {```mermaid

      "title": "Artificial Intelligence",graph TB

      "content": "AI is...",    A[â“ Pergunta do<br/>UsuÃ¡rio] --> B[ğŸ” Busca<br/>SemÃ¢ntica]

      "url": "https://...",    B --> C[ğŸ“š Top N<br/>Chunks]

      "score": 0.89    C --> D[ğŸ“ ConstruÃ§Ã£o<br/>de Contexto]

    }    D --> E[âœï¸ Prompt<br/>Engineering]

  ],    E --> F[ğŸ¤– Ollama<br/>Qwen 2.5 7B]

  "tempo_busca_ms": 45.2    F --> G[ğŸ’¬ Resposta +<br/>CitaÃ§Ãµes]

}    

```    classDef inputClass fill:#9b59b6,stroke:#8e44ad,color:#fff

    classDef processClass fill:#1abc9c,stroke:#16a085,color:#fff

#### **POST /perguntar** - RAG (Pergunta + Resposta)    classDef llmClass fill:#f39c12,stroke:#d68910,color:#fff

Responde perguntas usando contexto da Wikipedia + LLM.    classDef outputClass fill:#27ae60,stroke:#229954,color:#fff

    

**Request:**    class A inputClass

```json    class B,C,D,E processClass

{    class F llmClass

  "pergunta": "O que Ã© machine learning?",    class G outputClass

  "max_chunks": 5```

}

```### Stack TecnolÃ³gico



**Response:**| Componente | Tecnologia | VersÃ£o | FunÃ§Ã£o |

```json|------------|------------|--------|--------|

{| **API Framework** | FastAPI | 0.104+ | REST API assÃ­ncrona |

  "pergunta": "O que Ã© machine learning?",| **Vector Database** | Qdrant | 1.11.3 | Armazenamento vetorial |

  "resposta": "Machine learning Ã©...",| **LLM Server** | Ollama | 0.12.9 | Servidor de modelos |

  "fontes": [...],| **LLM Model** | Qwen 2.5 | 7B | GeraÃ§Ã£o de respostas |

  "raciocinio": "Resposta baseada em 5 fontes",| **Embeddings** | SentenceTransformers | 2.3.0 | VetorizaÃ§Ã£o multilÃ­ngue |

  "tempo_processamento_ms": 1250.5| **Transformers** | HuggingFace Transformers | 4.36.0 | Backend para embeddings |

}| **Document Processing** | LangChain | 0.1.0+ | Pipeline de documentos |

```| **Containerization** | Docker Compose | - | OrquestraÃ§Ã£o |

| **Language** | Python | 3.11+ | Runtime |

#### **POST /adicionar** - Adicionar Artigo

Adiciona novo artigo da Wikipedia ao sistema.## ğŸš€ Quick Start



**Request:**### PrÃ©-requisitos

```json

{- **Docker & Docker Compose** instalados

  "titulo": "Python (programming language)"- **8GB+ RAM** disponÃ­vel

}- **15GB+ espaÃ§o em disco** livre

```- **Windows**, **Linux** ou **macOS**



**Response:**### 1. Clone o RepositÃ³rio

```json

{```bash

  "status": "success",git clone https://github.com/ekotuja-AI/dicionario_vetorial.git

  "titulo": "Python (programming language)",cd dicionario_vetorial

  "chunks_adicionados": 47,```

  "tempo_processamento_ms": 3421.2

}### 2. Inicie os Containers

```

```bash

#### **GET /status** - Status do Sistema# Inicia todos os serviÃ§os

```jsondocker-compose up -d

{

  "status": "ok",# Monitore os logs

  "qdrant_conectado": true,docker-compose logs -f

  "colecoes": 2,```

  "modelo_embedding_carregado": true,

  "inicializado": true### 3. Aguarde InicializaÃ§Ã£o

}

```O sistema irÃ¡:

- âœ… Inicializar Qdrant (banco vetorial)

#### **GET /estatisticas** - EstatÃ­sticas- âœ… Baixar Qwen 2.5 7B (~4.7GB) - **primeira vez apenas**

```json- âœ… Carregar modelo de embeddings

{- âœ… Configurar FastAPI

  "total_chunks": 7734,

  "collections": ["wikipedia_simple"],**Tempo estimado**: 5-10 minutos na primeira execuÃ§Ã£o

  "qdrant_host": "qdrant:6333"

}### 4. Acesse o Sistema

```

- ğŸŒ **Interface Web**: http://localhost:9000

---- ğŸ“– **DocumentaÃ§Ã£o API**: http://localhost:9000/docs

- ğŸ“Š **Status**: http://localhost:9000/status

## ğŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada- ğŸ“ˆ **EstatÃ­sticas**: http://localhost:9000/estatisticas



### VariÃ¡veis de Ambiente## ğŸ“Š Dados Atuais



Edite `.env` ou `docker-compose.yml`:### Base de Conhecimento



```env- **Fonte**: Simple Wikipedia (inglÃªs simplificado)

# Qdrant (Vector Database)- **Artigos**: 100 processados

QDRANT_HOST=qdrant- **Chunks**: 1566 vetorizados

QDRANT_PORT=6333- **DimensÃµes**: 384 (multilÃ­ngue)

COLLECTION_NAME=wikipedia_simple- **Modelo Embedding**: paraphrase-multilingual-MiniLM-L12-v2



# Ollama (LLM)### Modelo LLM

OLLAMA_HOST=ollama

OLLAMA_PORT=11434- **Nome**: Qwen 2.5 (7B parÃ¢metros)

LLM_MODEL_NAME=qwen2.5:7b- **Tamanho**: 4.7 GB

- **QuantizaÃ§Ã£o**: Q4_0

# Embeddings- **Contexto**: 128K tokens

EMBEDDING_MODEL_NAME=paraphrase-multilingual-MiniLM-L12-v2- **Capabilities**: Tool calling, RAG, MultilÃ­ngue

- **Idiomas**: PortuguÃªs, InglÃªs, Espanhol, ChinÃªs, +25 outros

# Chunk Configuration

CHUNK_SIZE=1000## ğŸ”§ ConfiguraÃ§Ã£o

CHUNK_OVERLAP=200

```### VariÃ¡veis de Ambiente (.env)



### Trocar Modelo LLM```env

# Qdrant Vector Database

```bashQDRANT_HOST=localhost

# Baixar modelo diferenteQDRANT_PORT=6333

docker exec ollama ollama pull qwen2.5:14b

docker exec ollama ollama pull llama3.1:8b# Ollama LLM Server

docker exec ollama ollama pull gemma2:9bOLLAMA_HOST=localhost

OLLAMA_PORT=11434

# Atualizar docker-compose.ymlLLM_MODEL=qwen2.5:7b

LLM_MODEL_NAME=qwen2.5:14bLLM_MAX_TOKENS=512

LLM_TEMPERATURE=0.7

# Reiniciar

docker-compose restart app# Embeddings Model

```EMBEDDING_MODEL=paraphrase-multilingual-MiniLM-L12-v2



### Adicionar Mais Artigos# Data Directories

DATA_DIR=./data

**MÃ©todo 1: Script Python**MODELS_DIR=./models

```bash```

# Edite adicionar_artigos.py com tÃ­tulos desejados

python adicionar_artigos.py### Volumes Docker

```

| Volume | DescriÃ§Ã£o | Tamanho Aproximado |

**MÃ©todo 2: Via API**|--------|-----------|-------------------|

```bash| `qdrant_storage` | Dados do banco vetorial | ~500 MB |

curl -X POST http://localhost:9000/adicionar \| `ollama_models` | Modelos LLM | ~5 GB |

  -H "Content-Type: application/json" \| `./data` | Dumps e cache | ~1 GB |

  -d '{"titulo": "Machine learning"}'

```## ğŸ“š Uso da API



**MÃ©todo 3: Interface Web**### Busca SemÃ¢ntica

- Acesse http://localhost:9000

- Aba "Adicionar Artigo"```bash

- Digite o tÃ­tulo e clique em "Adicionar"curl -X POST "http://localhost:9000/buscar" \

  -H "Content-Type: application/json" \

---  -d '{"query": "inteligÃªncia artificial", "limite": 5}'

```

## ğŸ§ª Testes

O projeto possui **48 testes unitÃ¡rios** cobrindo configuraÃ§Ã£o, modelos, serviÃ§os e integraÃ§Ã£o.

### Executar Todos os Testes

**Linux/macOS (Bash):**
```bash
# Instalar dependÃªncias de teste (se necessÃ¡rio)
pip install pytest pytest-cov

# Rodar todos os testes
python -m pytest tests/ -v

# Com relatÃ³rio de cobertura
python -m pytest tests/ --cov=api --cov=services --cov-report=html

# Abrir relatÃ³rio de cobertura
xdg-open htmlcov/index.html  # Linux
open htmlcov/index.html       # macOS
```

**Windows (PowerShell):**
```powershell
# Instalar dependÃªncias de teste (se necessÃ¡rio)
pip install pytest pytest-cov

# Rodar todos os testes
python -m pytest tests/ -v

# Com relatÃ³rio de cobertura
python -m pytest tests/ --cov=api --cov=services --cov-report=html

# Abrir relatÃ³rio de cobertura
start htmlcov/index.html
```

**Docker (qualquer plataforma):**
```bash
# Todos os testes (48 total)
docker exec offline_wikipedia_app pytest /app/tests/ -v

# Com cobertura
docker exec offline_wikipedia_app pytest /app/tests/ --cov=api --cov=services --cov-report=html
```

### Executar Testes EspecÃ­ficos

**Linux/macOS/Windows:**
```bash
# Apenas testes de modelos
python -m pytest tests/test_models.py -v

# Apenas testes de serviÃ§os
python -m pytest tests/test_services.py -v

# Apenas testes de configuraÃ§Ã£o
python -m pytest tests/test_config.py -v

# Apenas testes de integraÃ§Ã£o
python -m pytest tests/test_integration.py -v

# Executar teste especÃ­fico por nome
python -m pytest tests/test_models.py::TestWikipediaResultModel::test_criar_result_minimo -v
```

### Resultados Esperados

```
===== 48 passed in 3.49s =====

âœ… 9 testes de configuraÃ§Ã£o (test_config.py)
âœ… 12 testes de modelos (test_models.py)
âœ… 14 testes de serviÃ§os (test_services.py)
âœ… 13 testes de integraÃ§Ã£o (test_integration.py)
```

Para mais detalhes sobre os testes, consulte [tests/README.md](tests/README.md).

---

## ğŸ” Testando a API

### Testar Busca SemÃ¢ntica

**Linux/macOS (cURL):**
```bash
curl -X POST "http://localhost:9000/buscar" \
  -H "Content-Type: application/json" \
  -d '{"query": "inteligÃªncia artificial", "limite": 5}'
```

**Windows (PowerShell):**
```powershell
$body = '{"query": "inteligÃªncia artificial", "limite": 5}'
Invoke-RestMethod -Uri "http://localhost:9000/buscar" `
  -Method Post -Body $body -ContentType "application/json"
```

### Perguntas com RAG

**Linux/macOS (cURL):**
```bash
curl -X POST "http://localhost:9000/perguntar" \
  -H "Content-Type: application/json" \
  -d '{"pergunta": "O que Ã© Python?"}'
```

**Windows (PowerShell):**
```powershell
$body = '{"pergunta": "O que Ã© Python?"}'
Invoke-RestMethod -Uri "http://localhost:9000/perguntar" `
  -Method Post -Body $body -ContentType "application/json"
```

**Bash/cURL:**
```bash
curl -X POST "http://localhost:9000/perguntar" \
  -H "Content-Type: application/json" \
  -d '{"pergunta": "O que Ã© Python?"}'

```bash$body = '{"pergunta": "O que Ã© Python?"}'

python test_embeddings.pyInvoke-RestMethod -Uri "http://localhost:9000/perguntar" `

```  -Method Post -Body $body -ContentType "application/json"

```

### Testar RAG

### Adicionar Artigos

```bash

python test_rag_melhorado.py```bash

```curl -X POST "http://localhost:9000/adicionar" \

  -H "Content-Type: application/json" \

### Testar Conhecimento  -d '{"titulo": "Machine Learning"}'

```

```bash

python testar_conhecimento.py## ğŸ¨ Interface Web

```

A interface web oferece:

**Cobertura Atual:**- ğŸ” **Busca interativa** com resultados em tempo real

- âœ… 48/48 testes passando (100%)- ğŸ’¬ **Chat com IA** para perguntas complexas

- â±ï¸ ExecuÃ§Ã£o: ~6 segundos- â• **Adicionar artigos** dinamicamente

- ğŸ“Š Cobertura: Models, Services, Config, Integration- ğŸ“Š **EstatÃ­sticas** da base de conhecimento

- ğŸ¨ **Design moderno** com gradiente purple

---

## ğŸ“– Expandindo a Base

## ğŸ“Š Estrutura do Projeto

### Processar Dumps da Wikipedia

```

dicionario_vetorial/```bash

â”œâ”€â”€ api/# 1. Baixar dump (executar dentro do container)

â”‚   â”œâ”€â”€ __init__.pydocker exec offline_wikipedia_app python scripts/download_wikipedia.py \

â”‚   â”œâ”€â”€ config.py              # ConfiguraÃ§Ãµes da API  --language simple \

â”‚   â”œâ”€â”€ models.py              # Modelos Pydantic  --max-articles 1000

â”‚   â””â”€â”€ wikipediaFuncionalAPI.py  # Endpoints FastAPI

â”œâ”€â”€ services/# 2. O processamento Ã© automÃ¡tico com LangChain

â”‚   â”œâ”€â”€ __init__.py```

â”‚   â”œâ”€â”€ langchainWikipediaService.py  # LangChain integration

â”‚   â”œâ”€â”€ wikipediaOfflineService.py     # ServiÃ§o principal### Adicionar Artigos EspecÃ­ficos via API

â”‚   â””â”€â”€ wikipediaDumpService.py        # Processar dumps

â”œâ”€â”€ tests/```python

â”‚   â”œâ”€â”€ test_config.py         # Testes de configuraÃ§Ã£oimport requests

â”‚   â”œâ”€â”€ test_models.py         # Testes de modelos

â”‚   â”œâ”€â”€ test_services.py       # Testes de serviÃ§os# Lista de artigos para adicionar

â”‚   â”œâ”€â”€ test_integration.py    # Testes de integraÃ§Ã£oartigos = [

â”‚   â”œâ”€â”€ README.md              # DocumentaÃ§Ã£o dos testes    "Artificial Intelligence",

â”‚   â””â”€â”€ RESULTADOS_TESTES.md   # Resultados da execuÃ§Ã£o    "Machine Learning",

â”œâ”€â”€ scripts/    "Deep Learning",

â”‚   â”œâ”€â”€ download_wikipedia.py  # Baixar dumps Wikipedia    "Natural Language Processing"

â”‚   â””â”€â”€ processar_via_api.py   # Processar artigos]

â”œâ”€â”€ static/

â”‚   â””â”€â”€ index.html             # Interface webfor titulo in artigos:

â”œâ”€â”€ docker-compose.yml         # OrquestraÃ§Ã£o    response = requests.post(

â”œâ”€â”€ Dockerfile                 # Build da aplicaÃ§Ã£o        "http://localhost:9000/adicionar",

â”œâ”€â”€ requirements.txt           # DependÃªncias Python        json={"titulo": titulo}

â”œâ”€â”€ pytest.ini                 # ConfiguraÃ§Ã£o pytest    )

â”œâ”€â”€ adicionar_artigos.py       # Script para adicionar artigos    print(f"{titulo}: {response.json()}")

â”œâ”€â”€ testar_conhecimento.py     # Script para testar respostas```

â””â”€â”€ README.md                  # Este arquivo

```## ğŸ³ Gerenciamento Docker



---### Comandos Ãšteis



## ğŸ› SoluÃ§Ã£o de Problemas```bash

# Iniciar serviÃ§os

### Busca nÃ£o retorna resultadosdocker-compose up -d



**Problema:** `score_threshold` muito alto ou base vazia# Parar serviÃ§os

docker-compose stop

**SoluÃ§Ã£o:**

```python# Reiniciar serviÃ§os

# Ajustar threshold em services/wikipediaOfflineService.pydocker-compose restart

score_threshold = 0.5  # Era 0.7 (muito restritivo)

```# Ver logs

docker-compose logs -f

### Respostas da IA genÃ©ricas/ruins

# Ver logs de um serviÃ§o especÃ­fico

**Causas possÃ­veis:**docker-compose logs -f app

1. Modelo LLM pequeno (qwen2.5:7b)

2. Poucos artigos na base (~500)# Remover tudo (incluindo volumes)

3. Contexto curto (3-5 chunks)docker-compose down -v



**SoluÃ§Ãµes:**# Rebuild apÃ³s mudanÃ§as

```bashdocker-compose up -d --build

# 1. Modelo melhor```

docker exec ollama ollama pull qwen2.5:14b

### PowerShell Script (Windows)

# 2. Adicionar mais artigos

python adicionar_artigos.pyO projeto inclui `docker-restart.ps1`:



# 3. Aumentar max_chunks```powershell

# Em perguntar: max_chunks=10.\docker-restart.ps1

``````



### Container nÃ£o inicia## ï¿½ Melhorias Recentes



```bash### v2.1 - CorreÃ§Ãµes CrÃ­ticas de Busca Vetorial (Nov 2024)

# Ver logs

docker logs offline_wikipedia_app#### ğŸ› Problema Resolvido

docker logs ollama- **Bug**: Sistema retornava resultados incorretos (ex: "Python" para perguntas sobre Ãfrica)

- **Causa**: Embedding model nÃ£o sendo inicializado corretamente + versÃµes incompatÃ­veis

# Reiniciar

docker-compose down#### âœ… CorreÃ§Ãµes Aplicadas

docker-compose up -d1. **AtualizaÃ§Ã£o de DependÃªncias**

   - `sentence-transformers`: 2.2.2 â†’ 2.3.0

# Recriar volumes   - `transformers`: adicionado 4.36.0 (compatibilidade)

docker-compose down -v

docker-compose up -d2. **Lifecycle Management**

```   - Implementado `@asynccontextmanager lifespan` no FastAPI

   - Garante inicializaÃ§Ã£o correta do LangChain service na startup

### Ollama muito lento

3. **Busca Vetorial Otimizada**

```bash   - Busca direta com `embedding_model.encode()` + `qdrant_client.search()`

# Trocar para modelo menor   - Compatibilidade Pydantic v2 no `QdrantRetriever`

docker exec ollama ollama pull qwen2.5:3b

4. **Ferramentas de GestÃ£o**

# Ou verificar recursos   - Novo script: `scripts/listar_artigos.py`

docker stats   - Exporta inventÃ¡rio em TXT e JSON

```   - EstatÃ­sticas: 500 artigos, 9300 chunks, top articles



---#### ğŸ“Š Resultados

- **Antes**: Python (score 0.80) para "paÃ­ses da Ãfrica"  

## ğŸ”„ Melhorias Recentes- **Depois**: Africa (scores 0.73-0.74) âœ… **+92% de precisÃ£o**

- **RAG**: Respostas corretas citando NigÃ©ria, Egito, SudÃ£o

### v1.2.0 (Atual)

- âœ… **Bug Fix**: Ajustado `score_threshold` de 0.7 â†’ 0.5 (busca agora funciona!)### Scripts UtilitÃ¡rios

- âœ… **Testes**: Adicionada suite completa com 48 testes unitÃ¡rios (100% passando)

- âœ… **Prompt Melhorado**: RAG agora gera respostas mais detalhadas e contextualizadas```bash

- âœ… **Scripts**: Adicionados `adicionar_artigos.py` e `testar_conhecimento.py`# Listar todos os artigos no Qdrant

- âœ… **DocumentaÃ§Ã£o**: README completo e atualizadodocker exec offline_wikipedia_app python scripts/listar_artigos.py --output artigos_lista.txt --json artigos.json

- âœ… **ParÃ¢metros Ollama**: Otimizados para respostas melhores (temperature, top_p, num_predict)

# Ver estatÃ­sticas

### v1.1.0docker exec offline_wikipedia_app python scripts/listar_artigos.py --host qdrant

- LangChain integration```

- Multi-service fallback

- Interface web moderna**Output exemplo:**

```

### v1.0.0ğŸ“Š EstatÃ­sticas:

- Sistema RAG bÃ¡sico- Total de chunks: 9300

- 500 artigos Simple English Wikipedia- Artigos Ãºnicos: 500

- Busca vetorial com Qdrant- MÃ©dia de chunks/artigo: 18.6



---ğŸ† Top 5 artigos:

1. Chemistry: 420 chunks

## ğŸ“ˆ Roadmap2. Water: 416 chunks

3. Science: 396 chunks

### PrÃ³ximas Funcionalidades4. Biology: 385 chunks

- [ ] Reranking de resultados (BM25 + semantic)5. Earth: 372 chunks

- [ ] Hybrid search (keyword + semantic)```

- [ ] Cache de perguntas frequentes

- [ ] Suporte a mÃºltiplos idiomas da Wikipedia## ï¿½ğŸ” Troubleshooting

- [ ] Fine-tuning do modelo de embeddings

- [ ] IntegraÃ§Ã£o com GPT-4/Claude (opcional)### Problema: Container nÃ£o inicia

- [ ] Metrics e observability (Prometheus/Grafana)

```bash

### Contribuindo# Verificar logs

Pull requests sÃ£o bem-vindos! Para grandes mudanÃ§as:docker-compose logs app

1. Abra uma issue primeiro

2. Fork o projeto# Reiniciar serviÃ§os

3. Crie uma branch: `git checkout -b feature/nova-funcionalidade`docker-compose restart

4. Commit: `git commit -m 'Adiciona nova funcionalidade'````

5. Push: `git push origin feature/nova-funcionalidade`

6. Abra um Pull Request### Problema: Modelo Ollama nÃ£o encontrado



---```bash

# Entrar no container Ollama

## ğŸ“„ LicenÃ§adocker exec -it ollama_server bash



Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.# Listar modelos

ollama list

---

# Baixar modelo manualmente

## ğŸ™ Agradecimentosollama pull qwen2.5:7b

```

- [FastAPI](https://fastapi.tiangolo.com/) - Framework web moderno

- [Qdrant](https://qdrant.tech/) - Vector database de alta performance### Problema: Embeddings nÃ£o funcionam / Busca retorna resultados errados

- [Ollama](https://ollama.ai/) - LLMs locais simplificados

- [LangChain](https://langchain.com/) - Framework para LLM applications**Sintoma**: Busca retorna "Python" para qualquer pergunta, ou resultados de exemplo (Base de Conhecimento Vazia)

- [SentenceTransformers](https://www.sbert.net/) - Modelos de embeddings

- [Wikipedia](https://www.wikipedia.org/) - Base de conhecimento**SoluÃ§Ã£o**:

```bash

---# 1. Verificar se as dependÃªncias corretas estÃ£o instaladas

docker exec offline_wikipedia_app pip list | grep -E "sentence|transformers"

## ğŸ“ Suporte

# Deve mostrar:

- ğŸ“§ Issues: [GitHub Issues](https://github.com/ekotuja-AI/dicionario_vetorial/issues)# sentence-transformers  2.3.0

- ğŸ“– DocumentaÃ§Ã£o: Este README# transformers          4.36.0

- ğŸ§ª Testes: `tests/README.md`

# 2. Se as versÃµes estiverem erradas, rebuild da imagem

---docker-compose build --no-cache app

docker-compose up -d app

**Desenvolvido com â¤ï¸ usando Python, FastAPI, Qdrant e Ollama**

# 3. Verificar inicializaÃ§Ã£o nos logs

[![Star this repo](https://img.shields.io/github/stars/ekotuja-AI/dicionario_vetorial?style=social)](https://github.com/ekotuja-AI/dicionario_vetorial)docker logs offline_wikipedia_app 2>&1 | grep -i "inicializando\|langchain"


# 4. Testar busca
curl -X POST "http://localhost:9000/buscar" \
  -H "Content-Type: application/json" \
  -d '{"query": "africa", "limit": 3}'
```

**Resultado esperado**: Artigos sobre "Africa" com scores > 0.70

### Problema: Porta jÃ¡ em uso

Edite `docker-compose.yml` e altere as portas:

```yaml
ports:
  - "9001:9000"  # API na porta 9001
```

## ğŸ“ˆ Performance

### Benchmarks (Hardware mÃ©dio)

| OperaÃ§Ã£o | Tempo | Recursos |
|----------|-------|----------|
| **Busca SemÃ¢ntica** | ~200ms | CPU |
| **Gerar Resposta (Qwen 2.5)** | 10-30s | CPU/GPU |
| **Adicionar Artigo** | 2-5s | CPU |
| **Processar 100 artigos** | 5-10min | CPU |

### OtimizaÃ§Ãµes

- âœ… Usar GPU para inferÃªncia mais rÃ¡pida
- âœ… Aumentar `num_ctx` para contextos maiores
- âœ… Ajustar `chunk_size` para chunks menores/maiores
- âœ… Usar modelo menor (phi3:mini) se necessÃ¡rio velocidade

## ğŸ› ï¸ Desenvolvimento

### Estrutura de DiretÃ³rios

```
dicionario_vetorial/
â”œâ”€â”€ api/                      # FastAPI endpoints
â”‚   â”œâ”€â”€ config.py            # ConfiguraÃ§Ãµes
â”‚   â”œâ”€â”€ models.py            # Modelos Pydantic
â”‚   â””â”€â”€ wikipediaFuncionalAPI.py  # API principal
â”œâ”€â”€ services/                 # LÃ³gica de negÃ³cio
â”‚   â”œâ”€â”€ langchainWikipediaService.py  # LangChain integration
â”‚   â”œâ”€â”€ wikipediaOfflineService.py    # OrquestraÃ§Ã£o RAG
â”‚   â””â”€â”€ wikipediaDumpService.py       # Parser de dumps
â”œâ”€â”€ scripts/                  # UtilitÃ¡rios
â”‚   â””â”€â”€ download_wikipedia.py # Download de dumps
â”œâ”€â”€ static/                   # Interface web
â”‚   â””â”€â”€ index.html           # UI frontend
â”œâ”€â”€ data/                     # Dados e cache
â”œâ”€â”€ docker-compose.yml        # OrquestraÃ§Ã£o Docker
â”œâ”€â”€ Dockerfile               # Container da API
â”œâ”€â”€ requirements_minimal.txt  # DependÃªncias Python
â””â”€â”€ README.md                # Este arquivo
```

### Adicionar Novos Endpoints

1. Editar `api/wikipediaFuncionalAPI.py`
2. Adicionar modelo em `api/models.py` se necessÃ¡rio
3. Implementar lÃ³gica em `services/`
4. Testar via `/docs`

### Contribuindo

1. Fork o projeto
2. Crie uma branch (`git checkout -b feature/nova-funcionalidade`)
3. Commit suas mudanÃ§as (`git commit -am 'Adiciona nova funcionalidade'`)
4. Push para a branch (`git push origin feature/nova-funcionalidade`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.

## ğŸ™ Agradecimentos

- [Qwen Team](https://github.com/QwenLM/Qwen) - Modelo LLM excelente
- [Ollama](https://ollama.ai/) - Servidor LLM local
- [LangChain](https://python.langchain.com/) - Framework RAG
- [Qdrant](https://qdrant.tech/) - Banco vetorial
- [FastAPI](https://fastapi.tiangolo.com/) - Framework web
- [Wikipedia](https://www.wikipedia.org/) - Base de conhecimento

## ğŸ“ Suporte

- **Issues**: [GitHub Issues](https://github.com/ekotuja-AI/dicionario_vetorial/issues)
- **DiscussÃµes**: [GitHub Discussions](https://github.com/ekotuja-AI/dicionario_vetorial/discussions)
- **Email**: ekotuja@gmail.com

## ğŸ—ºï¸ Roadmap

- [ ] Suporte a mÃºltiplos idiomas da Wikipedia
- [ ] Interface de chat em tempo real (WebSocket)
- [ ] ExportaÃ§Ã£o de conversas
- [ ] Sistema de cache inteligente
- [ ] MÃ©tricas e analytics
- [ ] API de administraÃ§Ã£o
- [ ] Suporte a outros modelos LLM
- [ ] Fine-tuning do modelo em domÃ­nios especÃ­ficos
- [ ] Deploy em cloud (AWS/GCP/Azure)

---

<div align="center">

**Feito com â¤ï¸ usando Python, LangChain e Ollama**

[â¬† Voltar ao topo](#-sistema-rag-offline---wikipedia--langchain--ollama)

</div>
