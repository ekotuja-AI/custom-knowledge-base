"""
Wikipedia Offline Service - Vers√£o com LangChain

Vers√£o que integra Qdrant + Ollama + dados da Wikipedia com LangChain
"""

import os
import time
import logging
import requests
import uuid
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import json

# LangChain integration
from .langchainWikipediaService import langchain_wikipedia_service, WikipediaDocument

# Utilit√°rios
from .wikipedia_utils import (
    WikipediaAPIClient,
    TextProcessor,
    QdrantHelper,
    WikipediaDataValidator,
    MetricsCollector
)

try:
    from qdrant_client import QdrantClient
    from qdrant_client.http import models
    from qdrant_client.models import PointStruct
    QDRANT_AVAILABLE = True
except ImportError:
    QDRANT_AVAILABLE = False

try:
    import ollama
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False

logger = logging.getLogger(__name__)


@dataclass
class SearchResult:
    """Resultado de busca da Wikipedia"""
    title: str
    content: str
    url: str
    score: float
    categories: List[str] = None
    chunk_info: Dict[str, Any] = None

    def __post_init__(self):
        if self.categories is None:
            self.categories = []
        if self.chunk_info is None:
            self.chunk_info = {}


@dataclass
class RAGResponse:
    """Resposta RAG com LLM local"""
    question: str
    answer: str
    sources: List[SearchResult]
    reasoning: str
    model_info: Dict[str, str]
    total_chunks: Optional[int] = None
    total_artigos: Optional[int] = None
    telemetria: Optional[Dict[str, Any]] = None


class WikipediaOfflineService:
    """Servi√ßo Wikipedia offline funcional"""
    
    def __init__(self):
        self.client = None
        self.collection_name = "wikipedia_langchain"
        self.ollama_host = os.getenv("OLLAMA_HOST", "ollama")
        self.ollama_port = int(os.getenv("OLLAMA_PORT", "11434"))
        # For√ßar modelo LLM correto para portugu√™s
        self.model_name = os.getenv("LLM_MODEL", "phi3")
        self._initialized = False
        
        # Inicializar utilit√°rios
        self.api_client = WikipediaAPIClient()
        self.text_processor = TextProcessor()
        self.qdrant_helper = QdrantHelper()
        self.validator = WikipediaDataValidator()
        self.metrics = MetricsCollector()
        
    def inicializar(self):
        """Inicializa todos os componentes"""
        if self._initialized:
            return
            
        logger.info("üöÄ Inicializando Wikipedia Offline Service...")
        
        try:
            # Inicializar LangChain Service
            logger.info("üîó Inicializando LangChain Wikipedia Service...")
            langchain_wikipedia_service.inicializar()
            
            self._conectar_qdrant()
            self._criar_colecao_wikipedia()
            self._testar_ollama()
            
            self._initialized = True
            logger.info("‚úÖ Wikipedia Offline Service inicializado com LangChain!")
            
        except Exception as e:
            logger.error(f"‚ùå Erro na inicializa√ß√£o: {e}")
            # Em modo funcional, ainda inicializa mas marca problemas
            self._initialized = True
            logger.warning("‚ö†Ô∏è Iniciando em modo degradado")
    
    def _conectar_qdrant(self):
        """Conecta ao Qdrant"""
        if not QDRANT_AVAILABLE:
            logger.warning("‚ö†Ô∏è Qdrant n√£o dispon√≠vel")
            return
            
        host = os.getenv("QDRANT_HOST", "localhost")
        port = int(os.getenv("QDRANT_PORT", "6333"))
        
        try:
            self.client = QdrantClient(host=host, port=port)
            self.client.get_collections()
            logger.info(f"‚úÖ Conectado ao Qdrant em {host}:{port}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao conectar ao Qdrant: {e}")
            self.client = None
    
    def _criar_colecao_wikipedia(self):
        """Cria cole√ß√£o espec√≠fica para Wikipedia se n√£o existir"""
        if not self.client:
            return
            
        try:
            collections = self.client.get_collections()
            collection_names = [col.name for col in collections.collections]
            
            if self.collection_name not in collection_names:
                logger.info(f"üìö Criando cole√ß√£o: {self.collection_name}")
                
                self.client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=models.VectorParams(
                        size=384,  # Dimens√£o do SentenceTransformers
                        distance=models.Distance.COSINE
                    )
                )
                logger.info(f"‚úÖ Cole√ß√£o {self.collection_name} criada")
            else:
                logger.info(f"‚úÖ Cole√ß√£o {self.collection_name} j√° existe")
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao criar cole√ß√£o: {e}")
    
    def _testar_ollama(self):
        """Testa conex√£o com Ollama"""
        try:
            url = f"http://{self.ollama_host}:{self.ollama_port}/api/version"
            response = requests.get(url, timeout=5)
            if response.status_code == 200:
                logger.info(f"‚úÖ Ollama conectado (vers√£o: {response.json().get('version')})")
                
                # Testar se o modelo est√° dispon√≠vel
                models_url = f"http://{self.ollama_host}:{self.ollama_port}/api/tags"
                models_response = requests.get(models_url, timeout=5)
                if models_response.status_code == 200:
                    models_data = models_response.json()
                    available_models = [m['name'] for m in models_data.get('models', [])]
                    if self.model_name in available_models:
                        logger.info(f"‚úÖ Modelo {self.model_name} dispon√≠vel")
                    else:
                        logger.warning(f"‚ö†Ô∏è Modelo {self.model_name} n√£o encontrado. Dispon√≠veis: {available_models}")
            else:
                logger.warning(f"‚ö†Ô∏è Ollama respondeu com status: {response.status_code}")
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao conectar com Ollama: {e}")
    
    def adicionar_artigo_wikipedia(self, titulo: str) -> int:
        """Adiciona artigo da Wikipedia ao banco vetorial usando LangChain"""
        try:
            if not self._initialized:
                logger.error("‚ùå Servi√ßo n√£o inicializado")
                raise Exception("Servi√ßo n√£o inicializado")
            
            # Buscar artigo na Wikipedia
            logger.info(f"üìñ Buscando artigo: {titulo}")
            artigo = self._buscar_artigo_wikipedia(titulo)
            
            if not artigo:
                logger.warning(f"‚ö†Ô∏è Artigo '{titulo}' n√£o encontrado")
                return 0
            
            # Validar artigo
            if not self.validator.validar_artigo(artigo):
                logger.warning(f"‚ö†Ô∏è Artigo '{titulo}' n√£o passou na valida√ß√£o")
                return 0
            
            logger.info(f"‚úÖ Artigo encontrado: {artigo['title']}, extract length: {len(artigo.get('extract', ''))}, content length: {len(artigo.get('content', ''))}")
            
            # Usar LangChain para processamento
            logger.info(f"üîó Processando com LangChain: {titulo}")
            documento = WikipediaDocument(
                title=artigo['title'],
                content=artigo['content'],
                url=artigo['url'],
                metadata={
                    'source': 'wikipedia_api',
                    'language': 'pt',
                    'processed_at': time.strftime('%Y-%m-%d %H:%M:%S')
                }
            )
            
            # Ingerir usando LangChain service
            chunks_criados = langchain_wikipedia_service.ingerir_documentos([documento])
            logger.info(f"‚úÖ {chunks_criados} chunks criados com LangChain para '{titulo}'")
            
            # Registrar m√©trica
            self.metrics.record_article_processed(chunks_criados)
            
            # Tamb√©m adicionar ao sistema legado para compatibilidade
            chunks_legado = self._processar_e_armazenar_artigo(artigo)
            
            return chunks_criados
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao adicionar artigo '{titulo}': {e}")
            return 0
    
    def adicionar_artigos_com_langchain(self, titulos: List[str]) -> Dict[str, int]:
        """Adiciona m√∫ltiplos artigos usando pipeline LangChain"""
        try:
            logger.info(f"üîó Processando {len(titulos)} artigos com LangChain")
            
            documentos = []
            resultados = {}
            
            # Buscar todos os artigos
            for titulo in titulos:
                logger.info(f"üìñ Buscando: {titulo}")
                artigo = self._buscar_artigo_wikipedia(titulo)
                
                if artigo:
                    documento = WikipediaDocument(
                        title=artigo['title'],
                        content=artigo['content'], 
                        url=artigo['url'],
                        metadata={
                            'source': 'wikipedia_api',
                            'language': 'pt',
                            'processed_at': time.strftime('%Y-%m-%d %H:%M:%S')
                        }
                    )
                    documentos.append(documento)
                    resultados[titulo] = 0  # Ser√° atualizado depois
                else:
                    logger.warning(f"‚ö†Ô∏è Artigo n√£o encontrado: {titulo}")
                    resultados[titulo] = 0
            
            if documentos:
                # Ingest√£o em lote com LangChain
                total_chunks = langchain_wikipedia_service.ingerir_documentos(documentos)
                chunks_por_doc = total_chunks // len(documentos) if documentos else 0
                
                # Atualizar resultados
                for titulo in resultados:
                    if resultados[titulo] == 0 and any(d.title == titulo for d in documentos):
                        resultados[titulo] = chunks_por_doc
                
                logger.info(f"‚úÖ Ingest√£o LangChain completa: {total_chunks} chunks para {len(documentos)} documentos")
            
            return resultados
            
        except Exception as e:
            logger.error(f"‚ùå Erro na ingest√£o em lote: {e}")
            return {titulo: 0 for titulo in titulos}
    
    def adicionar_chunk_direto(self, chunk_data: Dict) -> bool:
        """Adiciona um chunk j√° processado diretamente ao banco vetorial - usa QdrantHelper"""
        try:
            # Validar chunk
            if not self.validator.validar_chunk(chunk_data):
                logger.error("‚ùå Chunk inv√°lido - campos obrigat√≥rios ausentes")
                return False
            
            # Preparar usando helper
            chunk_id = self.qdrant_helper.gerar_id_unico()
            payload = self.qdrant_helper.criar_payload_chunk(chunk_data)
            dummy_vector = self.qdrant_helper.criar_vetor_dummy()
            
            # Criar ponto para Qdrant
            point = PointStruct(
                id=chunk_id,
                vector=dummy_vector,
                payload=payload
            )
            
            # Inserir no Qdrant
            self.client.upsert(
                collection_name=self.collection_name,
                points=[point]
            )
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao adicionar chunk: {e}")
            return False
    
    def _buscar_artigo_wikipedia(self, titulo: str) -> Optional[Dict]:
        """Busca artigo na Wikipedia API com conte√∫do completo - delegado ao WikipediaAPIClient"""
        return self.api_client.buscar_artigo_completo(titulo)
    
    def _processar_e_armazenar_artigo(self, artigo: Dict) -> int:
        """Processa artigo em chunks e armazena no Qdrant"""
        if not self.client:
            return 0
            
        try:
            # Combinar extract e content
            texto_completo = f"{artigo['extract']} {artigo['content']}"
            
            # Dividir em chunks simples (por par√°grafos)
            chunks = self._dividir_em_chunks(texto_completo)
            
            points = []
            for i, chunk in enumerate(chunks):
                if len(chunk.strip()) < 50:  # Ignorar chunks muito pequenos
                    continue
                    
                # Para simplificar, vamos usar um vetor fake por enquanto
                # Em produ√ß√£o, usar√≠amos sentence-transformers aqui
                fake_vector = [0.1] * 384  # Vetor de 384 dimens√µes
                
                # Gerar ID √∫nico como UUID
                point_id = str(uuid.uuid4())
                
                point = models.PointStruct(
                    id=point_id,
                    vector=fake_vector,
                    payload={
                        "title": artigo['title'],
                        "content": chunk,
                        "url": artigo['url'],
                        "chunk_index": i,
                        "total_chunks": len(chunks),
                        "description": artigo.get('description', ''),
                        "source": "wikipedia"
                    }
                )
                points.append(point)
            
            if points:
                self.client.upsert(
                    collection_name=self.collection_name,
                    points=points
                )
                
            return len(points)
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao processar artigo: {e}")
            return 0
    
    def _dividir_em_chunks(self, texto: str, max_size: int = 1000) -> List[str]:
        """Divide texto em chunks por par√°grafos"""
        # Dividir por quebras de linha duplas (par√°grafos)
        paragrafos = texto.split('\n\n')
        chunks = []
        chunk_atual = ""
        
        for paragrafo in paragrafos:
            paragrafo = paragrafo.strip()
            if not paragrafo:
                continue
                
            if len(chunk_atual) + len(paragrafo) < max_size:
                chunk_atual += paragrafo + "\n\n"
            else:
                if chunk_atual:
                    chunks.append(chunk_atual.strip())
                chunk_atual = paragrafo + "\n\n"
        
        if chunk_atual:
            chunks.append(chunk_atual.strip())
            
        return chunks
    
    def buscar_artigos(self, query: str, limit: int = 10) -> List[SearchResult]:
        """Busca artigos usando LangChain retriever e fallback para sistema legado"""
        try:
            logger.info(f"üîç Buscando por: '{query}' (limite: {limit})")
            
            # Primeiro: tentar busca com LangChain
            try:
                logger.info("üîó Tentando busca com LangChain...")
                langchain_results = langchain_wikipedia_service.buscar_documentos(
                    query=query, 
                    limit=limit,
                    score_threshold=0.05  # Threshold muito baixo para aceitar mais resultados
                )
                
                if langchain_results:
                    logger.info(f"‚úÖ LangChain encontrou {len(langchain_results)} resultados")
                    return langchain_results
                else:
                    logger.info("‚ö†Ô∏è LangChain n√£o encontrou resultados, tentando sistema legado...")
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro na busca LangChain: {e}, usando sistema legado...")
            
            # Fallback: usar sistema legado
            return self._buscar_artigos_legado(query, limit)
            
        except Exception as e:
            logger.error(f"‚ùå Erro geral na busca: {e}")
            return self._get_sample_results(query, limit)
    
    def _buscar_artigos_legado(self, query: str, limit: int = 10) -> List[SearchResult]:
        """Sistema de busca legado (backup)"""
        if not self.client:
            return self._get_sample_results(query, limit)
            
        try:
            logger.info(f"üîç Busca legado por: '{query}' (limite: {limit})")
            
            # Estrat√©gia 1: Tentar busca por texto em m√∫ltiplos campos
            search_results = None
            query_terms = query.lower().split()
            
            # Primeiro: tentar MatchText no conte√∫do
            try:
                search_results = self.client.scroll(
                    collection_name=self.collection_name,
                    scroll_filter=models.Filter(
                        must=[
                            models.FieldCondition(
                                key="content",
                                match=models.MatchText(text=query)
                            )
                        ]
                    ),
                    limit=limit,
                    with_payload=True
                )
                logger.info(f"üìù Busca por content encontrou {len(search_results[0])} resultados")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro na busca por content: {e}")
            
            # Se n√£o encontrou resultados, tentar busca no t√≠tulo
            if not search_results or len(search_results[0]) == 0:
                try:
                    search_results = self.client.scroll(
                        collection_name=self.collection_name,
                        scroll_filter=models.Filter(
                            must=[
                                models.FieldCondition(
                                    key="title",
                                    match=models.MatchText(text=query)
                                )
                            ]
                        ),
                        limit=limit,
                        with_payload=True
                    )
                    logger.info(f"üìã Busca por title encontrou {len(search_results[0])} resultados")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erro na busca por title: {e}")
            
            # Se ainda n√£o encontrou, fazer busca mais ampla sem filtros e filtrar manualmente
            if not search_results or len(search_results[0]) == 0:
                logger.info("üîÑ Tentando busca manual em todos os documentos...")
                try:
                    # Buscar todos os documentos (limitado para n√£o sobrecarregar)
                    all_results = self.client.scroll(
                        collection_name=self.collection_name,
                        limit=500,  # Buscar at√© 500 documentos
                        with_payload=True
                    )
                    
                    logger.info(f"üìö Encontrou {len(all_results[0])} documentos totais")
                    
                    # Filtrar manualmente por palavras-chave
                    filtered_results = []
                    for hit in all_results[0]:
                        content = hit.payload.get("content", "").lower()
                        title = hit.payload.get("title", "").lower()
                        
                        # Verificar se algum termo da query est√° no t√≠tulo ou conte√∫do
                        score = 0
                        for term in query_terms:
                            if term in title:
                                score += 3  # T√≠tulo tem peso maior
                            if term in content:
                                score += 1  # Conte√∫do tem peso menor
                        
                        if score > 0:
                            filtered_results.append((hit, score))
                    
                    # Ordenar por score e pegar os melhores
                    filtered_results.sort(key=lambda x: x[1], reverse=True)
                    search_results = ([item[0] for item in filtered_results[:limit]], None)
                    
                    logger.info(f"‚úÖ Busca manual encontrou {len(search_results[0])} resultados relevantes")
                    
                except Exception as e:
                    logger.error(f"‚ùå Erro na busca manual: {e}")
                    search_results = ([], None)
            
            # Processar resultados encontrados e unificar por artigo
            artigo_dict = {}
            if search_results and len(search_results[0]) > 0:
                for hit in search_results[0]:
                    content = hit.payload.get("content", "")
                    title = hit.payload.get("title", "")
                    score = 0.5
                    for term in query_terms:
                        if term in title.lower():
                            score += 0.3
                        if term in content.lower():
                            score += 0.1
                    score = min(score, 1.0)
                    if title not in artigo_dict or score > artigo_dict[title].score:
                        artigo_dict[title] = SearchResult(
                            title=title,
                            content=content[:200] + ("..." if len(content) > 200 else ""),
                            url=hit.payload.get("url", ""),
                            score=score,
                            categories=[],
                            chunk_info={
                                "chunk_index": hit.payload.get("chunk_index", 0),
                                "total_chunks": hit.payload.get("total_chunks", 1),
                                "source": hit.payload.get("source", "unknown")
                            }
                        )
                resultados_unificados = list(artigo_dict.values())
                resultados_unificados = sorted(resultados_unificados, key=lambda x: x.score, reverse=True)[:limit]
                logger.info(f"‚úÖ Retornando {len(resultados_unificados)} artigos reais unificados")
                return resultados_unificados
            
            # Processar resultados encontrados
            results = []
            if search_results and len(search_results[0]) > 0:
                for hit in search_results[0]:
                    # Calcular score baseado na relev√¢ncia dos termos
                    content = hit.payload.get("content", "").lower()
                    title = hit.payload.get("title", "").lower()
                    
                    score = 0.5  # Score base
                    for term in query_terms:
                        if term in title:
                            score += 0.3
                        if term in content:
                            score += 0.1
                    
                    score = min(score, 1.0)  # Limitar a 1.0
                    
                    result = SearchResult(
                        title=hit.payload.get("title", ""),
                        content=hit.payload.get("content", ""),
                        url=hit.payload.get("url", ""),
                        score=score,
                        categories=[],
                        chunk_info={
                            "chunk_index": hit.payload.get("chunk_index", 0),
                            "total_chunks": hit.payload.get("total_chunks", 1),
                            "source": hit.payload.get("source", "unknown")
                        }
                    )
                    results.append(result)
                
                logger.info(f"‚úÖ Retornando {len(results)} resultados reais")
                return results
            
            # Se realmente n√£o encontrou nada, tentar busca de exemplo como √∫ltimo recurso
            logger.warning("‚ö†Ô∏è Nenhum resultado encontrado, usando fallback")
            return self._get_sample_results(query, limit)
                
        except Exception as e:
            logger.error(f"‚ùå Erro geral na busca: {e}")
            return self._get_sample_results(query, limit)
    
    def buscar_para_rag(self, pergunta: str, max_chunks: int = 30) -> tuple[List[SearchResult], int, int, bool, dict]:
        """
        Executa apenas a busca sem√¢ntica e retorna os resultados com telemetria.
        Retorna: (documentos, total_chunks, total_artigos, encontrou_resultados, telemetria_busca)
        """
        import time
        
        inicio_total = time.time()
        telemetria = {
            "tempo_embedding_ms": 0,
            "tempo_busca_qdrant_ms": 0,
            "tempo_processamento_ms": 0,
            "tempo_total_ms": 0,
            "query_length": len(pergunta),
            "max_chunks_solicitados": max_chunks
        }
        
        try:
            # Fase 1: Gerar embedding da query (simulado - LangChain faz internamente)
            inicio_busca = time.time()
            
            # Buscar documentos (inclui embedding + busca no Qdrant)
            documentos = self.buscar_artigos(pergunta, limit=max_chunks)
            
            tempo_busca = (time.time() - inicio_busca) * 1000
            telemetria["tempo_busca_qdrant_ms"] = round(tempo_busca, 2)
            
            # Fase 2: Processar resultados
            inicio_processamento = time.time()
            
            if not documentos or len(documentos) == 0:
                # Verificar se base est√° vazia
                if self.client:
                    try:
                        collection_info = self.client.get_collection(self.collection_name)
                        total_points = collection_info.points_count
                        
                        if total_points == 0:
                            logger.warning(f"‚ö†Ô∏è Base de conhecimento vazia!")
                            tempo_processamento = (time.time() - inicio_processamento) * 1000
                            telemetria["tempo_processamento_ms"] = round(tempo_processamento, 2)
                            telemetria["tempo_total_ms"] = round((time.time() - inicio_total) * 1000, 2)
                            return ([], 0, 0, False, telemetria)
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Erro ao verificar collection: {e}")
                
                logger.warning(f"‚ö†Ô∏è Nenhum artigo relevante encontrado")
                tempo_processamento = (time.time() - inicio_processamento) * 1000
                telemetria["tempo_processamento_ms"] = round(tempo_processamento, 2)
                telemetria["tempo_total_ms"] = round((time.time() - inicio_total) * 1000, 2)
                return ([], 0, 0, False, telemetria)
            
            # Calcular estat√≠sticas
            total_chunks = len(documentos)
            artigos_unicos = set(doc.title for doc in documentos)
            total_artigos = len(artigos_unicos)
            
            tempo_processamento = (time.time() - inicio_processamento) * 1000
            telemetria["tempo_processamento_ms"] = round(tempo_processamento, 2)
            telemetria["tempo_total_ms"] = round((time.time() - inicio_total) * 1000, 2)
            telemetria["chunks_encontrados"] = total_chunks
            telemetria["artigos_encontrados"] = total_artigos
            
            logger.info(f"üìä Busca encontrou {total_chunks} chunks de {total_artigos} artigos em {telemetria['tempo_total_ms']}ms")
            
            return (documentos, total_chunks, total_artigos, True, telemetria)
            
        except Exception as e:
            logger.error(f"‚ùå Erro na busca: {e}")
            telemetria["tempo_total_ms"] = round((time.time() - inicio_total) * 1000, 2)
            telemetria["erro"] = str(e)
            return ([], 0, 0, False, telemetria)
    
    def perguntar_com_rag(self, pergunta: str, max_chunks: int = 3) -> RAGResponse:
        """Sistema RAG com Ollama"""
        start_time = time.time()
        
        try:
            # DETEC√á√ÉO DE PERGUNTAS META (sobre o pr√≥prio sistema)
            pergunta_lower = pergunta.lower()
            
            # Perguntas sobre listar/contar artigos cadastrados
            termos_meta = [
                "quais artigos", "lista artigos", "artigos cadastrados", 
                "listar artigos", "liste os artigos", "me liste",
                "quantos artigos", "quantidade de artigos", "n√∫mero de artigos",
                "artigos temos", "artigos existem", "artigos dispon√≠veis",
                "artigos na base", "que artigos"
            ]
            
            if any(termo in pergunta_lower for termo in termos_meta):
                logger.info(f"üéØ Detectada pergunta META sobre listar artigos: '{pergunta}'")
                artigos_info = self.listar_todos_artigos()
                
                total = artigos_info.get("total", 0)
                if total > 0:
                    artigos = artigos_info.get("artigos", [])
                    lista_nomes = "\n".join([f"‚Ä¢ {artigo['title']}" for artigo in artigos])
                    resposta = f"Temos {total} artigo(s) cadastrado(s) na base de conhecimento:\n\n{lista_nomes}"
                else:
                    resposta = "Ainda n√£o h√° artigos cadastrados na base de conhecimento."
                
                return RAGResponse(
                    question=pergunta,
                    answer=resposta,
                    sources=[],
                    reasoning="Pergunta meta sobre o sistema - lista de artigos",
                    model_info={"status": "meta_query", "type": "list_articles"},
                    total_chunks=0,
                    total_artigos=total,
                    telemetria={}
                )
            
            # Fase 1: Buscar documentos (SEMPRE buscar primeiro)
            search_start = time.time()
            documentos = self.buscar_artigos(pergunta, limit=max_chunks)
            search_time = time.time() - search_start
            # Telemetria da busca sem√¢ntica
            documentos, total_chunks, total_artigos, encontrou_resultados, telemetria_busca = self.buscar_para_rag(pergunta, max_chunks)
            
            # Log para debug
            if documentos:
                logger.warning(f"üîç buscar_artigos retornou {len(documentos)} docs: {[(d.title, round(d.score, 4)) for d in documentos]}")
            else:
                logger.warning(f"üîç buscar_artigos retornou 0 documentos")
            
            # Se n√£o encontrou nada, verificar se √© porque a base est√° vazia ou se realmente n√£o tem o assunto
            if not documentos or len(documentos) == 0:
                try:
                    if self.client:
                        collection_info = self.client.get_collection(self.collection_name)
                        total_points = collection_info.points_count
                        
                        if total_points == 0:
                            logger.warning(f"‚ö†Ô∏è Base de conhecimento vazia!")
                            return RAGResponse(
                                question=pergunta,
                                answer="A base de conhecimento est√° vazia. Por favor, adicione artigos atrav√©s da interface web.",
                                sources=[],
                                reasoning="Base de conhecimento vazia",
                                model_info={"status": "empty_database", "model": self.model_name}
                            )
                        else:
                            logger.warning(f"‚ö†Ô∏è Nenhum artigo relevante encontrado (base tem {total_points} documentos)")
                            return RAGResponse(
                                question=pergunta,
                                answer="N√£o encontrei artigos sobre este assunto na base de conhecimento.",
                                sources=[],
                                reasoning=f"Sem artigos relevantes (base com {total_points} documentos)",
                                model_info={"status": "no_relevant_docs", "model": self.model_name}
                            )
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erro ao verificar collection: {e}")
                    return RAGResponse(
                        question=pergunta,
                        answer="N√£o encontrei artigos sobre este assunto na base de conhecimento.",
                        sources=[],
                        reasoning="Sem artigos relevantes",
                        model_info={"status": "no_docs", "model": self.model_name}
                    )
            
            # Verificar se h√° conte√∫do suficiente na base
            try:
                if self.client:
                    collection_info = self.client.get_collection(self.collection_name)
                    total_points = collection_info.points_count
                    
                    # Threshold adaptativo baseado no tamanho da base (REDUZIDO para aceitar mais resultados)
                    if total_points < 10:
                        MIN_SIMILARITY_SCORE = 0.05  # 5% para bases muito pequenas (< 10 docs)
                        logger.info(f"üìä Base pequena ({total_points} chunks) - usando threshold {MIN_SIMILARITY_SCORE}")
                    elif total_points < 50:
                        MIN_SIMILARITY_SCORE = 0.08  # 8% para bases pequenas (10-50 docs)
                        logger.info(f"üìä Base m√©dia ({total_points} chunks) - usando threshold {MIN_SIMILARITY_SCORE}")
                    else:
                        MIN_SIMILARITY_SCORE = 0.12  # 12% para bases grandes (50+ docs)
                        logger.info(f"üìä Base grande ({total_points} chunks) - usando threshold {MIN_SIMILARITY_SCORE}")
                else:
                    MIN_SIMILARITY_SCORE = 0.08
            except Exception as e:
                MIN_SIMILARITY_SCORE = 0.08
                logger.warning(f"Erro ao verificar tamanho da base: {e}")
            
            # Estrat√©gia 1: Aplicar boosting para matches exatos no t√≠tulo ANTES de filtrar
            import re
            stopwords = ['o', 'que', '√©', 'a', 'de', 'da', 'do', 'um', 'uma', 'os', 'as', 'para', 'com', 'por']
            termos_pergunta = [re.sub(r'[^\w\s]', '', t.lower()) for t in pergunta.split() if t.lower() not in stopwords]
            termos_pergunta = [t for t in termos_pergunta if len(t) > 2]
            
            # Aplicar boosting de 3x para matches exatos no t√≠tulo
            for doc in documentos:
                titulo_lower = doc.title.lower()
                # Se algum termo da pergunta √© exatamente o t√≠tulo (ou vice-versa)
                if termos_pergunta and any(termo == titulo_lower or titulo_lower in termos_pergunta for termo in termos_pergunta):
                    doc.score = doc.score * 3.0
                    logger.info(f"üöÄ Boosting aplicado: '{doc.title}' - score {doc.score/3.0:.4f} ‚Üí {doc.score:.4f}")
            
            # Reordenar documentos ap√≥s boosting
            documentos = sorted(documentos, key=lambda x: x.score, reverse=True)
            
            # Filtrar por score m√≠nimo de similaridade
            documentos_relevantes = [doc for doc in documentos if doc.score >= MIN_SIMILARITY_SCORE]
            
            logger.warning(f"üìä Ap√≥s filtro de score ({MIN_SIMILARITY_SCORE}): {len(documentos_relevantes)} docs - {[(d.title, round(d.score, 4)) for d in documentos_relevantes]}")
            
            # Estrat√©gia 2: Verificar se termos da pergunta aparecem no t√≠tulo ou conte√∫do
            # SEMPRE verificar termos exatos para evitar respostas inventadas
            if documentos_relevantes:
                # Extrair termos principais da pergunta (remover palavras comuns e caracteres especiais)
                import re
                import unicodedata
                
                def normalizar_texto(texto):
                    """Remove acentos e normaliza texto para compara√ß√£o"""
                    # Normalizar unicode (NFD = decompor caracteres com acentos)
                    texto_nfd = unicodedata.normalize('NFD', texto)
                    # Remover marcas diacr√≠ticas (acentos)
                    texto_sem_acento = ''.join(c for c in texto_nfd if unicodedata.category(c) != 'Mn')
                    return texto_sem_acento.lower()
                
                stopwords = ['o', 'que', '√©', 'a', 'de', 'da', 'do', 'um', 'uma', 'os', 'as', 'para', 'com', 'por', 'onde', 'fica', 'qual', 'sobre', 'sabe', 'vc', 'voc√™', 'me', 'diz', 'fala']
                # Remover pontua√ß√£o e caracteres especiais, manter apenas letras e n√∫meros
                termos_pergunta = [re.sub(r'[^\w\s]', '', t.lower()) for t in pergunta.split() if t.lower() not in stopwords]
                termos_pergunta = [t for t in termos_pergunta if len(t) > 2]  # Filtrar termos muito curtos
                
                if not termos_pergunta:
                    # Se n√£o h√° termos v√°lidos, aceitar os documentos com score alto
                    logger.info("‚ö†Ô∏è Nenhum termo v√°lido extra√≠do da pergunta, usando apenas scores")
                else:
                    # Verificar se pelo menos um termo aparece no t√≠tulo ou conte√∫do
                    docs_com_termo_exato = []
                    docs_score_alto = []  # Documentos com score alto mesmo sem match exato
                    logger.warning(f"üîÑ Iniciando verifica√ß√£o de {len(documentos_relevantes)} documentos")
                    for doc in documentos_relevantes:
                        logger.warning(f"  üîé Verificando documento: '{doc.title}' (score: {doc.score})")
                        titulo_normalizado = normalizar_texto(doc.title)
                        conteudo_normalizado = normalizar_texto(doc.content)
                        
                        # Verificar com word boundaries para evitar falsos positivos
                        tem_termo = False
                        for termo in termos_pergunta:
                            termo_normalizado = normalizar_texto(termo)
                            # Usar word boundaries (\b) para matches exatos de palavras
                            pattern = r'\b' + re.escape(termo_normalizado) + r'\b'
                            if re.search(pattern, titulo_normalizado) or re.search(pattern, conteudo_normalizado):
                                tem_termo = True
                                break
                        
                        # Verifica√ß√£o adicional: se t√≠tulo aparece na pergunta (match reverso)
                        titulo_palavras = [p for p in titulo_normalizado.split() if len(p) > 2]
                        pergunta_normalizada = normalizar_texto(pergunta)
                        titulo_na_pergunta = any(palavra in pergunta_normalizada for palavra in titulo_palavras)
                        
                        if tem_termo or titulo_na_pergunta:
                            docs_com_termo_exato.append(doc)
                            razao = "termos" if tem_termo else "t√≠tulo na pergunta"
                            logger.warning(f"‚úÖ Documento '{doc.title}' aceito ({razao}): {termos_pergunta}")
                        elif doc.score > 0.60:  # Score MUITO alto (60%+), aceitar mesmo sem match exato
                            docs_score_alto.append(doc)
                            logger.warning(f"‚úÖ Documento '{doc.title}' aceito (score muito alto: {doc.score:.4f})")
                        else:
                            logger.warning(f"‚ö†Ô∏è Documento '{doc.title}' n√£o cont√©m termos da pergunta {termos_pergunta} (score: {doc.score})")
                    
                    # Estrat√©gia 3: Combinar documentos com termo exato + scores altos
                    # Priorizar docs com termo exato, mas incluir todos os relevantes
                    documentos_relevantes = docs_com_termo_exato + docs_score_alto
                    
                    # Se n√£o encontrou NENHUM documento com termo exato E n√£o h√° scores muito altos, rejeitar
                    if not docs_com_termo_exato and not docs_score_alto:
                        logger.warning(f"‚ö†Ô∏è Nenhum documento relevante para '{pergunta}' (termos: {termos_pergunta}, sem scores altos)")
                        return RAGResponse(
                            question=pergunta,
                            answer="Ainda n√£o existem artigos sobre este assunto na base de conhecimento.",
                            sources=[],
                            reasoning="Sem artigos relevantes para a pergunta (sem matches de termo e scores baixos)",
                            model_info={"status": "no_match", "model": self.model_name}
                        )
            
            if not documentos_relevantes:
                logger.warning(f"‚ö†Ô∏è Nenhum artigo com similaridade suficiente para: {pergunta} (scores: {[doc.score for doc in documentos]})")
                return RAGResponse(
                    question=pergunta,
                    answer="Ainda n√£o existem artigos sobre este assunto na base de conhecimento.",
                    sources=[],
                    reasoning="Sem artigos relevantes com similaridade suficiente",
                    model_info={"status": "low_similarity", "model": self.model_name}
                )
            
            # N√ÉO remover duplicatas - permitir m√∫ltiplos chunks do mesmo artigo
            # Isso √© crucial para queries t√©cnicas onde informa√ß√£o espec√≠fica pode estar em chunks diferentes
            documentos = documentos_relevantes
            
            logger.info(f"üìö Encontrou {len(documentos)} chunks para RAG (artigos: {list(set([d.title for d in documentos]))})")

            
            # OTIMIZA√á√ÉO 1: Limitar n√∫mero de chunks (m√°ximo 6 - balanceado)
            # Priorizar chunks mais relevantes (j√° v√™m ordenados por score)
            documentos_limitados = documentos[:6]  # M√°ximo 6 chunks (aumentado de 3)
            if len(documentos) > 6:
                logger.info(f"‚ö° Limitando de {len(documentos)} para 6 chunks mais relevantes")
            
            # OTIMIZA√á√ÉO 2: Chunks de tamanho m√©dio (250 chars)
            context_parts = []
            for i, doc in enumerate(documentos_limitados, 1):
                # Balanceamento: mais contexto, mas ainda r√°pido
                content_snippet = doc.content[:250]  # Aumentado de 200 para 250
                if len(doc.content) > 250:
                    content_snippet += "..."
                
                context_parts.append(f"[{i}] {doc.title}:\n{content_snippet}")
            
            context = "\n\n".join(context_parts)
            logger.info(f"üìù Contexto preparado com {len(context)} caracteres de {len(documentos_limitados)} fontes")
            
            # Fase 2: Gerar resposta com Ollama
            logger.info(f"ü§ñ Chamando Ollama com modelo {self.model_name}...")
            generation_start = time.time()
            resposta, telemetria_llm = self._generate_answer_with_ollama(pergunta, context)
            generation_time = time.time() - generation_start
            total_time = time.time() - start_time
            
            # Calcular estat√≠sticas
            total_chunks = len(documentos)
            artigos_unicos = set(doc.title for doc in documentos)
            total_artigos = len(artigos_unicos)
            
            logger.info(f"‚úÖ Resposta gerada com sucesso ({len(resposta)} caracteres)")
            logger.info(f"üìä Estat√≠sticas - {total_chunks} chunks de {total_artigos} artigos √∫nicos")
            logger.info(f"‚è±Ô∏è Tempos - Busca: {search_time:.2f}s, Gera√ß√£o: {generation_time:.2f}s, Total: {total_time:.2f}s")
            
            # Montar telemetria detalhada no formato esperado pelo frontend
            telemetria = {
                "tempo_total_ms": round(total_time * 1000, 2),
                "tempo_busca_qdrant_ms": round(search_time * 1000, 2),
                "tempo_filtragem_ms": telemetria_busca.get("tempo_filtragem_ms", 0),
                "resultados_antes_filtro": telemetria_busca.get("resultados_antes_filtro", total_chunks),
                "resultados_depois_filtro": len(documentos),
                # Campos extras
                "chunks_encontrados": total_chunks,
                "artigos_encontrados": total_artigos,
                # LLM
                "llm": telemetria_llm
            }

            return RAGResponse(
                question=pergunta,
                answer=resposta,
                sources=documentos,
                reasoning=f"Resposta gerada com {self.model_name} baseada em {total_chunks} chunks de {total_artigos} artigos",
                model_info={
                    "status": "ok", 
                    "model": self.model_name,
                    "timing": {
                        "search_time": round(search_time, 2),
                        "generation_time": round(generation_time, 2),
                        "total_time": round(total_time, 2)
                    }
                },
                total_chunks=total_chunks,
                total_artigos=total_artigos,
                telemetria=telemetria
            )
            
        except Exception as e:
            logger.error(f"‚ùå Erro no RAG: {e}", exc_info=True)  # exc_info=True mostra traceback completo
            return RAGResponse(
                question=pergunta,
                answer=f"Erro ao processar pergunta: {str(e)}",
                sources=[],
                reasoning=f"Erro t√©cnico: {str(e)}",
                model_info={"status": "error", "model": "none", "error": str(e)}
            )
    
    def _generate_answer_with_ollama(self, question: str, context: str) -> str:
        """Gera resposta usando Ollama"""
        total_start = time.time()
        
        # OTIMIZA√á√ÉO 3: Contexto balanceado (1200 chars)
        max_context_length = 1200  # Aumentado de 800 para 1200
        if len(context) > max_context_length:
            context = context[:max_context_length] + "..."
            logger.info(f"üìù Contexto truncado para {max_context_length} caracteres")
        
        prompt_build_start = time.time()
        # OTIMIZA√á√ÉO 4: Prompt MUITO mais curto para reduzir tokens (895‚Üí~300)
        # Prompt sempre for√ßa resposta em portugu√™s
        prompt = f"""Responda em portugu√™s usando APENAS estas informa√ß√µes:\n\n{context}\n\nPergunta: {question}\n\nRegras: Responda direto sem mencionar 'artigos' ou 'contexto'. Se n√£o souber, diga 'N√£o encontrei informa√ß√µes'."""
        prompt_build_time = time.time() - prompt_build_start

        logger.info(f"ü§ñ Enviando prompt para Ollama (tamanho: {len(prompt)} caracteres)")
        logger.info(f"‚è±Ô∏è Tempo de prepara√ß√£o do prompt: {prompt_build_time*1000:.1f}ms")
        
        url = f"http://{self.ollama_host}:{self.ollama_port}/api/generate"
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.6,
                "num_predict": 150,      # Aumentado: 100‚Üí150 tokens (respostas mais completas)
                "num_ctx": 1536,         # Aumentado: 1024‚Üí1536 (meio termo)
                "repeat_penalty": 1.1,
                "top_k": 40,
                "stop": ["Pergunta:", "\n\n\n", "\n\nRegras:"]
            }
        }
        
        logger.info(f"‚è±Ô∏è Aguardando resposta do Ollama (modelo: {self.model_name}, timeout: 600s)...")
        logger.info(f"‚öôÔ∏è Config: temp={payload['options']['temperature']}, num_predict={payload['options']['num_predict']}, num_ctx={payload['options']['num_ctx']}")
        
        request_start = time.time()
        
        try:
            response = requests.post(url, json=payload, timeout=600)
            request_time = time.time() - request_start
            
            logger.info(f"üì° Ollama respondeu com status {response.status_code} em {request_time:.1f}s")
            
            if response.status_code == 200:
                parse_start = time.time()
                data = response.json()
                answer = data.get('response', 'Erro ao gerar resposta').strip()
                parse_time = time.time() - parse_start
                
                total_time = time.time() - total_start
                
                # Estat√≠sticas detalhadas do Ollama (se dispon√≠veis)
                eval_count = data.get('eval_count', 0)
                eval_duration = data.get('eval_duration', 0) / 1e9 if data.get('eval_duration') else 0
                prompt_eval_count = data.get('prompt_eval_count', 0)
                prompt_eval_duration = data.get('prompt_eval_duration', 0) / 1e9 if data.get('prompt_eval_duration') else 0
                
                # Criar telemetria estruturada
                telemetria = {
                    "prompt_tokens": prompt_eval_count,
                    "prompt_eval_time": round(prompt_eval_duration, 2),
                    "prompt_tokens_per_sec": round(prompt_eval_count / prompt_eval_duration, 1) if prompt_eval_duration > 0 else 0,
                    "completion_tokens": eval_count,
                    "completion_time": round(eval_duration, 2),
                    "completion_tokens_per_sec": round(eval_count / eval_duration, 1) if eval_duration > 0 else 0,
                    "total_tokens": prompt_eval_count + eval_count,
                    "total_time": round(request_time, 2),
                    "model": self.model_name,
                    "config": {
                        "temperature": 0.6,
                        "num_predict": 150,
                        "num_ctx": 1536
                    }
                }
                
                logger.info(f"‚úÖ Resposta gerada em {request_time:.1f}s (tamanho: {len(answer)} caracteres)")
                logger.info(f"üìä BREAKDOWN DETALHADO:")
                logger.info(f"   ‚îî‚îÄ Prepara√ß√£o prompt: {prompt_build_time*1000:.1f}ms")
                logger.info(f"   ‚îî‚îÄ Rede/Processamento: {request_time:.2f}s")
                if prompt_eval_count > 0:
                    logger.info(f"      ‚îú‚îÄ Avalia√ß√£o do prompt: {prompt_eval_duration:.2f}s ({prompt_eval_count} tokens, {prompt_eval_count/prompt_eval_duration:.0f} tok/s)")
                if eval_count > 0:
                    logger.info(f"      ‚îî‚îÄ Gera√ß√£o da resposta: {eval_duration:.2f}s ({eval_count} tokens, {eval_count/eval_duration:.0f} tok/s)")
                logger.info(f"   ‚îî‚îÄ Parse JSON: {parse_time*1000:.1f}ms")
                logger.info(f"   ‚îî‚îÄ Total _generate_answer: {total_time:.2f}s")
                
                return answer, telemetria
            else:
                error_text = response.text[:200] if response.text else "sem detalhes"
                logger.error(f"‚ùå Ollama erro {response.status_code}: {error_text}")
                return f"Erro: LLM respondeu com status {response.status_code}", {}
                
        except requests.exceptions.Timeout:
            logger.error(f"‚è∞ Timeout ao gerar resposta (>600s)")
            return "Timeout: A pergunta demorou muito para ser processada. Tente ser mais espec√≠fico.", {}
        except requests.exceptions.ConnectionError as e:
            logger.error(f"üîå Erro de conex√£o com Ollama: {e}")
            return "Erro: N√£o foi poss√≠vel conectar ao servi√ßo de LLM.", {}
        except Exception as e:
            logger.error(f"‚ùå Erro inesperado ao chamar Ollama: {e}", exc_info=True)
            return f"Erro ao gerar resposta: {str(e)}", {}
    
    def _get_sample_results(self, query: str, limit: int) -> List[SearchResult]:
        """Retorna lista vazia - n√£o usar samples hardcoded"""
        logger.warning(f"‚ö†Ô∏è Nenhum resultado encontrado para '{query}' - retornando lista vazia")
        return []
    
    def obter_estatisticas(self) -> Dict[str, Any]:
        """Estat√≠sticas da base Wikipedia"""
        try:
            if self.client:
                collection_info = self.client.get_collection(self.collection_name)
                return {
                    "sistema_offline": True,
                    "colecao": self.collection_name,
                    "total_chunks": collection_info.points_count,
                    "dimensoes_vetor": collection_info.config.params.vectors.size,
                    "distancia": collection_info.config.params.vectors.distance.value,
                    "modelo_llm": self.model_name,
                    "status": "funcional"
                }
            else:
                return {
                    "sistema_offline": True,
                    "colecao": "nao_conectado",
                    "total_chunks": 0,
                    "status": "modo_exemplo"
                }
        except Exception as e:
            return {"erro": f"Erro ao obter estat√≠sticas: {str(e)}"}
    
    def listar_todos_artigos(self) -> Dict[str, Any]:
        """Lista todos os artigos √∫nicos na base de conhecimento"""
        try:
            if not self.client:
                logger.error("‚ùå Cliente Qdrant n√£o inicializado")
                return {"artigos": [], "total": 0}
            
            # Buscar todos os pontos da cole√ß√£o LangChain
            all_points = []
            offset = None
            
            while True:
                result = self.client.scroll(
                    collection_name="wikipedia_langchain",
                    limit=100,
                    offset=offset,
                    with_payload=True,
                    with_vectors=False
                )
                
                points, offset = result
                all_points.extend(points)
                
                if offset is None:
                    break
            
            # Agrupar por t√≠tulo e pegar informa√ß√µes √∫nicas
            artigos_dict = {}
            for point in all_points:
                title = point.payload.get('title', 'Sem t√≠tulo')
                if title not in artigos_dict:
                    artigos_dict[title] = {
                        'title': title,
                        'url': point.payload.get('url', ''),
                        'chunks': 0,
                        'timestamp': point.payload.get('timestamp', ''),
                        'preview': point.payload.get('content', '')[:200]
                    }
                artigos_dict[title]['chunks'] += 1
            
            # Converter para lista e ordenar por t√≠tulo
            artigos_list = sorted(artigos_dict.values(), key=lambda x: x['title'].lower())
            
            logger.info(f"üìö Encontrados {len(artigos_list)} artigos √∫nicos ({len(all_points)} chunks total)")
            
            return {
                "artigos": artigos_list,
                "total": len(artigos_list),
                "total_chunks": len(all_points)
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao listar artigos: {e}")
            return {"artigos": [], "total": 0, "erro": str(e)}
    
    def limpar_colecao(self) -> bool:
        """Remove todos os pontos da cole√ß√£o"""
        try:
            if not self.client:
                logger.error("‚ùå Cliente Qdrant n√£o inicializado")
                return False
                
            # Deletar e recriar a cole√ß√£o
            try:
                self.client.delete_collection(self.collection_name)
                logger.info(f"üóëÔ∏è Cole√ß√£o {self.collection_name} removida")
            except Exception:
                logger.info(f"‚ö†Ô∏è Cole√ß√£o {self.collection_name} n√£o existia")
            
            # Recriar cole√ß√£o vazia
            self._criar_colecao()
            logger.info(f"‚úÖ Cole√ß√£o {self.collection_name} recriada vazia")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao limpar cole√ß√£o: {e}")
            return False
    
    def _processar_lote_chunks(self, chunk_batch: List[Dict]) -> int:
        """Processa um lote de chunks de dumps"""
        if not self.client or not chunk_batch:
            return 0
            
        try:
            points = []
            
            for chunk_data in chunk_batch:
                # Para simplificar, usar vetor fake (em produ√ß√£o seria embeddings reais)
                fake_vector = [0.1] * 384
                
                point_id = str(uuid.uuid4())
                
                point = models.PointStruct(
                    id=point_id,
                    vector=fake_vector,
                    payload={
                        "title": chunk_data['title'],
                        "content": chunk_data['content'], 
                        "url": chunk_data['url'],
                        "chunk_index": chunk_data.get('chunk_index', 0),
                        "total_chunks": chunk_data.get('total_chunks', 1),
                        "article_id": chunk_data.get('article_id', 0),
                        "timestamp": chunk_data.get('timestamp', ''),
                        "source": chunk_data.get('source', 'wikipedia_dump')
                    }
                )
                points.append(point)
            
            if points:
                self.client.upsert(
                    collection_name=self.collection_name,
                    points=points
                )
                logger.info(f"‚úÖ Lote processado: {len(points)} chunks adicionados")
            
            return len(points)
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao processar lote: {e}")
            return 0
    
    def verificar_status(self) -> Dict[str, Any]:
        """Status completo do sistema"""
        # Verifica n√∫mero de cole√ß√µes no Qdrant
        colecoes_count = 0
        if self.client:
            try:
                colecoes_count = len(self.client.get_collections().collections)
            except Exception:
                colecoes_count = 0

        # Campos obrigat√≥rios do modelo StatusResponse
        # Verificar status do Ollama
        ollama_disponivel = False
        modelo_llm = getattr(self, 'model_name', 'unknown')
        try:
            url = f"http://{getattr(self, 'ollama_host', 'localhost')}:{getattr(self, 'ollama_port', 11434)}/api/version"
            import requests
            resp = requests.get(url, timeout=3)
            if resp.status_code == 200:
                ollama_disponivel = True
        except Exception:
            ollama_disponivel = False

        return {
            "status": "ok" if self._initialized else "error",
            "qdrant_conectado": self.client is not None,
            "colecoes": colecoes_count,
            "modelo_embedding_carregado": True,  # ajuste conforme l√≥gica real
            "text_splitter_configurado": True,    # ajuste conforme l√≥gica real
            "openai_configurado": False,
            "inicializado": self._initialized,
            "ollama_disponivel": ollama_disponivel,
            "modelo_llm": modelo_llm
        }
    
    def obter_metricas(self) -> Dict:
        """Retorna m√©tricas coletadas pelo servi√ßo"""
        return self.metrics.get_metrics()
    
    def resetar_metricas(self):
        """Reseta todas as m√©tricas coletadas"""
        self.metrics.reset_metrics()
    
    def _test_ollama_connection(self) -> bool:
        """Testa se Ollama est√° respondendo"""
        try:
            url = f"http://{self.ollama_host}:{self.ollama_port}/api/version"
            response = requests.get(url, timeout=2)
            return response.status_code == 200
        except:
            return False


# Inst√¢ncia global do servi√ßo
wikipedia_offline_service = WikipediaOfflineService()