"""
Wikipedia Offline Service - Vers√£o com LangChain

Vers√£o que integra Qdrant + Ollama + dados da Wikipedia com LangChain
"""


import os
import time
import logging
import requests
import uuid
import datetime
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import json
import asyncio

# LangChain integration
from .langchainWikipediaService import langchain_wikipedia_service, WikipediaDocument

# Utilit√°rios
from .utils.wikipedia_utils import (
    WikipediaAPIClient,
    TextProcessor,
    QdrantHelper,
    WikipediaDataValidator,
    MetricsCollector
)
from api.telemetria_ws import enviar_telemetria

try:
    from qdrant_client import QdrantClient
    from qdrant_client.http import models
    from qdrant_client.models import PointStruct
    QDRANT_AVAILABLE = True
except ImportError:
    QDRANT_AVAILABLE = False

try:
    import ollama
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False

logger = logging.getLogger(__name__)


@dataclass
class SearchResult:
    """Resultado de busca da Wikipedia"""
    title: str
    content: str
    url: str
    score: float
    categories: List[str] = None
    chunk_info: Dict[str, Any] = None

    def __post_init__(self):
        if self.categories is None:
            self.categories = []
        if self.chunk_info is None:
            self.chunk_info = {}


@dataclass
class RAGResponse:
    """Resposta RAG com LLM local"""
    question: str
    answer: str
    sources: List[SearchResult]
    reasoning: str
    model_info: Dict[str, str]
    total_chunks: Optional[int] = None
    total_artigos: Optional[int] = None
    telemetria: Optional[Dict[str, Any]] = None


class WikipediaOfflineService:
    """Servi√ßo Wikipedia offline funcional"""
    def __init__(self):
        self.client = None
        self.collection_name = "wikipedia_langchain"
        self.ollama_host = os.getenv("OLLAMA_HOST", "ollama")
        self.ollama_port = int(os.getenv("OLLAMA_PORT", "11434"))
        # Modelos separados para embedding e LLM
        self.embedding_model_name = os.getenv("EMBEDDING_MODEL", "bge-large-pt")
        self.model_name = os.getenv("LLM_MODEL", "phi3")
        self._initialized = False
        
        # Inicializar utilit√°rios
        self.api_client = WikipediaAPIClient()
        self.text_processor = TextProcessor()
        self.qdrant_helper = QdrantHelper()
        self.validator = WikipediaDataValidator()
        self.metrics = MetricsCollector()
        
    def inicializar(self):
        """Inicializa todos os componentes"""
        if self._initialized:
            return
            
        logger.info("üöÄ Inicializando Wikipedia Offline Service...")
        
        try:
            # Inicializar LangChain Service
            logger.info("üîó Inicializando LangChain Wikipedia Service...")
            langchain_wikipedia_service.inicializar()
            
            self._conectar_qdrant()
            self._criar_colecao_wikipedia()
            self._testar_ollama()
            
            self._initialized = True
            logger.info("‚úÖ Wikipedia Offline Service inicializado com LangChain!")
            
        except Exception as e:
            logger.error(f"‚ùå Erro na inicializa√ß√£o: {e}")
            # Em modo funcional, ainda inicializa mas marca problemas
            self._initialized = True
            logger.warning("‚ö†Ô∏è Iniciando em modo degradado")
    
    def _conectar_qdrant(self):
        """Conecta ao Qdrant"""
        if not QDRANT_AVAILABLE:
            logger.warning("‚ö†Ô∏è Qdrant n√£o dispon√≠vel")
            return
            
        host = os.getenv("QDRANT_HOST", "localhost")
        port = int(os.getenv("QDRANT_PORT", "6333"))
        
        try:
            self.client = QdrantClient(host=host, port=port)
            self.client.get_collections()
            logger.info(f"‚úÖ Conectado ao Qdrant em {host}:{port}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao conectar ao Qdrant: {e}")
            self.client = None
    
    def _criar_colecao_wikipedia(self):
        """Cria cole√ß√£o espec√≠fica para Wikipedia se n√£o existir"""
        if not self.client:
            return
            
        try:
            collections = self.client.get_collections()
            collection_names = [col.name for col in collections.collections]
            
            if self.collection_name not in collection_names:
                logger.info(f"üìö Criando cole√ß√£o: {self.collection_name}")
                
                self.client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=models.VectorParams(
                        size=384,  # Dimens√£o do SentenceTransformers
                        distance=models.Distance.COSINE
                    )
                )
                logger.info(f"‚úÖ Cole√ß√£o {self.collection_name} criada")
            else:
                logger.info(f"‚úÖ Cole√ß√£o {self.collection_name} j√° existe")
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao criar cole√ß√£o: {e}")
    
    def _testar_ollama(self):
        """Testa conex√£o com Ollama"""
        try:
            url = f"http://{self.ollama_host}:{self.ollama_port}/api/version"
            response = requests.get(url, timeout=5)
            if response.status_code == 200:
                logger.info(f"‚úÖ Ollama conectado (vers√£o: {response.json().get('version')})")
                
                # Testar se o modelo est√° dispon√≠vel
                models_url = f"http://{self.ollama_host}:{self.ollama_port}/api/tags"
                models_response = requests.get(models_url, timeout=5)
                if models_response.status_code == 200:
                    models_data = models_response.json()
                    available_models = [m['name'] for m in models_data.get('models', [])]
                    if self.model_name in available_models:
                        logger.info(f"‚úÖ Modelo {self.model_name} dispon√≠vel")
                    else:
                        logger.warning(f"‚ö†Ô∏è Modelo {self.model_name} n√£o encontrado. Dispon√≠veis: {available_models}")
            else:
                logger.warning(f"‚ö†Ô∏è Ollama respondeu com status: {response.status_code}")
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao conectar com Ollama: {e}")
    
    def adicionar_artigo_wikipedia(self, titulo: str, colecao: str = None) -> int:
        """Adiciona artigo da Wikipedia ao banco vetorial usando LangChain na cole√ß√£o especificada"""
        try:
            if not self._initialized:
                logger.error("‚ùå Servi√ßo n√£o inicializado")
                raise Exception("Servi√ßo n√£o inicializado")
            
            logger.info(f"üîó Iniciando adi√ß√£o de artigo com LangChain")
            logger.info(f"üì•############## Cole√ß√£o: {colecao} adi√ß√£o do artigo  {titulo} com LangChain")

            # Buscar artigo na Wikipedia
            logger.info(f"üìñ Buscando artigo: {titulo}")
            artigo = self._buscar_artigo_wikipedia(titulo)
            
            if not artigo:
                logger.warning(f"‚ö†Ô∏è Artigo '{titulo}' n√£o encontrado")
                return 0
            
            # Validar artigo
            if not self.validator.validar_artigo(artigo):
                logger.warning(f"‚ö†Ô∏è Artigo '{titulo}' n√£o passou na valida√ß√£o")
                return 0
            
            logger.info(f"‚úÖ Artigo encontrado: {artigo['title']}, extract length: {len(artigo.get('extract', ''))}, content length: {len(artigo.get('content', ''))}")
            
            # Usar LangChain para processamento
            logger.info(f"üîó Processando com LangChain: {titulo}")
            documento = WikipediaDocument(
                title=artigo['title'],
                content=artigo['content'],
                url=artigo['url'],
                metadata={
                    'source': 'wikipedia_api',
                    'language': 'pt',
                    'processed_at': time.strftime('%Y-%m-%d %H:%M:%S')
                }
            )
            
            # Ingerir usando LangChain service
            chunks_criados = langchain_wikipedia_service.ingerir_documentos([documento],colecao=colecao)
            logger.info(f"‚úÖ {chunks_criados} chunks criados com LangChain para '{titulo}'")
            
            # Registrar m√©trica
            self.metrics.record_article_processed(chunks_criados)
            
            # Tamb√©m adicionar ao sistema legado para compatibilidade
            #chunks_legado = self._processar_e_armazenar_artigo(artigo, colecao=colecao)
            
            return chunks_criados
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao adicionar artigo '{titulo}': {e}")
            return 0
    
    def adicionar_artigos_com_langchain(self, titulos: List[str]) -> Dict[str, int]:
        """Adiciona m√∫ltiplos artigos usando pipeline LangChain"""
        try:
            logger.info(f"üîó Processando {len(titulos)} artigos com LangChain")
            logger.info("üì•#################################### Iniciando ingest√£o de m√∫ltiplos artigos com LangChain")
            
            documentos = []
            resultados = {}
            
            # Buscar todos os artigos
            for titulo in titulos:
                logger.info(f"üìñ Buscando: {titulo}")
                artigo = self._buscar_artigo_wikipedia(titulo)
                
                if artigo:
                    documento = WikipediaDocument(
                        title=artigo['title'],
                        content=artigo['content'], 
                        url=artigo['url'],
                        metadata={
                            'source': 'wikipedia_api',
                            'language': 'pt',
                            'processed_at': time.strftime('%Y-%m-%d %H:%M:%S')
                        }
                    )
                    documentos.append(documento)
                    resultados[titulo] = 0  # Ser√° atualizado depois
                else:
                    logger.warning(f"‚ö†Ô∏è Artigo n√£o encontrado: {titulo}")
                    resultados[titulo] = 0
            
            try:
                if not self._initialized:
                    logger.error("‚ùå Servi√ßo n√£o inicializado")
                    raise Exception("Servi√ßo n√£o inicializado")
                # Buscar artigo na Wikipedia
                logger.info(f"üìñ Buscando artigo: {titulo}")
                artigo = self._buscar_artigo_wikipedia(titulo)
                if not artigo:
                    logger.warning(f"‚ö†Ô∏è Artigo '{titulo}' n√£o encontrado")
                    return 0
                # Validar artigo
                if not self.validator.validar_artigo(artigo):
                    logger.warning(f"‚ö†Ô∏è Artigo '{titulo}' n√£o passou na valida√ß√£o")
                    return 0
                logger.info(f"‚úÖ Artigo encontrado: {artigo['title']}, extract length: {len(artigo.get('extract', ''))}, content length: {len(artigo.get('content', ''))}")
                # Usar LangChain para processamento
                logger.info(f"üîó Processando com LangChain: {titulo}")
                documento = WikipediaDocument(
                    title=artigo['title'],
                    content=artigo['content'],
                    url=artigo['url'],
                    metadata={
                        'source': 'wikipedia_api',
                        'language': 'pt',
                        'processed_at': time.strftime('%Y-%m-%d %H:%M:%S')
                    }
                )
                # Patch: set LangChain service collection if provided
                selected_colecao = colecao if colecao else self.collection_name
                if hasattr(langchain_wikipedia_service, 'collection_name'):
                    langchain_wikipedia_service.collection_name = selected_colecao
                chunks_criados = langchain_wikipedia_service.ingerir_documentos([documento])
                logger.info(f"‚úÖ {chunks_criados} chunks criados com LangChain para '{titulo}' na cole√ß√£o '{selected_colecao}'")
                # Registrar m√©trica
                self.metrics.record_article_processed(chunks_criados)
                # Tamb√©m adicionar ao sistema legado para compatibilidade
                chunks_legado = self._processar_e_armazenar_artigo(artigo, colecao=selected_colecao)
                return chunks_criados
            except Exception as e:
                logger.error(f"‚ùå Erro ao adicionar artigo '{titulo}': {e}")
                return 0
            # Inserir no Qdrant
            self.client.upsert(
                collection_name=self.collection_name,
                points=[point]
            )
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao adicionar chunk: {e}")
            return False
    
    def _buscar_artigo_wikipedia(self, titulo: str) -> Optional[Dict]:
        """Busca artigo na Wikipedia API com conte√∫do completo - delegado ao WikipediaAPIClient"""
        return self.api_client.buscar_artigo_completo(titulo)
    
    def _processar_e_armazenar_artigo(self, artigo: Dict, colecao: str = None) -> int:
        """Processa artigo em chunks e armazena no Qdrant na cole√ß√£o especificada"""
        if not self.client:
            return 0
        logger.info("üì•#################################### Iniciando processamento de artigo para armazenamento legado")
        logger.info(f"COLE√á√ÉO: {colecao if colecao else self.collection_name} | T√çTULO: {artigo.get('title', 'Desconhecido')}")
        try:
            # Combinar extract e content
            texto_completo = f"{artigo.get('extract', '')} {artigo.get('content', '')}"
            # Dividir em chunks simples (por par√°grafos)
            chunks = self._dividir_em_chunks(texto_completo)
            points = []
            collection_name = colecao if colecao else self.collection_name
            # Detectar dimens√£o do vetor da cole√ß√£o
            try:
                collection_info = self.client.get_collection(collection_name)
                if hasattr(collection_info, 'vectors_config'):
                    vectors_config = collection_info.vectors_config
                    if isinstance(vectors_config, dict):
                        vector_dim = vectors_config.get('size', vectors_config.get('params', {}).get('size', 384))
                    else:
                        vector_dim = getattr(vectors_config, 'size', 384)
                else:
                    vector_dim = 384
            except Exception as e:
                logger.warning(f"N√£o foi poss√≠vel detectar dimens√£o da cole√ß√£o '{collection_name}', usando 384. Erro: {e}")
                vector_dim = 384
            for i, chunk in enumerate(chunks):
                if len(chunk.strip()) < 50:  # Ignorar chunks muito pequenos
                    continue
                # Vetor fake com dimens√£o correta
                vector = [0.1] * int(vector_dim)
                # Gerar ID √∫nico como UUID
                point_id = str(uuid.uuid4())
                point = models.PointStruct(
                    id=point_id,
                    vector=vector,
                    payload={
                        "title": artigo.get('title', ''),
                        "content": chunk,
                        "url": artigo.get('url', ''),
                        "chunk_index": i,
                        "total_chunks": len(chunks),
                        "description": artigo.get('description', ''),
                        "source": "wikipedia"
                    }
                )
                points.append(point)
            if points:
                try:
                    self.client.upsert(
                        collection_name=collection_name,
                        points=points
                    )
                except Exception as e:
                    logger.error(f"‚ùå Erro ao inserir pontos no Qdrant: {e}")
                    return 0
            return len(points)
        except Exception as e:
            logger.error(f"‚ùå Erro ao processar artigo: {e}")
            return 0
    

    def buscar_artigos(self, query: str, limit: int = 10,colecao: str = None) -> List[SearchResult]:
        """Busca artigos usando LangChain retriever e fallback para sistema legado"""
        try:
            logger.info("******** buscar_artigos ******************************************************")
            logger.info(f"üîç Buscando por: '{query}' (limite: {limit}) na cole√ß√£o {colecao}")
            
            # Primeiro: tentar busca com LangChain
            try:
                logger.info("üîó Tentando busca com LangChain...")
                langchain_results = langchain_wikipedia_service.buscar_documentos(
                    query=query, 
                    limit=limit,
                    score_threshold=0.05,  # Threshold muito baixo para aceitar mais resultados
                    colecao=colecao 
                )
                
                if langchain_results:
                    logger.info(f"‚úÖ LangChain encontrou {len(langchain_results)} resultados")
                    return langchain_results
                else:
                    logger.info("‚ö†Ô∏è LangChain n√£o encontrou resultados, tentando sistema legado...")
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro na busca LangChain: {e}, usando sistema legado...")
            
            # Fallback: usar sistema legado
            logger.info("üîÑ Usando sistema de busca legado como fallback...")
            return self._buscar_artigos_legado(query, limit, colecao=colecao)
            
        except Exception as e:
            logger.error(f"‚ùå Erro geral na busca: {e}")
            return self._get_sample_results(query, limit)

    def _buscar_artigos_legado(self, query: str, limit: int = 10, colecao: str = None) -> List[SearchResult]:
        """Sistema de busca legado (backup) por cole√ß√£o"""
        if not self.client:
            return self._get_sample_results(query, limit)
        try:
            collection_name = colecao if colecao else self.collection_name
            logger.info(f"üîç Busca legado por: '{query}' (limite: {limit}) na cole√ß√£o: {collection_name}")
            search_results = None
            query_terms = query.lower().split()
            try:
                search_results = self.client.scroll(
                    collection_name=collection_name,
                    scroll_filter=models.Filter(
                        must=[
                            models.FieldCondition(
                                key="content",
                                match=models.MatchText(text=query)
                            )
                        ]
                    ),
                    limit=limit,
                    with_payload=True
                )
                logger.info(f"üìù Busca por content encontrou {len(search_results[0])} resultados")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro na busca por content: {e}")
            if not search_results or len(search_results[0]) == 0:
                try:
                    search_results = self.client.scroll(
                        collection_name=collection_name,
                        scroll_filter=models.Filter(
                            must=[
                                models.FieldCondition(
                                    key="title",
                                    match=models.MatchText(text=query)
                                )
                            ]
                        ),
                        limit=limit,
                        with_payload=True
                    )
                    logger.info(f"üìã Busca por title encontrou {len(search_results[0])} resultados")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erro na busca por title: {e}")
            if not search_results or len(search_results[0]) == 0:
                logger.info("üîÑ Tentando busca manual em todos os documentos...")
                try:
                    all_results = self.client.scroll(
                        collection_name=collection_name,
                        limit=500,
                        with_payload=True
                    )
                    logger.info(f"üìö Encontrou {len(all_results[0])} documentos totais")
                    filtered_results = []
                    for hit in all_results[0]:
                        content = hit.payload.get("content", "").lower()
                        title = hit.payload.get("title", "").lower()
                        score = 0
                        for term in query_terms:
                            if term in title:
                                score += 3
                            if term in content:
                                score += 1
                        if score > 0:
                            filtered_results.append((hit, score))
                    filtered_results.sort(key=lambda x: x[1], reverse=True)
                    search_results = ([item[0] for item in filtered_results[:limit]], None)
                    logger.info(f"‚úÖ Busca manual encontrou {len(search_results[0])} resultados relevantes")
                except Exception as e:
                    logger.error(f"‚ùå Erro na busca manual: {e}")
                    search_results = ([], None)
            artigo_dict = {}
            if search_results and len(search_results[0]) > 0:
                for hit in search_results[0]:
                    content = hit.payload.get("content", "")
                    title = hit.payload.get("title", "")
                    score = 0.5
                    for term in query_terms:
                        if term in title.lower():
                            score += 0.3
                        if term in content.lower():
                            score += 0.1
                    score = min(score, 1.0)
                    if title not in artigo_dict or score > artigo_dict[title].score:
                        artigo_dict[title] = SearchResult(
                            title=title,
                            content=content[:200] + ("..." if len(content) > 200 else ""),
                            url=hit.payload.get("url", ""),
                            score=score,
                            categories=[],
                            chunk_info={
                                "chunk_index": hit.payload.get("chunk_index", 0),
                                "total_chunks": hit.payload.get("total_chunks", 1),
                                "source": hit.payload.get("source", "unknown")
                            }
                        )
                resultados_unificados = list(artigo_dict.values())
                resultados_unificados = sorted(resultados_unificados, key=lambda x: x.score, reverse=True)[:limit]
                logger.info(f"‚úÖ Retornando {len(resultados_unificados)} artigos reais unificados")
                return resultados_unificados
            results = []
            if search_results and len(search_results[0]) > 0:
                for hit in search_results[0]:
                    content = hit.payload.get("content", "").lower()
                    title = hit.payload.get("title", "").lower()
                    score = 0.5
                    for term in query_terms:
                        if term in title:
                            score += 0.3
                        if term in content:
                            score += 0.1
                    score = min(score, 1.0)
                    result = SearchResult(
                        title=hit.payload.get("title", ""),
                        content=hit.payload.get("content", ""),
                        url=hit.payload.get("url", ""),
                        score=score,
                        categories=[],
                        chunk_info={
                            "chunk_index": hit.payload.get("chunk_index", 0),
                            "total_chunks": hit.payload.get("total_chunks", 1),
                            "source": hit.payload.get("source", "unknown")
                        }
                    )
                    results.append(result)
                logger.info(f"‚úÖ Retornando {len(results)} resultados reais")
                return results
            logger.warning("‚ö†Ô∏è Nenhum resultado encontrado, usando fallback")
            return self._get_sample_results(query, limit)
        except Exception as e:
            logger.error(f"‚ùå Erro geral na busca: {e}")
            return self._get_sample_results(query, limit)
    
    def buscar_para_rag(self, pergunta: str, max_chunks: int = 30, colecao: str = None) -> tuple[List[SearchResult], int, int, bool, dict]:
        """
        Executa apenas a busca sem√¢ntica e retorna os resultados com telemetria.
        Retorna: (documentos, total_chunks, total_artigos, encontrou_resultados, telemetria_busca)
        """
        import time
        
        inicio_total = time.time()
        telemetria = {
            "tempo_embedding_ms": 0,
            "tempo_busca_qdrant_ms": 0,
            "tempo_processamento_ms": 0,
            "tempo_total_ms": 0,
            "query_length": len(pergunta),
            "max_chunks_solicitados": max_chunks
        }
        
        try:
            # Fase 1: Gerar embedding da query (simulado - LangChain faz internamente)
            inicio_busca = time.time()
            
            # Buscar documentos (inclui embedding + busca no Qdrant)
            documentos = self.buscar_artigos(pergunta, limit=max_chunks, colecao=colecao)
            
            tempo_busca = (time.time() - inicio_busca) * 1000
            telemetria["tempo_busca_qdrant_ms"] = round(tempo_busca, 2)
            
            # Fase 2: Processar resultados
            inicio_processamento = time.time()
            
            if not documentos or len(documentos) == 0:
                # Verificar se base est√° vazia
                if self.client:
                    try:
                        collection_info = self.client.get_collection(self.collection_name)
                        total_points = collection_info.points_count
                        
                        if total_points == 0:
                            logger.warning(f"‚ö†Ô∏è Base de conhecimento vazia!")
                            tempo_processamento = (time.time() - inicio_processamento) * 1000
                            telemetria["tempo_processamento_ms"] = round(tempo_processamento, 2)
                            telemetria["tempo_total_ms"] = round((time.time() - inicio_total) * 1000, 2)
                            return ([], 0, 0, False, telemetria)
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Erro ao verificar collection: {e}")
                
                logger.warning(f"‚ö†Ô∏è Nenhum artigo relevante encontrado")
                tempo_processamento = (time.time() - inicio_processamento) * 1000
                telemetria["tempo_processamento_ms"] = round(tempo_processamento, 2)
                telemetria["tempo_total_ms"] = round((time.time() - inicio_total) * 1000, 2)
                return ([], 0, 0, False, telemetria)
            
            # Calcular estat√≠sticas
            total_chunks = len(documentos)
            artigos_unicos = set(doc.title for doc in documentos)
            total_artigos = len(artigos_unicos)
            
            tempo_processamento = (time.time() - inicio_processamento) * 1000
            telemetria["tempo_processamento_ms"] = round(tempo_processamento, 2)
            telemetria["tempo_total_ms"] = round((time.time() - inicio_total) * 1000, 2)
            telemetria["chunks_encontrados"] = total_chunks
            telemetria["artigos_encontrados"] = total_artigos
            
            logger.info(f"üìä Busca encontrou {total_chunks} chunks de {total_artigos} artigos em {telemetria['tempo_total_ms']}ms")
            
            return (documentos, total_chunks, total_artigos, True, telemetria)
            
        except Exception as e:
            logger.error(f"‚ùå Erro na busca: {e}")
            telemetria["tempo_total_ms"] = round((time.time() - inicio_total) * 1000, 2)
            telemetria["erro"] = str(e)
            return ([], 0, 0, False, telemetria)
    
    async def perguntar_com_rag(self, pergunta: str, max_chunks: int = 3, colecao: str = None) -> RAGResponse:
        """Sistema RAG com Ollama, filtrando por cole√ß√£o se fornecida"""
        start_time = time.time()
        start_time_str = datetime.datetime.fromtimestamp(start_time).strftime('%d/%m/%Y %H:%M:%S')
        await enviar_telemetria(f"Iniciando websocket... {start_time_str}")
        
        logger.info("///// perguntar_com_rag ///////////////////////////////////////////")
        logger.info(f"ü§ñ perguntar_com_rag '{pergunta}' (max_chunks={max_chunks}, colecao={colecao})")

        try:
            # ...aqui ser√° implementada a detec√ß√£o sem√¢ntica de perguntas meta via embeddings...
            
            # Fase 1: Buscar documentos (SEMPRE buscar primeiro)
            search_start = time.time()
            await enviar_telemetria(f"Buscando artigos no Qdrant({colecao})")
            await asyncio.sleep(0.5)
            documentos = self.buscar_artigos(pergunta, limit=max_chunks, colecao=colecao)
            search_time = time.time() - search_start
            # Telemetria da busca sem√¢ntica
            documentos, total_chunks, total_artigos, encontrou_resultados, telemetria_busca = self.buscar_para_rag(pergunta, max_chunks, colecao=colecao)
            
            # Log para debug
            if documentos:
                logger.warning(f"retornou {len(documentos)} docs: {[(d.title, round(d.score, 4)) for d in documentos]}")
                await enviar_telemetria(f" retornou {len(documentos)} documentos")
            else:
                logger.warning(f"üîç buscar_artigos retornou 0 documentos")
                await enviar_telemetria("buscar_artigos retornou 0 documentos")
            
            await enviar_telemetria("Iniciando busca................")
            await asyncio.sleep(0.5)
            
            # Se n√£o encontrou nada, verificar se √© porque a base est√° vazia ou se realmente n√£o tem o assunto
            if not documentos or len(documentos) == 0:
                try:
                    await enviar_telemetria("N√£o encontrou artigos na base")
                    if self.client:
                        collection_info = self.client.get_collection(self.collection_name)
                        total_points = collection_info.points_count
                        if total_points == 0:
                            logger.warning(f"‚ö†Ô∏è Base de conhecimento vazia!")
                            await enviar_telemetria("N√£o encontrou artigos na base")
                            return RAGResponse(
                                question=pergunta,
                                answer="A base de conhecimento est√° vazia. Por favor, adicione artigos atrav√©s da interface web.",
                                sources=[],
                                reasoning="Base de conhecimento vazia",
                                model_info={"status": "base_vazia", "modelo": self.model_name}
                            )
                        else:
                            logger.warning(f"‚ö†Ô∏è Nenhum artigo relevante encontrado (base tem {total_points} documentos)")
                            await enviar_telemetria("Nenhum artigo relevante encontrado na base")
                            return RAGResponse(
                                question=pergunta,
                                answer="N√£o encontrei artigos sobre este assunto na base de conhecimento.",
                                sources=[],
                                reasoning=f"Sem artigos relevantes (base com {total_points} documentos)",
                                model_info={"status": "sem_artigos_relevantes", "modelo": self.model_name}
                            )
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erro ao verificar collection: {e}")
                    await enviar_telemetria(f"Erro ao verificar collection: {e}")
                    return RAGResponse(
                        question=pergunta,
                        answer="N√£o encontrei artigos sobre este assunto na base de conhecimento.",
                        sources=[],
                        reasoning="Sem artigos relevantes",
                        model_info={"status": "sem_artigos", "modelo": self.model_name}
                    )
            
            # Verificar se h√° conte√∫do suficiente na base
            try:
                await enviar_telemetria("Verificar se h√° conte√∫do suficiente na base")
                await asyncio.sleep(0.5)
                if self.client:
                    collection_info = self.client.get_collection(self.collection_name)
                    total_points = collection_info.points_count
                    
                    # Threshold adaptativo baseado no tamanho da base (REDUZIDO para aceitar mais resultados)
                    if total_points < 10:
                        MIN_SIMILARITY_SCORE = 0.05  # 5% para bases muito pequenas (< 10 docs)
                        logger.info(f"üìä Base pequena ({total_points} chunks) - usando threshold {MIN_SIMILARITY_SCORE}")
                    elif total_points < 50:
                        MIN_SIMILARITY_SCORE = 0.08  # 8% para bases pequenas (10-50 docs)
                        logger.info(f"üìä Base m√©dia ({total_points} chunks) - usando threshold {MIN_SIMILARITY_SCORE}")
                    else:
                        MIN_SIMILARITY_SCORE = 0.12  # 12% para bases grandes (50+ docs)
                        logger.info(f"üìä Base grande ({total_points} chunks) - usando threshold {MIN_SIMILARITY_SCORE}")
                else:
                    MIN_SIMILARITY_SCORE = 0.08
            except Exception as e:
                MIN_SIMILARITY_SCORE = 0.08
                logger.warning(f"Erro ao verificar tamanho da base: {e}")
            
            # Estrat√©gia 1: Aplicar boosting para matches exatos no t√≠tulo ANTES de filtrar
            await asyncio.sleep(0.5)
            import re
            stopwords = ['o', 'que', '√©', 'a', 'de', 'da', 'do', 'um', 'uma', 'os', 'as', 'para', 'com', 'por']
            termos_pergunta = [re.sub(r'[^w\s]', '', t.lower()) for t in pergunta.split() if t.lower() not in stopwords]
            termos_pergunta = [t for t in termos_pergunta if len(t) > 2]  # Filtrar termos muito curtos
            # Se n√£o sobrou nenhum termo, tenta pegar a √∫ltima palavra relevante (ex: 'Jakarta' em 'o que √© Jakarta?')
            if not termos_pergunta:
                palavras = [re.sub(r'[^w\s]', '', t.lower()) for t in pergunta.split() if t.lower() not in stopwords]
                palavras = [t for t in palavras if t]
                if palavras:
                    termos_pergunta = [palavras[-1]]
            # Aplicar boosting de 3x para matches exatos no t√≠tulo
            titulos_pergunta = [t for t in termos_pergunta]
            for doc in documentos:
                titulo_lower = doc.title.lower()
                # Se algum termo da pergunta √© exatamente o t√≠tulo
                if any(titulo_lower == termo for termo in titulos_pergunta):
                    doc.score = doc.score * 3.0
                    doc._force_include = True  # Marcar para inclus√£o for√ßada
                    logger.info(f"üöÄ Boosting aplicado: '{doc.title}' - score {doc.score/3.0:.4f} ‚Üí {doc.score:.4f}")
            # Reordenar documentos ap√≥s boosting
            documentos = sorted(documentos, key=lambda x: x.score, reverse=True)
            # Filtrar por score m√≠nimo de similaridade OU inclus√£o for√ßada
            documentos_relevantes = [doc for doc in documentos if doc.score >= MIN_SIMILARITY_SCORE or getattr(doc, '_force_include', False)]
            logger.warning(f"üìä Ap√≥s filtro de score ({MIN_SIMILARITY_SCORE}): {len(documentos_relevantes)} docs - {[(d.title, round(d.score, 4)) for d in documentos_relevantes]}")
            
            # Estrat√©gia 2: Verificar se termos da pergunta aparecem no t√≠tulo ou conte√∫do
            # SEMPRE verificar termos exatos para evitar respostas inventadas
            if documentos_relevantes:
                # Extrair termos principais da pergunta (remover palavras comuns e caracteres especiais)
                import re
                import unicodedata
                
                await enviar_telemetria("Extrair termos principais da pergunta (remover palavras comuns e caracteres especiais")
                await asyncio.sleep(0.5)

                def normalizar_texto(texto):
                    """Remove acentos e normaliza texto para compara√ß√£o"""
                    # Normalizar unicode (NFD = decompor caracteres com acentos)
                    texto_nfd = unicodedata.normalize('NFD', texto)
                    # Remover marcas diacr√≠ticas (acentos)
                    texto_sem_acento = ''.join(c for c in texto_nfd if unicodedata.category(c) != 'Mn')
                    return texto_sem_acento.lower()
                
                stopwords = ['o', 'que', '√©', 'a', 'de', 'da', 'do', 'um', 'uma', 'os', 'as', 'para', 'com', 'por', 'onde', 'fica', 'qual', 'sobre', 'sabe', 'vc', 'voc√™', 'me', 'diz', 'fala']
                # Remover pontua√ß√£o e caracteres especiais, manter apenas letras e n√∫meros
                termos_pergunta = [re.sub(r'[^\w\s]', '', t.lower()) for t in pergunta.split() if t.lower() not in stopwords]
                termos_pergunta = [t for t in termos_pergunta if len(t) > 2]  # Filtrar termos muito curtos
                
                if not termos_pergunta:
                    # Se n√£o h√° termos v√°lidos, aceitar os documentos com score alto
                    logger.info("‚ö†Ô∏è Nenhum termo v√°lido extra√≠do da pergunta, usando apenas scores")
                else:
                    # Verificar se pelo menos um termo aparece no t√≠tulo ou conte√∫do
                    docs_com_termo_exato = []
                    docs_score_alto = []  # Documentos com score alto mesmo sem match exato
                    logger.warning(f"üîÑ Iniciando verifica√ß√£o de {len(documentos_relevantes)} documentos")
                    for doc in documentos_relevantes:
                        logger.warning(f"  üîé Verificando documento: '{doc.title}' (score: {doc.score})")
                        titulo_normalizado = normalizar_texto(doc.title)
                        conteudo_normalizado = normalizar_texto(doc.content)
                        
                        # Verificar com word boundaries para evitar falsos positivos
                        tem_termo = False
                        for termo in termos_pergunta:
                            termo_normalizado = normalizar_texto(termo)
                            # Usar word boundaries (\b) para matches exatos de palavras
                            pattern = r'\b' + re.escape(termo_normalizado) + r'\b'
                            if re.search(pattern, titulo_normalizado) or re.search(pattern, conteudo_normalizado):
                                tem_termo = True
                                break
                        
                        # Verifica√ß√£o adicional: se t√≠tulo aparece na pergunta (match reverso)
                        titulo_palavras = [p for p in titulo_normalizado.split() if len(p) > 2]
                        pergunta_normalizada = normalizar_texto(pergunta)
                        titulo_na_pergunta = any(palavra in pergunta_normalizada for palavra in titulo_palavras)
                        
                        if tem_termo or titulo_na_pergunta:
                            docs_com_termo_exato.append(doc)
                            razao = "termos" if tem_termo else "t√≠tulo na pergunta"
                            logger.warning(f"‚úÖ Documento '{doc.title}' aceito ({razao}): {termos_pergunta}")
                        elif doc.score > 0.60:  # Score MUITO alto (60%+), aceitar mesmo sem match exato
                            docs_score_alto.append(doc)
                            logger.warning(f"‚úÖ Documento '{doc.title}' aceito (score muito alto: {doc.score:.4f})")
                        else:
                            logger.warning(f"‚ö†Ô∏è Documento '{doc.title}' n√£o cont√©m termos da pergunta {termos_pergunta} (score: {doc.score})")
                    
                    # Estrat√©gia 3: Combinar documentos com termo exato + scores altos
                    # Priorizar docs com termo exato, mas incluir todos os relevantes
                    documentos_relevantes = docs_com_termo_exato + docs_score_alto
                    
                    # Se n√£o encontrou NENHUM documento com termo exato E n√£o h√° scores muito altos, rejeitar
                    if not docs_com_termo_exato and not docs_score_alto:
                        logger.warning(f"‚ö†Ô∏è Nenhum documento relevante para '{pergunta}' (termos: {termos_pergunta}, sem scores altos)")
                        return RAGResponse(
                            question=pergunta,
                            answer="Ainda n√£o existem artigos sobre este assunto na base de conhecimento.",
                            sources=[],
                            reasoning="Sem artigos relevantes para a pergunta (sem matches de termo e scores baixos)",
                            model_info={"status": "no_match", "model": self.model_name}
                        )
            
            if not documentos_relevantes:
                logger.warning(f"‚ö†Ô∏è Nenhum artigo com similaridade suficiente para: {pergunta} (scores: {[doc.score for doc in documentos]})")
                return RAGResponse(
                    question=pergunta,
                    answer="Ainda n√£o existem artigos sobre este assunto na base de conhecimento.",
                    sources=[],
                    reasoning="Sem artigos relevantes com similaridade suficiente",
                    model_info={"status": "baixa_similaridade", "modelo": self.model_name}
                )
            
            # N√ÉO remover duplicatas - permitir m√∫ltiplos chunks do mesmo artigo
            # Isso √© crucial para queries t√©cnicas onde informa√ß√£o espec√≠fica pode estar em chunks diferentes
            documentos = documentos_relevantes
            
            logger.info(f"üìö Encontrou {len(documentos)} chunks para RAG (artigos: {list(set([d.title for d in documentos]))})")
            await enviar_telemetria("Gerando resposta com Ollama. Isso pode demorar.....")
            await asyncio.sleep(0.5)
            
            # OTIMIZA√á√ÉO 1: Limitar n√∫mero de chunks (m√°ximo 6 - balanceado)
            # Priorizar chunks mais relevantes (j√° v√™m ordenados por score)
            documentos_limitados = documentos[:6]  # M√°ximo 6 chunks (aumentado de 3)
            if len(documentos) > 6:
                logger.info(f"‚ö° Limitando de {len(documentos)} para 6 chunks mais relevantes")
            

            
            # OTIMIZA√á√ÉO 2: Chunks de tamanho m√©dio (250 chars)
            context_parts = []
            for i, doc in enumerate(documentos_limitados, 1):
                # Balanceamento: mais contexto, mas ainda r√°pido
                content_snippet = doc.content[:500]  # Aumentado para 500 para respostas mais completas
                if len(doc.content) > 250:
                    content_snippet += "..."
                
                context_parts.append(f"[{i}] {doc.title}:\n{content_snippet}")
            
            context = "\n\n".join(context_parts)
            logger.info(f"üìù Contexto preparado com {len(context)} caracteres de {len(documentos_limitados)} fontes")
            
            # Fase 2: Gerar resposta com Ollama
            logger.info(f"ü§ñ Chamando Ollama com modelo {self.model_name}...")
            generation_start = time.time()
            
            resposta, telemetria_llm = self._generate_answer_with_ollama(pergunta, context)
            generation_time = time.time() - generation_start
            total_time = time.time() - start_time

            await enviar_telemetria("Resposta com Ollama recebida!")
            await asyncio.sleep(0.5)
            await enviar_telemetria(f"Processando rede... ({round(generation_time,2)}s)")
            await asyncio.sleep(0.5)
            await enviar_telemetria(f"Gerando resposta... ({round(generation_time-0.82,2)}s)")
            await asyncio.sleep(0.5)
            
            # Calcular estat√≠sticas
            total_chunks = len(documentos)
            artigos_unicos = set(doc.title for doc in documentos)
            total_artigos = len(artigos_unicos)
            
            logger.info(f"üìä Estat√≠sticas - {total_chunks} chunks de {total_artigos} artigos √∫nicos")
            logger.info(f"‚è±Ô∏è Tempos - Busca: {search_time:.2f}s, Gera√ß√£o: {generation_time:.2f}s, Total: {total_time:.2f}s")
            
            # Montar telemetria detalhada no formato esperado pelo frontend
            telemetria = {
                "tempo_total_ms": round(total_time * 1000, 2),
                "tempo_busca_qdrant_ms": round(search_time * 1000, 2),
                "tempo_filtragem_ms": telemetria_busca.get("tempo_filtragem_ms", 0),
                "resultados_antes_filtro": telemetria_busca.get("resultados_antes_filtro", total_chunks),
                "resultados_depois_filtro": len(documentos),
                # Campos extras
                "chunks_encontrados": total_chunks,
                "artigos_encontrados": total_artigos,
                # LLM
                "llm": telemetria_llm,
                "sucesso": "Verdadeiro" if total_chunks > 0 else "Falso"
            }

            await enviar_telemetria("Finalizando...")
            await asyncio.sleep(0.5)
            
            return RAGResponse(
                question=pergunta,
                answer=resposta,
                sources=documentos,
                reasoning=f"Resposta gerada com {self.model_name} baseada em {total_chunks} chunks de {total_artigos} artigos",
                model_info={
                    "status": "ok", 
                    "modelo": self.model_name,
                    "tempos": {
                        "busca": round(search_time, 2),
                        "geracao": round(generation_time, 2),
                        "total": round(total_time, 2)
                    }
                },
                total_chunks=total_chunks,
                total_artigos=total_artigos,
                telemetria=telemetria
            )
            
        except Exception as e:
            logger.error(f"‚ùå Erro no RAG: {e}", exc_info=True)  # exc_info=True mostra traceback completo
            return RAGResponse(
                question=pergunta,
                answer=f"Erro ao processar a pergunta: {str(e)}",
                sources=[],
                reasoning=f"Erro t√©cnico: {str(e)}",
                model_info={"status": "erro", "modelo": "nenhum", "erro": str(e)}
            )
    
    def _generate_answer_with_ollama(self, question: str, context: str) -> str:
        """Gera resposta usando Ollama"""
        total_start = time.time()
        
        # OTIMIZA√á√ÉO 3: Contexto balanceado (1200 chars)
        max_context_length = 3000  # Aumentado para 3000 para permitir mais contexto
        if len(context) > max_context_length:
            context = context[:max_context_length] + "..."
            logger.info(f"üìù Contexto truncado para {max_context_length} caracteres")
        
        prompt_build_start = time.time()
        # OTIMIZA√á√ÉO 4: Prompt MUITO mais curto para reduzir tokens (895‚Üí~300)
        # Prompt sempre for√ßa resposta em portugu√™s e limita o conhecimento ao contexto fornecido
        prompt = (
            f"""Responda em portugu√™s usando SOMENTE as informa√ß√µes abaixo extra√≠das dos artigos cadastrados na base de conhecimento. 
               N√ÉO utilize nenhum conhecimento externo, n√£o invente fatos e n√£o fa√ßa suposi√ß√µes. Se n√£o encontrar a resposta nos textos fornecidos, 
               responda apenas: 'N√£o encontrei informa√ß√µes nos artigos cadastrados. Mas verifique todos artigos.'\n\n"
            f"Contexto dos artigos:\n\n{context}\n\nPergunta: {question}"""
        )
        prompt_build_time = time.time() - prompt_build_start

        logger.info(f"ü§ñ Enviando prompt para Ollama (tamanho: {len(prompt)} caracteres)")
        logger.info(f"‚è±Ô∏è Tempo de prepara√ß√£o do prompt: {prompt_build_time*1000:.1f}ms")
        
        url = f"http://{self.ollama_host}:{self.ollama_port}/api/generate"
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.6,
                "num_predict": 400,      # Aumentado para 400 tokens (respostas mais completas)
                "num_ctx": 1536,         # Aumentado: 1024‚Üí1536 (meio termo)
                "repeat_penalty": 1.1,
                "top_k": 40,
                "stop": ["Pergunta:", "\n\n\n", "\n\nRegras:"]
            }
        }
        
        logger.info(f"‚è±Ô∏è Aguardando resposta do Ollama (modelo: {self.model_name}, timeout: 600s)...")
        logger.info(f"‚öôÔ∏è Config: temp={payload['options']['temperature']}, num_predict={payload['options']['num_predict']}, num_ctx={payload['options']['num_ctx']}")
        
        request_start = time.time()
        
        try:
            response = requests.post(url, json=payload, timeout=600)
            request_time = time.time() - request_start
            
            logger.info(f"üì° Ollama respondeu com status {response.status_code} em {request_time:.1f}s")
            
            if response.status_code == 200:
                parse_start = time.time()
                data = response.json()
                answer = data.get('response', 'Erro ao gerar resposta').strip()
                parse_time = time.time() - parse_start
                
                total_time = time.time() - total_start
                
                # Estat√≠sticas detalhadas do Ollama (se dispon√≠veis)
                eval_count = data.get('eval_count', 0)
                eval_duration = data.get('eval_duration', 0) / 1e9 if data.get('eval_duration') else 0
                prompt_eval_count = data.get('prompt_eval_count', 0)
                prompt_eval_duration = data.get('prompt_eval_duration', 0) / 1e9 if data.get('prompt_eval_duration') else 0
                
                # Criar telemetria estruturada
                telemetria = {
                    "prompt_tokens": prompt_eval_count,
                    "prompt_eval_time": round(prompt_eval_duration, 2),
                    "prompt_tokens_per_sec": round(prompt_eval_count / prompt_eval_duration, 1) if prompt_eval_duration > 0 else 0,
                    "completion_tokens": eval_count,
                    "completion_time": round(eval_duration, 2),
                    "completion_tokens_per_sec": round(eval_count / eval_duration, 1) if eval_duration > 0 else 0,
                    "total_tokens": prompt_eval_count + eval_count,
                    "total_time": round(request_time, 2),
                    "model": self.model_name,
                    "config": {
                        "temperature": 0.6,
                        "num_predict": 150,
                        "num_ctx": 1536
                    }
                }
                
                logger.info(f"‚úÖ Resposta gerada em {request_time:.1f}s (tamanho: {len(answer)} caracteres)")
                logger.info(f"üìä BREAKDOWN DETALHADO:")
                logger.info(f"   ‚îî‚îÄ Prepara√ß√£o prompt: {prompt_build_time*1000:.1f}ms")
                logger.info(f"   ‚îî‚îÄ Rede/Processamento: {request_time:.2f}s")
                if prompt_eval_count > 0:
                    logger.info(f"      ‚îú‚îÄ Avalia√ß√£o do prompt: {prompt_eval_duration:.2f}s ({prompt_eval_count} tokens, {prompt_eval_count/prompt_eval_duration:.0f} tok/s)")
                if eval_count > 0:
                    logger.info(f"      ‚îî‚îÄ Gera√ß√£o da resposta: {eval_duration:.2f}s ({eval_count} tokens, {eval_count/eval_duration:.0f} tok/s)")
                logger.info(f"   ‚îî‚îÄ Parse JSON: {parse_time*1000:.1f}ms")
                logger.info(f"   ‚îî‚îÄ Total _generate_answer: {total_time:.2f}s")
                
                return answer, telemetria
            else:
                error_text = response.text[:200] if response.text else "sem detalhes"
                logger.error(f"‚ùå Ollama erro {response.status_code}: {error_text}")
                return f"Erro: LLM respondeu com status {response.status_code}", {}
                
        except requests.exceptions.Timeout:
            logger.error(f"‚è∞ Timeout ao gerar resposta (>600s)")
            return "Timeout: A pergunta demorou muito para ser processada. Tente ser mais espec√≠fico.", {}
        except requests.exceptions.ConnectionError as e:
            logger.error(f"üîå Erro de conex√£o com Ollama: {e}")
            return "Erro: N√£o foi poss√≠vel conectar ao servi√ßo de LLM.", {}
        except Exception as e:
            logger.error(f"‚ùå Erro inesperado ao chamar Ollama: {e}", exc_info=True)
            return f"Erro ao gerar resposta: {str(e)}", {}
    
    def _get_sample_results(self, query: str, limit: int) -> List[SearchResult]:
        """Retorna lista vazia - n√£o usar samples hardcoded"""
        logger.warning(f"‚ö†Ô∏è Nenhum resultado encontrado para '{query}' - retornando lista vazia")
        return []
    
    def obter_estatisticas(self, colecao=None) -> Dict[str, Any]:
        """Estat√≠sticas da base Wikipedia ou cole√ß√£o escolhida"""
        try:
            collection_name = colecao if colecao else self.collection_name
            if self.client:
                collection_info = self.client.get_collection(collection_name)
                # Buscar n√∫mero de artigos √∫nicos
                all_points = []
                offset = None
                while True:
                    result = self.client.scroll(
                        collection_name=collection_name,
                        limit=100,
                        offset=offset,
                        with_payload=True,
                        with_vectors=False
                    )
                    points, offset = result
                    all_points.extend(points)
                    if offset is None:
                        break
                artigos_dict = {}
                for point in all_points:
                    title = point.payload.get('title', 'Sem t√≠tulo')
                    artigos_dict[title] = True
                total_artigos = len(artigos_dict)
                return {
                    "sistema_offline": True,
                    "colecao": collection_name,
                    "total_chunks": collection_info.points_count,
                    "total_artigos": total_artigos,
                    "dimensoes_vetor": collection_info.config.params.vectors.size,
                    "distancia": collection_info.config.params.vectors.distance.value,
                    "modelo_llm": self.model_name,
                    "status": "funcional"
                }
            else:
                return {
                    "sistema_offline": True,
                    "colecao": "nao_conectado",
                    "total_chunks": 0,
                    "total_artigos": 0,
                    "status": "modo_exemplo"
                }
        except Exception as e:
            return {"erro": f"Erro ao obter estat√≠sticas: {str(e)}"}
    
    def listar_todos_artigos(self, colecao=None) -> Dict[str, Any]:
        """Lista todos os artigos √∫nicos na base de conhecimento, filtrando por cole√ß√£o se fornecida"""
        try:
            if not self.client:
                logger.error("‚ùå Cliente Qdrant n√£o inicializado")
                return {"artigos": [], "total": 0}

            # Usar cole√ß√£o fornecida ou padr√£o
            collection_name = colecao if colecao else "wikipedia_langchain"
            all_points = []
            offset = None
            while True:
                result = self.client.scroll(
                    collection_name=collection_name,
                    limit=100,
                    offset=offset,
                    with_payload=True,
                    with_vectors=False
                )
                
                points, offset = result
                all_points.extend(points)
                
                if offset is None:
                    break
            
            # Agrupar por t√≠tulo e pegar informa√ß√µes √∫nicas
            artigos_dict = {}
            for point in all_points:
                title = point.payload.get('title', 'Sem t√≠tulo')
                if title not in artigos_dict:
                    artigos_dict[title] = {
                        'title': title,
                        'url': point.payload.get('url', ''),
                        'chunks': 0,
                        'timestamp': point.payload.get('timestamp', ''),
                        'preview': point.payload.get('content', '')[:200]
                    }
                artigos_dict[title]['chunks'] += 1
            
            # Converter para lista e ordenar por t√≠tulo
            artigos_list = sorted(artigos_dict.values(), key=lambda x: x['title'].lower())
            
            logger.info(f"üìö Encontrados {len(artigos_list)} artigos √∫nicos ({len(all_points)} chunks total)")
            
            return {
                "artigos": artigos_list,
                "total": len(artigos_list),
                "total_chunks": len(all_points)
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao listar artigos: {e}")
            return {"artigos": [], "total": 0, "erro": str(e)}
    
    def limpar_colecao(self) -> bool:
        """Remove todos os pontos da cole√ß√£o"""
        try:
            if not self.client:
                logger.error("‚ùå Cliente Qdrant n√£o inicializado")
                return False
                
            # Deletar e recriar a cole√ß√£o
            try:
                self.client.delete_collection(self.collection_name)
                logger.info(f"üóëÔ∏è Cole√ß√£o {self.collection_name} removida")
            except Exception:
                logger.info(f"‚ö†Ô∏è Cole√ß√£o {self.collection_name} n√£o existia")
            
            # Recriar cole√ß√£o vazia
            self._criar_colecao(self)
            logger.info(f"‚úÖ Cole√ß√£o {self.collection_name} recriada vazia")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao limpar cole√ß√£o: {e}")
            return False
    
    def _processar_lote_chunks(self, chunk_batch: List[Dict]) -> int:
        """Processa um lote de chunks de dumps"""
        if not self.client or not chunk_batch:
            return 0
            
        try:
            points = []
            
            for chunk_data in chunk_batch:
                # TODO: Gerar embedding real aqui
                # Para simplificar, usar vetor fake (em produ√ß√£o seria embeddings reais)
                fake_vector = [0.1] * 384
                
                point_id = str(uuid.uuid4())
                
                point = models.PointStruct(
                    id=point_id,
                    vector=fake_vector,
                    payload={
                        "title": chunk_data['title'],
                        "content": chunk_data['content'], 
                        "url": chunk_data['url'],
                        "chunk_index": chunk_data.get('chunk_index', 0),
                        "total_chunks": chunk_data.get('total_chunks', 1),
                        "article_id": chunk_data.get('article_id', 0),
                        "timestamp": chunk_data.get('timestamp', ''),
                        "source": chunk_data.get('source', 'wikipedia_dump')
                    }
                )
                points.append(point)
            
            if points:
                self.client.upsert(
                    collection_name=self.collection_name,
                    points=points
                )
                logger.info(f"‚úÖ Lote processado: {len(points)} chunks adicionados")
            
            return len(points)
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao processar lote: {e}")
            return 0
    
    def verificar_status(self, colecao=None) -> Dict[str, Any]:
        """Status completo do sistema, opcionalmente para uma cole√ß√£o espec√≠fica"""
        # Verifica n√∫mero de cole√ß√µes no Qdrant
        colecoes_count = 0
        colecoes_lista = []
        collection_name = colecao if colecao else self.collection_name
        if self.client:
            try:
                colecoes_obj = self.client.get_collections()
                colecoes_count = len(colecoes_obj.collections)
                colecoes_lista = [col.name for col in colecoes_obj.collections]
            except Exception:
                colecoes_count = 0
                colecoes_lista = []

        # Campos obrigat√≥rios do modelo StatusResponse
        # Verificar status do Ollama
        ollama_disponivel = False
        modelo_llm = getattr(self, 'model_name', 'unknown')
        try:
            url = f"http://{getattr(self, 'ollama_host', 'localhost')}:{getattr(self, 'ollama_port', 11434)}/api/version"
            import requests
            resp = requests.get(url, timeout=3)
            if resp.status_code == 200:
                ollama_disponivel = True
        except Exception:
            ollama_disponivel = False

        # Se quiser retornar status da cole√ß√£o espec√≠fica, pode customizar aqui
        return {
            "status": "ok" if self._initialized else "error",
            "qdrant_conectado": self.client is not None,
            "colecao": collection_name,
            "colecoes": colecoes_count,
            "colecoes_lista": colecoes_lista,
            "modelo_embedding_carregado": True,  # ajuste conforme l√≥gica real
            "modelo_embedding_nome": self.embedding_model_name,
            "text_splitter_configurado": True,    # ajuste conforme l√≥gica real
            "openai_configurado": False,
            "inicializado": self._initialized,
            "ollama_disponivel": ollama_disponivel,
            "modelo_llm": modelo_llm
        }
    
    def obter_metricas(self) -> Dict:
        """Retorna m√©tricas coletadas pelo servi√ßo"""
        return self.metrics.get_metrics()
    
    def resetar_metricas(self):
        """Reseta todas as m√©tricas coletadas"""
        self.metrics.reset_metrics()
    
    def _test_ollama_connection(self) -> bool:
        """Testa se Ollama est√° respondendo"""
        try:
            url = f"http://{self.ollama_host}:{self.ollama_port}/api/version"
            response = requests.get(url, timeout=2)
            return response.status_code == 200
        except:
            return False


# Inst√¢ncia global do servi√ßo
wikipedia_offline_service = WikipediaOfflineService()