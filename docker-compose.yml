services:
  # Banco de dados vetorial Qdrant
  qdrant:
    image: qdrant/qdrant:v1.11.3
    container_name: qdrant_offline
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_storage:/qdrant/storage
    environment:
      QDRANT__STORAGE__STORAGE_PATH: /qdrant/storage
      QDRANT__SERVICE__GRPC_PORT: 6334
      QDRANT__SERVICE__HTTP_PORT: 6333
    networks:
      - offline_wikipedia_network
    restart: unless-stopped

  # Servidor Ollama para LLM local
  ollama:
    image: ollama/ollama:latest
    container_name: ollama_server
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    environment:
      - OLLAMA_MODELS=/root/.ollama/models
    networks:
      - offline_wikipedia_network
    restart: unless-stopped
    # Para GPU (descomente se tiver GPU NVIDIA)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # Aplicação FastAPI Offline
  app:
    build: .
    container_name: offline_wikipedia_app
    ports:
      - "9000:9000"
    depends_on:
      - qdrant
      - ollama
    volumes:
      - .:/app
      - ./data:/app/data  # Para dumps XML e cache
    working_dir: /app
    command: uvicorn api.wikipediaFuncionalAPI:app --host 0.0.0.0 --port 9000 --reload
    environment:
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - DATA_DIR=/app/data
      - EMBEDDING_MODEL=paraphrase-multilingual-MiniLM-L12-v2
      - LLM_TYPE=ollama
      - LLM_MODEL=qwen2.5:7b
      - LLM_MAX_TOKENS=512
      - LLM_TEMPERATURE=0.7
      - OLLAMA_HOST=ollama
      - OLLAMA_PORT=11434
      - PYTHONPATH=/app
    networks:
      - offline_wikipedia_network
    restart: unless-stopped

volumes:
  qdrant_storage:
    external: true
    name: dicionario_vetorial_qdrant_storage
  ollama_models:

networks:
  offline_wikipedia_network:
    driver: bridge
