#!/usr/bin/env python3
"""
Script para listar artigos da Wikipedia armazenados no Qdrant
"""

import requests
import json
from collections import Counter

def listar_artigos_qdrant(
    host="localhost",
    port=6333,
    collection="wikipedia_langchain",
    output_file=None
):
    """
    Lista todos os artigos √∫nicos do Qdrant
    
    Args:
        host: Host do Qdrant
        port: Porta do Qdrant
        collection: Nome da cole√ß√£o
        output_file: Arquivo para salvar a lista (opcional)
    """
    
    url = f"http://{host}:{port}/collections/{collection}/points/scroll"
    
    all_titles = []
    offset = None
    
    print(f"üîç Buscando artigos da cole√ß√£o '{collection}'...")
    
    while True:
        # Preparar payload
        payload = {
            "limit": 1000,
            "with_payload": True,
            "with_vector": False
        }
        
        if offset:
            payload["offset"] = offset
        
        # Fazer requisi√ß√£o
        response = requests.post(url, json=payload)
        data = response.json()
        
        # Extrair t√≠tulos
        points = data.get("result", {}).get("points", [])
        titles = [point["payload"].get("title", "") for point in points]
        all_titles.extend(titles)
        
        print(f"  Processados: {len(all_titles)} chunks...")
        
        # Verificar se h√° mais p√°ginas
        offset = data.get("result", {}).get("next_page_offset")
        if not offset:
            break
    
    # Obter t√≠tulos √∫nicos
    unique_titles = sorted(set(all_titles))
    
    # Estat√≠sticas
    title_counts = Counter(all_titles)
    
    print(f"\n‚úÖ Processamento conclu√≠do!")
    print(f"üìä Total de chunks: {len(all_titles)}")
    print(f"üìö Artigos √∫nicos: {len(unique_titles)}")
    print(f"üìà M√©dia de chunks por artigo: {len(all_titles) / len(unique_titles):.1f}")
    
    # Top 10 artigos com mais chunks
    print(f"\nüèÜ Top 10 artigos com mais chunks:")
    for title, count in title_counts.most_common(10):
        print(f"  {count:3d} chunks - {title}")
    
    # Primeiros 50 artigos
    print(f"\nüìã Primeiros 50 artigos (alfabeticamente):")
    for i, title in enumerate(unique_titles[:50], 1):
        print(f"  {i:2d}. {title}")
    
    # Salvar em arquivo se especificado
    if output_file:
        with open(output_file, 'w', encoding='utf-8') as f:
            for title in unique_titles:
                f.write(f"{title}\n")
        print(f"\nüíæ Lista salva em: {output_file}")
    
    return unique_titles


def exportar_json(titles, output_file="artigos.json"):
    """Exporta lista em formato JSON com estat√≠sticas"""
    data = {
        "total_artigos": len(titles),
        "artigos": titles,
        "data_exportacao": "2025-11-06"
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"üíæ JSON exportado: {output_file}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Lista artigos da Wikipedia no Qdrant")
    parser.add_argument("--host", default="localhost", help="Host do Qdrant")
    parser.add_argument("--port", type=int, default=6333, help="Porta do Qdrant")
    parser.add_argument("--collection", default="wikipedia_langchain", help="Nome da cole√ß√£o")
    parser.add_argument("--output", help="Arquivo de sa√≠da (.txt)")
    parser.add_argument("--json", help="Exportar como JSON")
    
    args = parser.parse_args()
    
    # Listar artigos
    titles = listar_artigos_qdrant(
        host=args.host,
        port=args.port,
        collection=args.collection,
        output_file=args.output
    )
    
    # Exportar JSON se solicitado
    if args.json:
        exportar_json(titles, args.json)
